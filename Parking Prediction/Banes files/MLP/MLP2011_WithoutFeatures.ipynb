{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "MLP2011_WithoutFeatures.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_aVTChKIx3",
        "colab_type": "code",
        "outputId": "691b861a-e23a-493b-b4da-8d9b13f6f7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkZGl_g0KFc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.version\n",
        "#Import Libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "import shutil\n",
        "import tensorflow.contrib.learn as tflearn\n",
        "import tensorflow.contrib.layers as tflayers\n",
        "from tensorflow.contrib.learn.python.learn import learn_runner\n",
        "import tensorflow.contrib.metrics as metrics\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "from random import shuffle\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import xgboost as xgb\n",
        "#TF Version\n",
        "tf.__version__\n",
        "\n",
        "#with warnings.catch_warnings():\n",
        "#    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "#    import h5py\n",
        "\n",
        "num_periods_output = 12 #to predict\n",
        "num_periods_input=24 #input\n",
        "l_rate = 0.0004   \n",
        "MiniBatches_size=128\n",
        "keep_probab=0.9            #(1-dropout_rate)\n",
        "keep_prob_testval=1.0  \n",
        "epochs = 400   \n",
        "Number_of_TimeFeatures=num_periods_input\n",
        "inputs = Number_of_TimeFeatures \n",
        "lamda=0.4\n",
        "\n",
        "\n",
        "No_Of_weeks=1\n",
        "\n",
        "ALL_Test_Data=[]\n",
        "ALL_Test_Prediction=[]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4odAQaTMKFdB",
        "colab_type": "text"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpCUqQK9KFdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "def preprocessing(df_,num_features):\n",
        "    \n",
        "    if df_.ID[0]!=7 and df_.ID[0]!=8:\n",
        "        print(df_.Year.unique())\n",
        "        df=df_[(df_.Year==2017) | (df_.Year==2018)]\n",
        "        #.isin(years)\n",
        "        #print(df.loc[df['Year'].isin([2017,2018])])\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    else:\n",
        "        df=df_[(df_.Year==2015) | (df_.Year==2016)]\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    \n",
        "    '''df_=df[['ID','Occupancy','Year', 'Month', 'Day', 'Hour','Minute', 'Capacity', \n",
        "    'DayOfWeek','IsWeekend', 'temperature', 'dew_point', 'humidity', 'wind_speed', \n",
        "    'feels_like', 'Status', 'light_snow','snow_shower', 'fog', \n",
        "    'thunder', 'mostly_cloudy','rain', 'heavy_rain', 'mist', 'shallow_fog','light_freezing_rain',\n",
        "    'partly_cloudy', 'haze', 'light_rain', 'rain_shower', 'snow', 'light_snow_shower']]'''\n",
        "    \n",
        "    \n",
        "    \n",
        "    # select features\n",
        "    df=df[['ID','Occupancy']]\n",
        "    \n",
        "    \n",
        "    ################################################encoding########################\n",
        "    df['Occupancy'] = pd.to_numeric(df['Occupancy'],errors='coerce')\n",
        "    df['Occupancy'] = df['Occupancy'].abs()\n",
        "    \n",
        "    #Status=df.pop('Status')\n",
        "    #df.loc[:,'Status_filling']=(Status=='Filling')*1.0\n",
        "    #df.loc[:,'Status_static']=(Status=='Static')*1.0\n",
        "    #df.loc[:,'Status_emptying']=(Status=='Emptying')*1.0\n",
        "    \n",
        "    Number_Of_Features=num_features\n",
        "    df=df.values\n",
        "    df = df.astype('float32')\n",
        "    split=num_periods_output+num_periods_input\n",
        "    \n",
        "    ##################################SPLIT##############################################\n",
        "    \n",
        "    ########################## SPLITTING FOR TESTING ##########################\n",
        "    test_val=np.floor(len(df)*0.2)\n",
        "    mod=test_val%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    test_val=int(test_val-mod)\n",
        "    Test_cut=int(np.floor(test_val/2))\n",
        "    Test=df[(len(df)-Test_cut):,:]\n",
        "    Valid=df[(len(df)-test_val):(len(df)-Test_cut),:]\n",
        "    ########################### SPLITTING FOR TRAIN ###########################\n",
        "    \n",
        "    new_cutted_df=df[:(len(df)-test_val),:]\n",
        "    Start_train_index=12*24*7*No_Of_weeks\n",
        "    Start_train_index=np.floor(Start_train_index)\n",
        "    Start_train_index=int(Start_train_index)\n",
        "    print('instances',Start_train_index)\n",
        "    Train=new_cutted_df[len(new_cutted_df)-Start_train_index:,:]\n",
        "    train_len=len(Train)\n",
        "    mod=train_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    train_len=int(train_len-mod)\n",
        "    Train=Train[0:train_len,:]\n",
        "    print('len Train',len(Train))\n",
        "   \n",
        "        \n",
        "     ############################################ Valid minibatches ##################################\n",
        "    \n",
        "    end_valid=len(Valid)\n",
        "    start=0\n",
        "    next=0\n",
        "    x_validbatches=[]\n",
        "    y_validbatches=[]\n",
        "    \n",
        "    count=0\n",
        "    #print('lennnn',len(Train))\n",
        "    while next+(num_periods_input+num_periods_output)<end_valid:\n",
        "        next=start+num_periods_input\n",
        "        x_validbatches.append(Valid[start:next,:])\n",
        "        y_validbatches.append(Valid[next:next+num_periods_output,1])\n",
        "        start=start+1\n",
        "    y_validbatches=np.asarray(y_validbatches)\n",
        "    y_validbatches = y_validbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_validbatches=np.asarray(x_validbatches)\n",
        "    x_validbatches = x_validbatches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "    print('len x_validbatches ',len(x_validbatches))\n",
        "    ############################################ TRAIN minibatches ##################################\n",
        "    \n",
        "    end=len(Train)\n",
        "    start=0\n",
        "    next=0\n",
        "    x_batches=[]\n",
        "    y_batches=[]\n",
        "    \n",
        "    count=0\n",
        "    #print('lennnn',len(Train))\n",
        "    while next+(num_periods_input+num_periods_output)<end:\n",
        "        next=start+num_periods_input\n",
        "        x_batches.append(Train[start:next,:])\n",
        "        y_batches.append(Train[next:next+num_periods_output,1])\n",
        "        start=start+1\n",
        "    y_batches=np.asarray(y_batches)\n",
        "    y_batches = y_batches.reshape(-1, num_periods_output, 1)   \n",
        "    x_batches=np.asarray(x_batches)\n",
        "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "    print('len x_batches ',len(x_batches))\n",
        "    \n",
        "    ###########################################TEST#####################################\n",
        "    ############################################ TEST minibatches ##################################\n",
        "    end_test=len(Test)\n",
        "    start_test=0\n",
        "    next_test=0\n",
        "    x_testbatches=[]\n",
        "    y_testbatches=[]\n",
        "    \n",
        "    \n",
        "    #print('lennnn',len(Train))\n",
        "    while next_test+(num_periods_input+num_periods_output)<end_test:\n",
        "        next_test=start_test+num_periods_input\n",
        "        x_testbatches.append(Test[start_test:next_test,:])\n",
        "        y_testbatches.append(Test[next_test:next_test+num_periods_output,1])\n",
        "        start_test=start_test+1\n",
        "    y_testbatches=np.asarray(y_testbatches)\n",
        "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_testbatches=np.asarray(x_testbatches)\n",
        "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features) \n",
        "    print('len Test',len(Test))\n",
        "    print('len xTestbatches',len(x_testbatches))\n",
        "    ######################## Sampling##########################################\n",
        "    \n",
        "    #x_batches, y_batches, x_validbatches, y_validbatches, x_testbatches, y_testbatches\n",
        "    \n",
        "    return x_batches, y_batches, x_validbatches, y_validbatches, x_testbatches, y_testbatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GeTnVETKFdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_locationfiles(path,loc_id):\n",
        "    filename=path + '/BN00'+str(loc_id)+'.csv'\n",
        "    print(filename)\n",
        "    data_loc=pd.read_csv(filename)\n",
        "    #print(data_loc.head())\n",
        "    #mod=len(data_loc)%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    #data_loc=data_loc[:len(data_loc)-mod]\n",
        "    return data_loc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBMD4AcXKFdI",
        "colab_type": "code",
        "outputId": "b3681bc4-2c60-4e5c-8e10-61ff018d2485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "    data_path=r'/content/drive/My Drive/FINAL_DATA_EVENTS'\n",
        "    #r'/home/shero/Desktop/OurProject/BanesData/Model/occupation_loc/'\n",
        "    data_All=pd.DataFrame()\n",
        "    x_batches_Full=[]\n",
        "    y_batches_Full=[]\n",
        "    X_Valid_Full=[]\n",
        "    Y_Valid_Full=[]\n",
        "    X_Test_Full=[]\n",
        "    Y_Test_Full=[]\n",
        "    for loc_id in range(1,9):\n",
        "        #========\n",
        "        data=load_locationfiles(data_path,loc_id)\n",
        "        header=list(data.columns.values)\n",
        "        data=pd.DataFrame(data,columns=header)\n",
        "        x_batches, y_batches,x_valid,y_valid,X_Test,Y_Test=preprocessing(data,2)\n",
        "        #===============================\n",
        "        for element1 in (x_batches):\n",
        "            x_batches_Full.append(element1)\n",
        "            \n",
        "        for element2 in (y_batches):\n",
        "            y_batches_Full.append(element2)\n",
        "\n",
        "        for element3 in (x_valid):\n",
        "            X_Valid_Full.append(element3)\n",
        "            \n",
        "        for element4 in (y_valid):\n",
        "            Y_Valid_Full.append(element4)\n",
        "                        \n",
        "        for element5 in (X_Test):\n",
        "            X_Test_Full.append(element5)\n",
        "            \n",
        "        for element6 in (Y_Test):\n",
        "            Y_Test_Full.append(element6)\n",
        "    #---------------------shuffle minibatches X and Y together-------------------------------------\n",
        "    print(len(x_batches_Full),'     length of all file : ',len(y_batches_Full))\n",
        "    combined = list(zip(x_batches_Full, y_batches_Full))\n",
        "    random.shuffle(combined)\n",
        "    shuffled_batch_features, shuffled_batch_y = zip(*combined)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN001.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[628]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  18229\n",
            "len x_batches  1957\n",
            "len Test 18288\n",
            "len xTestbatches 18229\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN002.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1056]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  19075\n",
            "len x_batches  1957\n",
            "len Test 19134\n",
            "len xTestbatches 19075\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN003.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[ 860 1500 1160 1360]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  14773\n",
            "len x_batches  1957\n",
            "len Test 14832\n",
            "len xTestbatches 14773\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN004.csv\n",
            "[2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[698]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  19615\n",
            "len x_batches  1957\n",
            "len Test 19674\n",
            "len xTestbatches 19615\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN005.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1320 1230]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  14647\n",
            "len x_batches  1957\n",
            "len Test 14706\n",
            "len xTestbatches 14647\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN006.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[521 519 525 513]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  19579\n",
            "len x_batches  1957\n",
            "len Test 19638\n",
            "len xTestbatches 19579\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN007.csv\n",
            "[2015 2016]\n",
            "[720 626 800 860 700 623 519 528 563 638 603 630 596 602 840]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  9625\n",
            "len x_batches  1957\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN008.csv\n",
            "[2015 2016]\n",
            "[140 180 132]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  9625\n",
            "len x_batches  1957\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "15656      length of all file :  15656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQ2LajOKFdL",
        "colab_type": "code",
        "outputId": "650d894d-56b7-41b0-803d-c75114a27832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "#xgboost part\n",
        "print(len(x_batches_Full))\n",
        "All_Training_Instances=[]\n",
        "\n",
        "Static_Features=[[1,0,0,0,0,0,0,0,1,3,4,6,7,19,9,17,51.3787,-2.3622],[0,1,0,0,0,0,0,0,0,3,9,11,3,29,10,13,51.3843,-2.3686],\n",
        "                     [0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,51.4113,-2.3869],[0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,51.3902,-2.4059]\n",
        "                    ,[0,0,0,0,1,0,0,0,0,0,0,4,2,3,0,0,51.3529,-2.3838],[0,0,0,0,0,1,0,0,2,4,8,6,3,39,13,14,51.3842,-2.3590],\n",
        "                     [0,0,0,0,0,0,1,0,1,1,3,6,6,17,9,7,51.3782,-2.3589],[0,0,0,0,0,0,0,1,1,2,3,6,6,18,9,7,51.3783,-2.3593]]\n",
        " \n",
        "#=============== change each window into Instance =================================\n",
        "for i in range(0,len(shuffled_batch_features)):\n",
        "    hold=[]\n",
        "    temp=[]\n",
        "    for j in range(0,len(shuffled_batch_features[i])):\n",
        "      #print(len(hold))\n",
        "      \n",
        "      if j==(len(shuffled_batch_features[i])-1):\n",
        "          index=int(shuffled_batch_features[i][j][0])\n",
        "          #hold=np.concatenate((hold, Static_Features[index-1]), axis=None)\n",
        "          temp=np.delete(shuffled_batch_features[i][j], [0], axis=0) \n",
        "          #print('occupancy ',temp)\n",
        "          hold=np.concatenate((hold, temp), axis=None)\n",
        "          \n",
        "      else:\n",
        "          hold=np.concatenate((hold, shuffled_batch_features[i][j][1]), axis=None)\n",
        "          \n",
        "    #print(len(hold))\n",
        "    All_Training_Instances.append(hold)\n",
        "    \n",
        "\n",
        "print(len(All_Training_Instances[0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15656\n",
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5zKLm39w34R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    #batch_size=\n",
        "    X_returned=[]\n",
        "    Y_returned=[]\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        if (start + batch_size)>len(features):\n",
        "            break\n",
        "        else:\n",
        "            end = start + batch_size\n",
        "        X_returned.append(features[start:end])\n",
        "        Y_returned.append(labels[start:end])\n",
        "    return X_returned, Y_returned\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H23SYSNC3y6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####### MLP session##################\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, inputs])   #create variable objects\n",
        "y = tf.placeholder(tf.float32, [None, num_periods_output])\n",
        "learning_rate=tf.placeholder(tf.float32, None)\n",
        "keep_prob = tf.placeholder(tf.float32, None)\n",
        "\n",
        "\n",
        "MLP1= tf.layers.dense(X, units=100,activation=tf.nn.relu) \n",
        "MLP2= tf.layers.dense(MLP1, units=50,activation=tf.nn.relu)  \n",
        "Dropout = tf.nn.dropout(MLP2,keep_prob=keep_probab)\n",
        "output= tf.layers.dense(Dropout, units=num_periods_output)  \n",
        "\n",
        "\n",
        "outputs = tf.reshape(output, [-1, num_periods_output])          #shape of results\n",
        "#Regularization part\n",
        "tv = tf.trainable_variables()\n",
        "regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
        "Error=tf.square(outputs - y)\n",
        "#Total_err=Error+regularization_cost\n",
        "loss = tf.reduce_mean(tf.square(outputs - y))+ lamda*regularization_cost   #define the cost function which evaluates the quality of our model\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  \n",
        "\n",
        "training_op = optimizer.minimize(loss)    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3QbiznjKFdN",
        "colab_type": "code",
        "outputId": "80663b90-2fae-4f6c-ec61-ecaf5dbe7351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#=================Testing=====================\n",
        "All_Testing_Instances=[]\n",
        "All_Validation_Instances=[]\n",
        "#=============== change each window into Instance test=================================\n",
        "print(len(X_Test_Full))\n",
        "for i in range(0,len(X_Test_Full)):\n",
        "  hold=[]\n",
        "  temp=[]\n",
        "  for j in range(0,len(X_Test_Full[i])):\n",
        "       #print(len(hold))\n",
        "      if j==(len(X_Test_Full[i])-1):\n",
        "          index=int(X_Test_Full[i][j][0])\n",
        "          #hold=np.concatenate((hold, Static_Features[index-1]), axis=None)\n",
        "          temp=np.delete(X_Test_Full[i][j], [0], axis=0) \n",
        "          hold=np.concatenate((hold, temp), axis=None)\n",
        "      else:\n",
        "          hold=np.concatenate((hold, X_Test_Full[i][j][1]), axis=None)\n",
        "   \n",
        "  All_Testing_Instances.append(hold)\n",
        "#=============== change each window into Instance validation =================================\n",
        "print(len(X_Valid_Full))\n",
        "for i in range(0,len(X_Valid_Full)):\n",
        "  hold=[]\n",
        "  temp=[]\n",
        "  for j in range(0,len(X_Valid_Full[i])):\n",
        "       #print(len(hold))\n",
        "      if j==(len(X_Valid_Full[i])-1):\n",
        "          index=int(X_Valid_Full[i][j][0])\n",
        "          #hold=np.concatenate((hold, Static_Features[index-1]), axis=None)\n",
        "          temp=np.delete(X_Valid_Full[i][j], [0], axis=0) \n",
        "          hold=np.concatenate((hold, temp), axis=None)\n",
        "      else:\n",
        "          hold=np.concatenate((hold, X_Valid_Full[i][j][1]), axis=None)\n",
        "   \n",
        "  All_Validation_Instances.append(hold)\n",
        "#prediction=multioutput.predict(All_Testing_Instances)\n",
        "print(len(All_Testing_Instances[0]))\n",
        "#===========================calling MultiOutput XGoost=========================\n",
        "All_Testing_Instances=np.reshape(All_Testing_Instances, (len(All_Testing_Instances),len(All_Testing_Instances[0])))\n",
        "Y_Test_Full=np.reshape(Y_Test_Full, (len(Y_Test_Full),num_periods_output))\n",
        "#===========================  Validation set =========================\n",
        "All_Validation_Instances=np.reshape(All_Validation_Instances, (len(All_Validation_Instances),len(All_Validation_Instances[0])))\n",
        "Y_Valid_Full=np.reshape(Y_Valid_Full, (len(Y_Valid_Full),num_periods_output))\n",
        "#========== reshape train ==============================\n",
        "All_Training_Instances=np.reshape(All_Training_Instances, (len(All_Training_Instances),len(All_Training_Instances[0])))\n",
        "shuffled_batch_y=np.reshape(shuffled_batch_y, (len(shuffled_batch_y),num_periods_output))\n",
        "\n",
        "# ======= mini batches for MLP=======\n",
        "All_Training_Instances_batches,shuffled_batch_y_batches=batch_features_labels(All_Training_Instances,shuffled_batch_y,MiniBatches_size)\n",
        "\n",
        "Training_Error=[]\n",
        "Validation_Error=[]\n",
        "train_mse=[]\n",
        "validation_mse=[]\n",
        "\n",
        "\n",
        "print(All_Training_Instances.shape)\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()           #initialize all the variables\n",
        "    init.run()\n",
        "    for ep in range(epochs):\n",
        "        for i in range(0,len(All_Training_Instances_batches)): \n",
        "            _,Train_Batch_error=sess.run([training_op,Error],\n",
        "                                                        feed_dict={\n",
        "                                                            keep_prob:keep_probab,\n",
        "                                                            X: All_Training_Instances_batches[i],\n",
        "                                                            y: shuffled_batch_y_batches[i],\n",
        "                                                            learning_rate: l_rate                                                     \n",
        "                                                            })  \n",
        "            Training_Error.append(Train_Batch_error)\n",
        "        if ep % 3 == 0:\n",
        "            Sum_train=np.sum(Training_Error,axis=2)\n",
        "            Sum_train_1=np.sum(Sum_train,axis=1)\n",
        "            Sum_train_2=np.sum(Sum_train_1,axis=0)\n",
        "            Mean_train=Sum_train_2/(len(Training_Error)*MiniBatches_size*num_periods_output)\n",
        "            print(\"epoch:\",int(ep+1))\n",
        "            print(\"\\tRMSE Training:\", (Mean_train)**0.5)\n",
        "            train_mse.append((Mean_train)**0.5)\n",
        "            All_Validation_Instances_batches,Y_Validation_Full_batches=batch_features_labels(All_Validation_Instances,Y_Valid_Full,MiniBatches_size)\n",
        "            for i in range(0,len(All_Validation_Instances_batches)): \n",
        "                y_predict,Valid_error_batch=sess.run([outputs,Error], \n",
        "                                                            feed_dict={\n",
        "                                                            keep_prob:keep_prob_testval,\n",
        "                                                            X: All_Validation_Instances_batches[i], \n",
        "                                                            y: Y_Validation_Full_batches[i]\n",
        "                                                            })\n",
        "                Validation_Error.append(Valid_error_batch)\n",
        "            Sum_valid=np.sum(Validation_Error,axis=2)\n",
        "            Sum_valid_1=np.sum(Sum_valid,axis=1)\n",
        "            Sum_valid_2=np.sum(Sum_valid_1,axis=0)  \n",
        "            Mean_valid=Sum_valid_2/(len(Validation_Error)*MiniBatches_size*num_periods_output)\n",
        "            print(\"\\tRMSE Validation:\", (Mean_valid)**0.5)\n",
        "            validation_mse.append((Mean_valid)**0.5)\n",
        "\n",
        "    print('Fitting Done!')\n",
        "    ################################################# Testing  ###########################################\n",
        "    Testing_Error=[]\n",
        "    All_Testing_Instances_batches,Y_Test_Full_batches=batch_features_labels(All_Testing_Instances,Y_Test_Full,MiniBatches_size)\n",
        "    for i in range(0,len(All_Testing_Instances_batches)): \n",
        "          y_predict,Test_error_batch=sess.run([outputs,Error], \n",
        "                                                            feed_dict={\n",
        "                                                            keep_prob:keep_prob_testval,\n",
        "                                                            X: All_Testing_Instances_batches[i], \n",
        "                                                            y: Y_Test_Full_batches[i]\n",
        "                                                            })\n",
        "          Testing_Error.append(Test_error_batch)\n",
        "    Sum_test=np.sum(Testing_Error,axis=2)\n",
        "    Sum_test_1=np.sum(Sum_test,axis=1)\n",
        "    Sum_test_2=np.sum(Sum_test_1,axis=0)  \n",
        "    Mean_test=Sum_test_2/(len(Testing_Error)*MiniBatches_size*num_periods_output)\n",
        "    print(\"\\tRMSE Testing:\", (Mean_test)**0.5)\n",
        "    #testing_mse.append((Mean_test)**0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125168\n",
            "125168\n",
            "24\n",
            "(15656, 24)\n",
            "epoch: 1\n",
            "\tRMSE Training: 21.46665709427713\n",
            "\tRMSE Validation: 13.332472357906587\n",
            "epoch: 4\n",
            "\tRMSE Training: 14.80789799623633\n",
            "\tRMSE Validation: 12.077737707182887\n",
            "epoch: 7\n",
            "\tRMSE Training: 13.060747060497658\n",
            "\tRMSE Validation: 11.36528149955815\n",
            "epoch: 10\n",
            "\tRMSE Training: 12.155140415667363\n",
            "\tRMSE Validation: 10.929863481691816\n",
            "epoch: 13\n",
            "\tRMSE Training: 11.593360753298244\n",
            "\tRMSE Validation: 10.626217392676683\n",
            "epoch: 16\n",
            "\tRMSE Training: 11.19293263400855\n",
            "\tRMSE Validation: 10.39620132227902\n",
            "epoch: 19\n",
            "\tRMSE Training: 10.904146781209981\n",
            "\tRMSE Validation: 10.223899003556197\n",
            "epoch: 22\n",
            "\tRMSE Training: 10.671014224767777\n",
            "\tRMSE Validation: 10.078625141815342\n",
            "epoch: 25\n",
            "\tRMSE Training: 10.483538032178409\n",
            "\tRMSE Validation: 9.961330613205218\n",
            "epoch: 28\n",
            "\tRMSE Training: 10.329075134317055\n",
            "\tRMSE Validation: 9.86352367480917\n",
            "epoch: 31\n",
            "\tRMSE Training: 10.202072667305028\n",
            "\tRMSE Validation: 9.785227366130387\n",
            "epoch: 34\n",
            "\tRMSE Training: 10.090436549312699\n",
            "\tRMSE Validation: 9.715298220031695\n",
            "epoch: 37\n",
            "\tRMSE Training: 9.99208490847607\n",
            "\tRMSE Validation: 9.6512446811671\n",
            "epoch: 40\n",
            "\tRMSE Training: 9.904330619851217\n",
            "\tRMSE Validation: 9.600299937131007\n",
            "epoch: 43\n",
            "\tRMSE Training: 9.82784928419892\n",
            "\tRMSE Validation: 9.547386113434358\n",
            "epoch: 46\n",
            "\tRMSE Training: 9.759853322328496\n",
            "\tRMSE Validation: 9.498363539492058\n",
            "epoch: 49\n",
            "\tRMSE Training: 9.697021311472692\n",
            "\tRMSE Validation: 9.454077604844384\n",
            "epoch: 52\n",
            "\tRMSE Training: 9.637166952181989\n",
            "\tRMSE Validation: 9.413427033463877\n",
            "epoch: 55\n",
            "\tRMSE Training: 9.585075437241251\n",
            "\tRMSE Validation: 9.375309316905765\n",
            "epoch: 58\n",
            "\tRMSE Training: 9.537413744247534\n",
            "\tRMSE Validation: 9.342370545554278\n",
            "epoch: 61\n",
            "\tRMSE Training: 9.493542622141064\n",
            "\tRMSE Validation: 9.313076824807217\n",
            "epoch: 64\n",
            "\tRMSE Training: 9.452857205595842\n",
            "\tRMSE Validation: 9.28354994032092\n",
            "epoch: 67\n",
            "\tRMSE Training: 9.414863796414275\n",
            "\tRMSE Validation: 9.25630442850274\n",
            "epoch: 70\n",
            "\tRMSE Training: 9.37965237151466\n",
            "\tRMSE Validation: 9.236312300657309\n",
            "epoch: 73\n",
            "\tRMSE Training: 9.345521874150853\n",
            "\tRMSE Validation: 9.212692059579918\n",
            "epoch: 76\n",
            "\tRMSE Training: 9.314539728034076\n",
            "\tRMSE Validation: 9.192247543539816\n",
            "epoch: 79\n",
            "\tRMSE Training: 9.285335492700735\n",
            "\tRMSE Validation: 9.176247280832731\n",
            "epoch: 82\n",
            "\tRMSE Training: 9.257319850349523\n",
            "\tRMSE Validation: 9.156139967696907\n",
            "epoch: 85\n",
            "\tRMSE Training: 9.231344832625222\n",
            "\tRMSE Validation: 9.13548057720741\n",
            "epoch: 88\n",
            "\tRMSE Training: 9.206811003407152\n",
            "\tRMSE Validation: 9.118206635912733\n",
            "epoch: 91\n",
            "\tRMSE Training: 9.182526553787199\n",
            "\tRMSE Validation: 9.1011051048966\n",
            "epoch: 94\n",
            "\tRMSE Training: 9.160961801965833\n",
            "\tRMSE Validation: 9.091004405284119\n",
            "epoch: 97\n",
            "\tRMSE Training: 9.13882248686325\n",
            "\tRMSE Validation: 9.074848410313614\n",
            "epoch: 100\n",
            "\tRMSE Training: 9.118032934255279\n",
            "\tRMSE Validation: 9.059904429087213\n",
            "epoch: 103\n",
            "\tRMSE Training: 9.098773601100712\n",
            "\tRMSE Validation: 9.04603370210561\n",
            "epoch: 106\n",
            "\tRMSE Training: 9.080897661599732\n",
            "\tRMSE Validation: 9.034464319902686\n",
            "epoch: 109\n",
            "\tRMSE Training: 9.062917875779371\n",
            "\tRMSE Validation: 9.022306359393312\n",
            "epoch: 112\n",
            "\tRMSE Training: 9.045635436110238\n",
            "\tRMSE Validation: 9.01127932214056\n",
            "epoch: 115\n",
            "\tRMSE Training: 9.029684948198392\n",
            "\tRMSE Validation: 9.000341660011475\n",
            "epoch: 118\n",
            "\tRMSE Training: 9.014776571202036\n",
            "\tRMSE Validation: 8.988312666205248\n",
            "epoch: 121\n",
            "\tRMSE Training: 8.999663167912978\n",
            "\tRMSE Validation: 8.97656487722051\n",
            "epoch: 124\n",
            "\tRMSE Training: 8.984980955910345\n",
            "\tRMSE Validation: 8.965078134208435\n",
            "epoch: 127\n",
            "\tRMSE Training: 8.970770390414094\n",
            "\tRMSE Validation: 8.954652452457431\n",
            "epoch: 130\n",
            "\tRMSE Training: 8.956920741634425\n",
            "\tRMSE Validation: 8.944642083562629\n",
            "epoch: 133\n",
            "\tRMSE Training: 8.943539202304986\n",
            "\tRMSE Validation: 8.934752599303808\n",
            "epoch: 136\n",
            "\tRMSE Training: 8.931559603515005\n",
            "\tRMSE Validation: 8.925247521222456\n",
            "epoch: 139\n",
            "\tRMSE Training: 8.919596413562607\n",
            "\tRMSE Validation: 8.916370265858596\n",
            "epoch: 142\n",
            "\tRMSE Training: 8.90848678317204\n",
            "\tRMSE Validation: 8.907999020452952\n",
            "epoch: 145\n",
            "\tRMSE Training: 8.896590980379441\n",
            "\tRMSE Validation: 8.900019171334023\n",
            "epoch: 148\n",
            "\tRMSE Training: 8.885634623277962\n",
            "\tRMSE Validation: 8.891732116641732\n",
            "epoch: 151\n",
            "\tRMSE Training: 8.874878658999046\n",
            "\tRMSE Validation: 8.883771422785596\n",
            "epoch: 154\n",
            "\tRMSE Training: 8.864847683306767\n",
            "\tRMSE Validation: 8.875866239187317\n",
            "epoch: 157\n",
            "\tRMSE Training: 8.855341152727506\n",
            "\tRMSE Validation: 8.86835215780168\n",
            "epoch: 160\n",
            "\tRMSE Training: 8.845026542386355\n",
            "\tRMSE Validation: 8.860694652574992\n",
            "epoch: 163\n",
            "\tRMSE Training: 8.834852080500308\n",
            "\tRMSE Validation: 8.853056038569193\n",
            "epoch: 166\n",
            "\tRMSE Training: 8.825407273847429\n",
            "\tRMSE Validation: 8.845678475072408\n",
            "epoch: 169\n",
            "\tRMSE Training: 8.816090150005806\n",
            "\tRMSE Validation: 8.839134969134694\n",
            "epoch: 172\n",
            "\tRMSE Training: 8.807121161257289\n",
            "\tRMSE Validation: 8.832323663307804\n",
            "epoch: 175\n",
            "\tRMSE Training: 8.798612923824658\n",
            "\tRMSE Validation: 8.826140219560182\n",
            "epoch: 178\n",
            "\tRMSE Training: 8.790327779269566\n",
            "\tRMSE Validation: 8.819688127671869\n",
            "epoch: 181\n",
            "\tRMSE Training: 8.78285341011042\n",
            "\tRMSE Validation: 8.813461807186817\n",
            "epoch: 184\n",
            "\tRMSE Training: 8.775135299516911\n",
            "\tRMSE Validation: 8.810199900126342\n",
            "epoch: 187\n",
            "\tRMSE Training: 8.767353808123673\n",
            "\tRMSE Validation: 8.805316932270008\n",
            "epoch: 190\n",
            "\tRMSE Training: 8.759871788629988\n",
            "\tRMSE Validation: 8.799620476990196\n",
            "epoch: 193\n",
            "\tRMSE Training: 8.7525835171861\n",
            "\tRMSE Validation: 8.795691362649729\n",
            "epoch: 196\n",
            "\tRMSE Training: 8.745646449966582\n",
            "\tRMSE Validation: 8.791428647427578\n",
            "epoch: 199\n",
            "\tRMSE Training: 8.738520492392801\n",
            "\tRMSE Validation: 8.786115314594458\n",
            "epoch: 202\n",
            "\tRMSE Training: 8.731508854459648\n",
            "\tRMSE Validation: 8.781646823370846\n",
            "epoch: 205\n",
            "\tRMSE Training: 8.725035787903677\n",
            "\tRMSE Validation: 8.777394402965168\n",
            "epoch: 208\n",
            "\tRMSE Training: 8.718690229681483\n",
            "\tRMSE Validation: 8.772737327226132\n",
            "epoch: 211\n",
            "\tRMSE Training: 8.712366487424843\n",
            "\tRMSE Validation: 8.767823235702863\n",
            "epoch: 214\n",
            "\tRMSE Training: 8.70637768885885\n",
            "\tRMSE Validation: 8.763253354111432\n",
            "epoch: 217\n",
            "\tRMSE Training: 8.700475636207502\n",
            "\tRMSE Validation: 8.759050239652188\n",
            "epoch: 220\n",
            "\tRMSE Training: 8.694938172645632\n",
            "\tRMSE Validation: 8.754602256275186\n",
            "epoch: 223\n",
            "\tRMSE Training: 8.6893104869693\n",
            "\tRMSE Validation: 8.752264999041248\n",
            "epoch: 226\n",
            "\tRMSE Training: 8.684150296191653\n",
            "\tRMSE Validation: 8.748587385940592\n",
            "epoch: 229\n",
            "\tRMSE Training: 8.678856350104896\n",
            "\tRMSE Validation: 8.7448132947941\n",
            "epoch: 232\n",
            "\tRMSE Training: 8.673622550127282\n",
            "\tRMSE Validation: 8.740900715509321\n",
            "epoch: 235\n",
            "\tRMSE Training: 8.668534092676426\n",
            "\tRMSE Validation: 8.736938694438011\n",
            "epoch: 238\n",
            "\tRMSE Training: 8.663599197173895\n",
            "\tRMSE Validation: 8.733125269359034\n",
            "epoch: 241\n",
            "\tRMSE Training: 8.658671524396745\n",
            "\tRMSE Validation: 8.729614291532478\n",
            "epoch: 244\n",
            "\tRMSE Training: 8.653760744591622\n",
            "\tRMSE Validation: 8.725839507346622\n",
            "epoch: 247\n",
            "\tRMSE Training: 8.64892084445414\n",
            "\tRMSE Validation: 8.722866658441683\n",
            "epoch: 250\n",
            "\tRMSE Training: 8.644561131628947\n",
            "\tRMSE Validation: 8.720030221771584\n",
            "epoch: 253\n",
            "\tRMSE Training: 8.64017363761467\n",
            "\tRMSE Validation: 8.717196091440776\n",
            "epoch: 256\n",
            "\tRMSE Training: 8.635825648491727\n",
            "\tRMSE Validation: 8.713907073951274\n",
            "epoch: 259\n",
            "\tRMSE Training: 8.631408526825206\n",
            "\tRMSE Validation: 8.711066130473801\n",
            "epoch: 262\n",
            "\tRMSE Training: 8.627233617342675\n",
            "\tRMSE Validation: 8.707827604654467\n",
            "epoch: 265\n",
            "\tRMSE Training: 8.623026034851787\n",
            "\tRMSE Validation: 8.704794128774866\n",
            "epoch: 268\n",
            "\tRMSE Training: 8.61892604348449\n",
            "\tRMSE Validation: 8.702402942614444\n",
            "epoch: 271\n",
            "\tRMSE Training: 8.614662443145175\n",
            "\tRMSE Validation: 8.699240533223694\n",
            "epoch: 274\n",
            "\tRMSE Training: 8.610344823069461\n",
            "\tRMSE Validation: 8.696508661630492\n",
            "epoch: 277\n",
            "\tRMSE Training: 8.606159603061819\n",
            "\tRMSE Validation: 8.693416096721407\n",
            "epoch: 280\n",
            "\tRMSE Training: 8.602463939557186\n",
            "\tRMSE Validation: 8.691022659423513\n",
            "epoch: 283\n",
            "\tRMSE Training: 8.598694076722312\n",
            "\tRMSE Validation: 8.688530581523787\n",
            "epoch: 286\n",
            "\tRMSE Training: 8.595257339780776\n",
            "\tRMSE Validation: 8.685651921449535\n",
            "epoch: 289\n",
            "\tRMSE Training: 8.591485406778348\n",
            "\tRMSE Validation: 8.6837345878432\n",
            "epoch: 292\n",
            "\tRMSE Training: 8.587892881668761\n",
            "\tRMSE Validation: 8.681033480786905\n",
            "epoch: 295\n",
            "\tRMSE Training: 8.58427810060197\n",
            "\tRMSE Validation: 8.678274537727091\n",
            "epoch: 298\n",
            "\tRMSE Training: 8.580865247485033\n",
            "\tRMSE Validation: 8.675593124945454\n",
            "epoch: 301\n",
            "\tRMSE Training: 8.577440266645763\n",
            "\tRMSE Validation: 8.673134599428677\n",
            "epoch: 304\n",
            "\tRMSE Training: 8.574055612416375\n",
            "\tRMSE Validation: 8.670617901888146\n",
            "epoch: 307\n",
            "\tRMSE Training: 8.570516965621673\n",
            "\tRMSE Validation: 8.668072170430527\n",
            "epoch: 310\n",
            "\tRMSE Training: 8.567365593751253\n",
            "\tRMSE Validation: 8.666002449619688\n",
            "epoch: 313\n",
            "\tRMSE Training: 8.563811004373367\n",
            "\tRMSE Validation: 8.663422220304712\n",
            "epoch: 316\n",
            "\tRMSE Training: 8.560684062255254\n",
            "\tRMSE Validation: 8.660955706507712\n",
            "epoch: 319\n",
            "\tRMSE Training: 8.557648352459758\n",
            "\tRMSE Validation: 8.658633306174586\n",
            "epoch: 322\n",
            "\tRMSE Training: 8.55466568165121\n",
            "\tRMSE Validation: 8.65623688967149\n",
            "epoch: 325\n",
            "\tRMSE Training: 8.551497684755283\n",
            "\tRMSE Validation: 8.654438264586586\n",
            "epoch: 328\n",
            "\tRMSE Training: 8.5485852831056\n",
            "\tRMSE Validation: 8.65221923062738\n",
            "epoch: 331\n",
            "\tRMSE Training: 8.545550357821654\n",
            "\tRMSE Validation: 8.649866218034171\n",
            "epoch: 334\n",
            "\tRMSE Training: 8.542825535651966\n",
            "\tRMSE Validation: 8.648890296343849\n",
            "epoch: 337\n",
            "\tRMSE Training: 8.539692208372276\n",
            "\tRMSE Validation: 8.646721353492557\n",
            "epoch: 340\n",
            "\tRMSE Training: 8.536870512576918\n",
            "\tRMSE Validation: 8.644996715116385\n",
            "epoch: 343\n",
            "\tRMSE Training: 8.533976391277761\n",
            "\tRMSE Validation: 8.642887426271813\n",
            "epoch: 346\n",
            "\tRMSE Training: 8.530925546063486\n",
            "\tRMSE Validation: 8.640742861880776\n",
            "epoch: 349\n",
            "\tRMSE Training: 8.528084902681732\n",
            "\tRMSE Validation: 8.63914582879681\n",
            "epoch: 352\n",
            "\tRMSE Training: 8.525315429174164\n",
            "\tRMSE Validation: 8.637048341382238\n",
            "epoch: 355\n",
            "\tRMSE Training: 8.522575631217672\n",
            "\tRMSE Validation: 8.634954398385124\n",
            "epoch: 358\n",
            "\tRMSE Training: 8.519917171728894\n",
            "\tRMSE Validation: 8.632814170121408\n",
            "epoch: 361\n",
            "\tRMSE Training: 8.51723544218873\n",
            "\tRMSE Validation: 8.63085679481351\n",
            "epoch: 364\n",
            "\tRMSE Training: 8.514519955996725\n",
            "\tRMSE Validation: 8.628667908340248\n",
            "epoch: 367\n",
            "\tRMSE Training: 8.511927173986157\n",
            "\tRMSE Validation: 8.626640116875485\n",
            "epoch: 370\n",
            "\tRMSE Training: 8.50922423379449\n",
            "\tRMSE Validation: 8.624644247408737\n",
            "epoch: 373\n",
            "\tRMSE Training: 8.506693960094488\n",
            "\tRMSE Validation: 8.622904603086305\n",
            "epoch: 376\n",
            "\tRMSE Training: 8.504490425669388\n",
            "\tRMSE Validation: 8.621037384463206\n",
            "epoch: 379\n",
            "\tRMSE Training: 8.502035048638653\n",
            "\tRMSE Validation: 8.619193565556946\n",
            "epoch: 382\n",
            "\tRMSE Training: 8.499574628241225\n",
            "\tRMSE Validation: 8.61731166831894\n",
            "epoch: 385\n",
            "\tRMSE Training: 8.497252913467872\n",
            "\tRMSE Validation: 8.615690930157268\n",
            "epoch: 388\n",
            "\tRMSE Training: 8.494853329912585\n",
            "\tRMSE Validation: 8.614069540754922\n",
            "epoch: 391\n",
            "\tRMSE Training: 8.492459461572682\n",
            "\tRMSE Validation: 8.612464744637887\n",
            "epoch: 394\n",
            "\tRMSE Training: 8.490390118404948\n",
            "\tRMSE Validation: 8.61066334594126\n",
            "epoch: 397\n",
            "\tRMSE Training: 8.487899127442434\n",
            "\tRMSE Validation: 8.609088013013295\n",
            "epoch: 400\n",
            "\tRMSE Training: 8.485548628057332\n",
            "\tRMSE Validation: 8.60737706369304\n",
            "Fitting Done!\n",
            "\tRMSE Testing: 9.150025798242067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffvx1P9i34te",
        "colab_type": "code",
        "outputId": "e461314f-f20b-4b3c-be13-e124d0aa7f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "Train_draw=[]\n",
        "Valid_draw=[]\n",
        "for i in range(0,len(train_mse)):\n",
        "  if i%1==0:\n",
        "    Train_draw.append(train_mse[i])\n",
        "    Valid_draw.append(validation_mse[i])\n",
        "\n",
        "plt.title(\"Training vs. Validation Error\", fontsize=14)\n",
        "#plt.legend(loc=\"lower right\")\n",
        "plt.plot(Train_draw[1:], markersize=10, label=\"Training Error\",color='red')\n",
        "plt.plot(Valid_draw[1:], markersize=10, label=\"Validation Error\",color='blue')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig('Training vs Testing Error.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEJCAYAAACNNHw2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU9b3/8dcnJCEQCElYDYjgxhYW\nMUW9CIi4ULVQ3KpXizvqbdXq7UKtrbb311atV9Ha2tbd1kK9Wpe61lIsWkULiIACQgU1bAlgWMMS\n+P7++J6EyTCTjcnMnOT9fDzO48zZP3My+cx3vud7vsecc4iISPhkpDoAERFpGiVwEZGQUgIXEQkp\nJXARkZBSAhcRCSklcBGRkFICb+HMbIaZPd3IbeaY2V3NFVMYmdnTZjYj3nScbd4ws/sTfWyRakrg\nKWZmrp7hsYM8xNXAlY3c5gzgxwd53JQys3ZmVmFm34mz/EYz22JmuU08xJX4c5swZnZW8DfPae5j\nxTn+3DifwYea+9jSNJmpDkA4JOL1WcCDUfMqY21kZlnOuT317dw5t7mxATnnNjV2m3TjnKs0s+nA\nZcAvYqxyBTDDObe9ifuvOJj40vVYwP3AT6Pm7Yi3cqzPoZm1AZxzbl9jD97Qz7V4KoGnmHNuXfUA\nVETPc85tNrP+QUnoPDP7h5ntBC4xs+5m9iczW21mO8xssZldFLn/6CqUoHrkHjP7hZltMrN1ZvZz\nM7Oode6KmF5nZt8zs0fMbKuZfW5m10cdZ6CZ/dPMdprZR2Z2qplVmdkFsd63mU0ws0ozy4uaf7eZ\nvRe87mxmfzSz8mC/K8zs2kac3oeAAWZ2QtQxjgMGBcsxs0PM7KmI87jIzC6sa8cxqlTygli3m9ka\nM7sxxjZXmtl8M9sWnNMnzaxbsKwY+EuwamXw974/zrHam9kDEeflLTP7UsTy6pL86OB4O8zsHTMb\n1IBztj3q87fOObelOsZgv+eY2ZvB5/AiM/tm8H7ONrMlwC6gt5m1MbP/F5zXXWb2vpmNj4gz5v4a\nEKMElMDD5XbgHmAA8DLQDpgDnAkUAw8Aj5vZifXs53JgM3Ac8N/Ad4Gv1rPNt4H3gGOAe4F7zWw4\ngJllAs8DW4ERwBTgZ9T9+XoF2A6cXT3DzDKAC4A/RLzfo4AvA/2D/a6vJ84azrl5wAf49xvpCmCR\nc+69YLo98Db+PA4Gfgf83syOb+ix8CXXE4CvAKcDJwPDo9bJBL4PDAEmAX2Bx4NlS4GvB6/74H+F\nfT/OsX4ZxHoRcCywEnjNzAqj1vspcANQAuwBnmjE+6nLHcBd+M/ha8G8fOBG/C+eYqAMuBn4L+Bb\n+Pc8E3jBzI5uwP6kIZxzGtJkAM71f5ID5vcHHPCNBuzjOeD+iOkZwNMR03OAWVHbvBm1zRzgrojp\ndcCjUdt8Dnw7eD0R2A10i1h+chDzBXXE+mvg9YjpU/CJplsw/VfggYM8p98EtgC5wXT7YPqGerZ7\nEZgWMf00vsrlgGmgK1AFTIxYXoiveri/jmOUBOeoUzB9VjCdE7Ve5LG6AHuBsyOWZwNrgKlR+xkZ\nsc7pwbz8OuKZG/wdt0UNlwTLi4N9XB3jHDugX8Q8w/+ivCnGMX5T1/40NHxQCTxc5kZOmFmmmd0a\n/OTfZGbb8CWz3vXsZ2HU9Bqg20Fs0x9Y5Zwri1j+bj37A1/SPtnMquv8L8In9Or9/Aq4NPjpfWcD\nflnEO0YWcF4wfR4+4VWX8jGzLDP7cdR5PJ36z2O1o4E2wDvVM5y/jrAsciUzO97MXjKzz8xsK/CP\nYNFhjXg/R+N/2fwz4li78b+OBkatG/k3WxOM6/s7PwIMixqejVpnbvRGwDbnXOT7PQToFBln4K0Y\nccbanzSAEni4RF9w+wHwDeDnwFj8P9vL+ARVl+iLRI76PwtN2aZOzrm3gVXABeZbXpxNRGJ1zj2P\nT2734hPCa2b2QCOPUQH8mf3VKFcAzznnNkas9iPgGny1T/V5fI36z2ODBdUbr+KrFv4TX/qeFCxO\n1HGiuxbdE2NZfX+zCufciqhhS9Q6sS78NuZicHScTbqQLErgYXci8Kxz7o/OuQ+AT/AltGRbChxm\nZl0j5o1o4LZP4kveX8F/Hp+LXOicK3POPeac+zq+PvXKoK68MR4CRpnZmcCoYDrSifhqpunBeVxJ\n487jx8A+oKbO3MwKovYxGF8i/a5z7q2gtNojaj+7g3Gbeo61FxgZcaxs/Pn+qBExN7e1+OssI6Pm\nn0h6xRlqakYYbh8DZwatLCqAm4Ai4NMkx/ES8Bn+AupUoCP+AqTjwNJWtD8APwRuwX8Z1TRZM7Of\n4evjPwLa4i+0LnNB87SgZcZW59xV9RzjDeDfwO/xJf6ZUcs/BsYHFy234C/Ydg/m18s5V25mfwTu\nMbMtwAZ8aT6yGd1KfD359Wb2CDA0eM+RVgXjs8zs78AOF9XM0Tm3wfy9AfcE1TCr8Rc7c/BNUA9W\nrplFf7Hscs590ZidOOec+ZZMt5jZZ8AifHv2IfhfIJIAKoGH2634es7X8UmqDH/BK6mcc1X4C5n5\nwL/wJdyfBIt31rPtx8E2Q4ioPgnswbdQWAjMxpdMz45Y3gc4tAHxOXzdbgHwSDAd6Yf4L4m/AX8H\nSon6JdAA3wzex8v4v8c/gHkRMXyGT2BfB5YA38G3AIqM82P8F999+NY2d8Q51g3BcaYD8/GtWU53\niWm//0186TlymN7Eff0c3zLqPnwCPwWYELxPSQA78LMscvCCttZzgGLn3IepjkekJVICl4Qws/OA\nL4AVwBHANHwVwHEpDUykBVMduCRKJ/xP5l7ARnw9800pjUikhVMJXEQkpHQRU0QkpJJahdKlSxfX\np0+fZB5SRCT05s2bt8E51zV6fr0JPGizehZQ5pwrDubdBlwFlAer3eyce7m+ffXp04e5c3XXrIhI\nY5hZzHs7GlKF8hgwPsb8e5xzw4Kh3uQtIiKJVW8Cd87NBkLfwb+ISEtzMBcxv2lmC8138l8QbyUz\nm2L+UU1zy8vL460mIiKN1KBmhGbWB3gxog68O76/Bwf8D3CIcy660/wDlJSUONWBizSPPXv2UFpa\nys6ddfZeIGksJyeHXr16kZWVVWu+mc1zzpVEr9+kVijOuZqnopjZg/jO70UkhUpLS+nYsSN9+vTB\n9j8hT0LCOcfGjRspLS2lb9++DdqmSVUoER3wg+/TeHFT9iMiibNz5046d+6s5B1SZkbnzp0b9Quq\nIc0IpwMnAV3MrBTfA95JZjYMX4WyCri6KQGLSGIpeYdbY/9+9SZw51ysp3M/3KijHKwXX4TFi2Hq\n1KQeVkQknYXjVvrXXoM770x1FCJSh40bNzJs2DCGDRtGjx496NmzZ8307t27698BcNlll7Fs2bI6\n1/nVr37Fk08+mYiQOfHEE+nXr19NnF/72tcSst9kCUdvhO3bw3Y9Nk8knXXu3JkFCxYAcNttt9Gh\nQwe+/e1v11qn5mnqGbHLjo8++mi9x/nGN75x8MFG+NOf/sSwYcPiLq+qqiIzMzPudEO3aw7hKIHn\n5sLu3VBVlepIRKSRVqxYwcCBA7nooosYNGgQa9euZcqUKZSUlDBo0CB+8pOf1Kx74oknsmDBAqqq\nqsjPz2fq1KkMHTqUE044gbKyMgBuueUWpk2bVrP+1KlTGTFiBP369ePtt98GYPv27ZxzzjkMHDiQ\nc889l5KSkpovl4a4+OKLufbaaxkxYgQ333wzt9xyC5MnT2bkyJFceumlVFZWcskllzB48GCGDx/O\n7NmzAXjooYf46le/ytixYzn99NMTdQrjCkcJPDfXj7dvh06dUhuLSBh861vQiITVIMOGQZA4G2vp\n0qU88cQTlJT4psy33347hYWFVFVVMXbsWM4991wGDhxYa5vNmzczZswYbr/9dm666SYeeeQRpsa4\nDuac47333uOFF17gJz/5Ca+++iq//OUv6dGjB8888wwffPABw4cPjxvb1772Ndq1awfA+PHjuf32\n2wFYu3Ytc+bMISMjg1tuuYWlS5cye/ZscnJyuOOOO2jbti2LFi3iww8/5IwzzmD58uUAvP/++yxY\nsICCgrj3NyZMuBL4jh1K4CIhdMQRR9Qkb4Dp06fz8MMPU1VVxZo1a/joo48OSODt2rXjy1/+MgDH\nHnssb775Zsx9n3322TXrrFq1CoC33nqL733vewAMHTqUQYMGxY0tXhXKeeedV6uqZ+LEieTk5NTs\n/zvf+Q4AgwYNoqioiBUrVgBw2mmnJSV5Q9gSuOrBRRqmiSXl5pJb/T8MLF++nHvvvZf33nuP/Px8\nLr744phtn7Ozs2tet2nThqo4Vaht27atd52DjTnWdEO3a07hqQMHJXCRFmDLli107NiRvLw81q5d\ny2uvvZbwY4wcOZKnnnoKgEWLFvHRRx8ldP+jRo2qaQmzZMkS1q5dy5FHHpnQYzREOErg7dv7sRK4\nSOgNHz6cgQMH0r9/fw477DBGjhyZ8GNcd911TJ48mYEDB9YMneJUv0bWgXfv3r1BXyjXXXcdV199\nNYMHDyYrK4snnnii1i+GZEnqMzGb3JnVm2/C6NHw17/CqacmPjCRFmDJkiUMGDAg1WGkhaqqKqqq\nqsjJyWH58uWcdtppLF++vNmb9SVCrL9jQjuzSrrIi5giIvXYtm0b48aNo6qqCuccv/3tb0ORvBsr\nHO9IdeAi0gj5+fnMmzcv1WE0O13EFBEJqXAkcF3EFBE5QDgSuErgIiIHCEcCz86GNm10EVNEJEI4\nEriZL4WrBC6StsaOHXtAG+pp06Zx7bXX1rldhw4dAFizZg3nnntuzHVOOukk6muCPG3aNHZEFPLO\nOOMMKioqGhJ6nW677bZaXeMOGzYsIftNhHAkcFACF0lzF154ITNmzKg1b8aMGVx4YaxnwhyoqKiI\np59+usnHj07gL7/8Mvn5+U3eX6Qbb7yRBQsW1AzR+42+hb+ht/Q759i3b1+T4wpPAlef4CJp7dxz\nz+Wll16qeXjDqlWrWLNmDaNGjapplz18+HAGDx7M888/f8D2q1atori4GIDKykouuOACBgwYwKRJ\nk6isrKxZ79prr63pivbWW28F4L777mPNmjWMHTuWsWPHAtCnTx82bNgAwN13301xcTHFxcU1XdGu\nWrWKAQMGcNVVVzFo0CBOO+20Wsepz2OPPcaECRM4+eSTGTduHG+88QajRo1iwoQJNR1zxTtuv379\nmDx5MsXFxXz++eeNOs+RwtEOHFQCF2mEVPQmW1hYyIgRI3jllVeYOHEiM2bM4Pzzz8fMyMnJ4dln\nnyUvL48NGzZw/PHHM2HChLjPgHzggQdo3749S5YsYeHChbW6g/3pT39KYWEhe/fuZdy4cSxcuJDr\nr7+eu+++m1mzZtGlS5da+5o3bx6PPvoo7777Ls45jjvuOMaMGUNBQQHLly9n+vTpPPjgg5x//vk8\n88wzXHzxxQfEc8899/CHP/wBgIKCAmbNmgXA/PnzWbhwIYWFhbzxxhvMnz+fxYsX07dv33qP+/jj\nj3P88cc39s9QS3hK4Lm5uogpkuYiq1Eiq0+cc9x8880MGTKEU045hdWrV7N+/fq4+5k9e3ZNIh0y\nZAhDhgypWfbUU08xfPhwjjnmGD788MN6O6p66623mDRpErm5uXTo0IGzzz67pmvavn371nQlG9kd\nbbTIKpTq5A1w6qmnUlhYWDM9YsQI+vbtW+9xDzvssINO3qASuEiLlKreZCdOnMiNN97I/Pnz2bFj\nB8ceeywATz75JOXl5cybN4+srCz69OkTswvZ+qxcuZK77rqLf/3rXxQUFHDppZc2aT/VqruiBd8d\nbWOqUCD1Xc6GqwSuBC6S1jp06MDYsWO5/PLLa1283Lx5M926dSMrK4tZs2bx6aef1rmf0aNH88c/\n/hGAxYsXs3DhQsB3RZubm0unTp1Yv349r7zySs02HTt2ZOvWrQfsa9SoUTz33HPs2LGD7du38+yz\nzzJq1KhEvN06JeO49ZbAzewR4CygzDlXHLXsv4G7gK7OuQ0JjSyaErhIKFx44YVMmjSpVouUiy66\niK985SsMHjyYkpIS+vfvX+c+rr32Wi677DIGDBjAgAEDakryQ4cO5ZhjjqF///4ceuihtbqinTJl\nCuPHj6eoqKhWNcfw4cO59NJLGTFiBABXXnklxxxzTNzqklgi68ABnnvuuXq3ScRx61Nvd7JmNhrY\nBjwRmcDN7FDgIaA/cGxDEniTu5MFuOoqePFFWLu2aduLtHDqTrZlaEx3svVWoTjnZgObYiy6B/gu\nkJwOxXURU0SklibVgZvZRGC1c+6DBMcTX3UVShIfQCEiks4a3QrFzNoDNwOnNXD9KcAUgN69ezf2\ncPvl5sLevbB7N0RcORaR/ZxzcdtWS/pr7BPSmlICPwLoC3xgZquAXsB8M+sRJ6DfOedKnHMlXbt2\nbcLhAuqRUKROOTk5bNy4sdFJQNKDc46NGzeSk5PT4G0aXQJ3zi0CulVPB0m8pNlboUT2CR7RcF5E\nvF69elFaWkp5eXmqQ5EmysnJoVevXg1evyHNCKcDJwFdzKwUuNU593CTI2wqPRdTpE5ZWVk1dwFK\n61BvAnfO1dmVmHOuT8KiqYuqUEREagnXnZigBC4iElACFxEJKSVwEZGQCk8Cr26FoouYIiJAmBK4\nSuAiIrUogYuIhJQSuIhISIUngbdp4/tAUQIXEQHClMDBX8jURUwRESBsCVxP5RERqaEELiISUkrg\nIiIhpQQuIhJS4UrguogpIlIjXAlcJXARkRpK4CIiIaUELiISUkrgIiIhFb4EvmMH6KnbIiIhS+DV\nfYJXVqY2DhGRNBCuBK4eCUVEaiiBi4iElBK4iEhI1ZvAzewRMyszs8UR8/7HzBaa2QIz+6uZFTVv\nmAElcBGRGg0pgT8GjI+a9wvn3BDn3DDgReBHiQ4spvx8P/7ii6QcTkQkndWbwJ1zs4FNUfO2REzm\nAslp19e1qx+XlyflcCIi6SyzqRua2U+BycBmYGwd600BpgD07t27qYfzlMBFRGo0+SKmc+4HzrlD\ngSeBb9ax3u+ccyXOuZKu1Qm4qfLzITNTCVxEhMS0QnkSOCcB+6mfGXTpAhs2JOVwIiLprEkJ3MyO\nipicCCxNTDgN0LWrSuAiIjSgDtzMpgMnAV3MrBS4FTjDzPoB+4BPgWuaM8haunRRAhcRoQEJ3Dl3\nYYzZDzdDLA3TtSssWJCyw4uIpItw3YkJqkIREQmEM4F/8QXs2ZPqSEREUiqcCRxg48bUxiEikmLh\nTeCqRhGRVi58CbxLFz9WW3ARaeXCl8BVAhcRAZTARURCK3wJvHNnP1YCF5FWLnwJPDMTCguVwEWk\n1QtfAgfdzCMighK4iEhohTOBq0tZEZGQJnCVwEVEQpzAN2yAfftSHYmISMqEN4Hv3QsVFamOREQk\nZcKbwEHVKCLSqimBi4iEVDgTeHWHVkrgItKKhTOBV5fA1ZRQRFqxcCdwlcBFpBULZwLPyYH8fFi9\nOtWRiIikTCgS+KZNsGxZ1My+fWHlypTEIyKSDupN4Gb2iJmVmdniiHm/MLOlZrbQzJ41s/zmDHLq\nVBg9OmqmEriItHINKYE/BoyPmvc6UOycGwJ8DHw/wXHV0rMnlJVFPYj+8MN9AtfdmCLSStWbwJ1z\ns4FNUfP+6pyrCibnAL2aIbYaRUV+vHZtxMzDD4ddu2DduuY8tIhI2kpEHfjlwCvxFprZFDOba2Zz\ny5vYaqRnTz9esyZiZt++fvzJJ03ap4hI2B1UAjezHwBVwJPx1nHO/c45V+KcK+la3fyvkapL4LUa\nnRx+uB8rgYtIK5XZ1A3N7FLgLGCcc84lLKIYYpbADzsMzHQhU0RarSYlcDMbD3wXGOOc25HYkA7U\nuTNkZUWVwNu29ZldJXARaaUa0oxwOvAO0M/MSs3sCuB+oCPwupktMLPfNGuQGb4a5YD7dtSUUERa\nsXpL4M65C2PMfrgZYqlTUVFUFQr4evC//S3ZoYiIpIVQ3IkJvrbkgBL44Yf7mTt3piQmEZFUCk0C\nj1kCr25K+OmnSY9HRCTVQpPAe/aErVv9UENNCUWkFQtNAq9uCx7zZh5dyBSRVig0Cby6LXitevAe\nPXzXsiqBi0grFJoEHrMEnpHhS+FK4CLSCoUugcdsC/7vfyc9HhGRVAtNAu/Y0Q8HtEQZOBCWLo3q\na1ZEpOULTQKHOG3Bhw6F3btjPLJHRKRlC1UCj9kWfNgwP/7gg6THIyKSSqFK4DFL4P36QXa2EriI\ntDqhSuBFRf6pPLWeopaVBYMGKYGLSKsTqgTes6e/VrlhQ9SCoUNhwYKUxCQikiqhS+AQ50JmWZme\njykirUqoEvgRR/jxxx9HLRg61I9VjSIirUioEni/fv7myw8/jFqgBC4irVCoEnhODhx5JCxeHLWg\nsBAOPVQJXERalVAlcIDi4hglcPClcCVwEWlFQpfABw2CFStiPIRn6FB/S72eziMirUQoE/i+fT5X\n13LssbB3L8yfn5K4RESSLXQJvLjYjw+oRhk1yo9nzUpqPCIiqRK6BH7UUZCZGeNCZpcuMGSIEriI\ntBr1JnAze8TMysxsccS888zsQzPbZ2YlzRtibdnZcPTRcS5knnwy/POfsGtXMkMSEUmJhpTAHwPG\nR81bDJwNzE50QA0RtyXK2LH+IuacOUmPSUQk2epN4M652cCmqHlLnHMp64B70CD/FLXt26MWjB7t\n7/RRNYqItALNXgduZlPMbK6ZzS0vL0/IPgcN8uMlS6IW5OfD8OHw978n5DgiIums2RO4c+53zrkS\n51xJ165dE7LPuC1RwFejzJkDO3Yk5FgiIukqdK1QwHdqlZMTpwfZk0/2fc6+/XbS4xIRSaZQJvDM\nTDjuOHjrrRgLTzzRN1V59dWkxyUikkwNaUY4HXgH6GdmpWZ2hZlNMrNS4ATgJTN7rbkDjTZ6tL/p\ncsuWqAUdOsC4cfDss+BcssMSEUmahrRCudA5d4hzLss518s597Bz7tngdVvnXHfn3OnJCDbS6NH+\nlvqYNSWTJvlmKosWJTssEZGkCWUVCsAJJ/iqlNmxWqJPmABmvhQuItJChTaB5+b6/qtiJvDu3WHk\nSCVwEWnRQpvAAcaMgffeg8rKGAsnTfL9g69cmfS4RESSIdQJfPRo32Iw5p3zkyb5sUrhItJChTqB\njxzpq7pjVqP07esf8vDUU0mPS0QkGUKdwPPzfY7+xz/irDB5Mrz7bpxbNkVEwi3UCRzg1FP9DT0V\nFTEWTp4MWVnw4INJj0tEpLmFPoGffbavB3/ppRgLu3TxdeG//72elSkiLU7oE/iIEVBUBH/+c5wV\nrroKNm3SxUwRaXFCn8AzMnwh+5VX4nRAePLJ/oKmqlFEpIUJfQIHX41SWQmvxeqRJSMDrrzSP+Th\ngAdpioiEV4tI4KNHQ2FhHdUoV1/tb928/fakxiUi0pxaRALPzISJE+Evf4Hdu2Os0LkzXHstTJ/u\nO7kSEWkBWkQCB/ja12Dz5jpK4Tfd5DP9nXcmNS4RkebSYhL4qafCkUfCfffFWeGQQ+Dyy+HRR2H1\n6qTGJiLSHFpMAs/IgG9+E955B+bOjbPSd7/rOxG/9dakxiYi0hxaTAIHuPRSf63yl7+Ms0LfvnDD\nDfDIIzBvXjJDExFJuBaVwDt18kl8xgxYvz7OSj/8IXTtCtdfr0euiUiotagEDr4aZfduuP/+OCt0\n6gQ/+5l/Ftsf/5jU2EREEqnFJfD+/eGcc+Dee/0d9DFddhmUlMCNN8KGDUmNT0QkUVpcAge47TbY\ntg3+93/jrJCRAQ8/7LswvP76ZIYmIpIwLTKBFxfD+ef7UnjcAvaQIfCDH/ibe55/PqnxiYgkQr0J\n3MweMbMyM1scMa/QzF43s+XBuKB5w2y8W2/1/aPccUcdK33/+z6RX311HVc9RUTSU0NK4I8B46Pm\nTQVmOueOAmYG02llwAD4+td9Kfyjj+KslJ3t+wrfvBkuvhj27k1qjCIiB6PeBO6cmw1EXw6cCDwe\nvH4c+GqC40qIO++Ejh19AXvfvjgrDRniG47/7W/w858nNT4RkYPR1Drw7s65tcHrdUD3BMWTUN26\nwS9+4R+59vDDdax4xRVw0UW+3uX115MWn4jIwTjoi5jOOQfEvSPGzKaY2Vwzm1teXn6wh2u0yy6D\nMWP8XfSffRZnJTP4zW9g4EA47zxYujSpMYqINEVTE/h6MzsEIBiXxVvROfc751yJc66ka9euTTxc\n05nBQw/56u0LLvDPz4ypQwffH23btnDWWbBxY1LjFBFprKYm8BeAS4LXlwBp3Q7vyCN9Fco77/iG\nJ3H16QPPPQelpT6Jb92arBBFRBqtIc0IpwPvAP3MrNTMrgBuB041s+XAKcF0WjvvPPjGN/zNPdOn\n17HiCSf4zlT+9S+fxGM+aFNEJPXMJbFDp5KSEjc3bl+vzW/XLjjtNN8NynPPwZln1rHyjBnwn//p\nH4r83HO+ikVEJAXMbJ5zriR6fou8EzOetm19NfewYb6/lL//vY6VL7gAHnvMPwz5lFNUJy4iaadV\nJXCAvDx49VVfL37mmf51XJMnw9NPw/vv+ycnf/pp0uIUEalPq0vg4J9xPGuW77lwwgR45pk6Vp40\nyWf50lIYMcLXv4iIpIFWmcDBP9Nh1izfq+z559fRfzjA2LEwZ44vvo8dC088kbQ4RUTiabUJHCA/\n3994edZZcN11/sH1cbtDGTAA3n0XTjwRLrkEpk5V3ykiklKtOoGDf4bmn//sE/g99/i7Nj/5JM7K\nhYW+OuWaa3w3hxMmQFnce5hERJpVq0/gAG3awH33+Y4JFy/2/Vs9+GCcR2ZmZcEDD/hh5kwYPBhe\nfjnpMYuIKIFHuPhiWLQIjjsOpkzxBex16+KsfM01MHcu9Ojhm7NcfLH6FBeRpFICj3Loob5efNo0\n38NscbGvYompuNjXi//oR/B//+ebtfz2t3X0XSsikjhK4DFkZMANN8D8+b57lHPO8dctYz6eLScH\nfvxj+OADf4fQNdf4C50LFiQ7bBFpZZTA6zBggO8A60c/giefhKOO8iXzmD0a9u/vb+18/HFYvhyG\nD/ePBFq5Mulxi0jroARej87XWusAAA5vSURBVKwsX8BeuNDfx3Pjjf665SuvxFjZzN+9+fHHvgPy\np5+Gfv38k+9VPy4iCaYE3kADB/oWhH/5i6/iPuMMOP10fx3zAAUFcPvtsGKFf6LEr38NRxwB3/kO\nrFmT9NhFpGVSAm8EM3/Tz+LFvlvauXPhS1/yd9u/+WaMZoc9e/qLmh99BBMnwt13Q9++vonL8uUp\neQ8i0nIogTdBdra/a3PlSl+98sYbvq+rYcN8+/Ht26M2OPpoX4m+fLl//uYTT/iqlXPP9fXmSezS\nV0RaDiXwg5CX5y9wrl7tH9tm5gvXvXrBt77l25TXcvjhvjpl1Sr43vd88h43zl8tnTYNvvgiFW9D\nREJKCTwB2rf3Bev334e33oLx4/2NmkOG+Aufv/kNbN4csUGPHvDzn/vM//jjvs78xhuhqMi3XHn9\ndfWzIiL1alVP5EmmjRt9rcnDD/sWLDk5cOqpvg79zDN99XgtCxb4+vIZM6Ciwifziy7yCb242Bfv\nRaRVivdEHiXwZuacvyHoiSd8C5bqZuHDhvlEfvrpcPzxvrkiADt3wksv+Q1efhmqqnwd+qRJcPbZ\nvv/bDP1wEmlNlMDTgHOwZInPz3/5i382xN690LGjr2o57jgYNcoPublAeblvS/7ss77z8qoqX3Sf\nNMkPo0dDZmaq35aINDMl8DRUUeGvY/7tb/55EQsX+oSelQUnnOCvb44b55N71rYv4MUXfTJ/9VWo\nrPTd255+ui/Kjx/vHzUkIi2OEngIbN/uL4LOnOmH99/3pfYOHXw/5WPGwMiRMLz/DnL+8Ro8/7y/\nJbSszFerHHecT+hjxvh6mZycVL8lEUkAJfAQ2rjR15zMnOlL6h9/vH9Zz56+anz4MY6Swk8oWf8S\nR7z9e2z+PJ/127b1SXzMGDjpJP+6XbuUvRcRabpmSeBmdgNwFWDAg865aXWtrwR+cNav9/XmixbB\nv/8NS5f6ThB37fLL8/Ph2KF7KOmyimN2zmHAJy9x1NK/0M7t8HcffelLvm7mP/7Dj3v0SO0bEpEG\nSXgCN7NiYAYwAtgNvApc45xbEW8bJfDE27PH39o/d+7+YeFCf70TwMzRu2sl/dp/Tr+dC+lX/hb9\n9n5If5bSs0829qUS33PiMcf4cdeuqX1DInKA5kjg5wHjnXNXBNM/BHY55+6Mt40SeHLs3Olbuyxb\nduAQeZt/bpudHN1mBf12L6Ify/zQrYKjS/LoMGKgT+jDh/s26WqHLpIyzZHABwDPAycAlcBMYK5z\n7rqo9aYAUwB69+597Kefftqk48nBc853hnhAYl+yl1WfZeDc/iTdi8/3J/UOa+jXD476Uj69Rh9O\n9peG+k652rRJ4bsRaT2aqw78CuC/gO3Ah/gS+Lfira8SePqqrPS939Yk9Q/3sOyDnSxbmc3myrY1\n62WwlyLW0Mc+o0/eJvocsos+R7ThsOKO9DmuO4eO6kPbLh1T+E5EWp5mb4ViZj8DSp1zv463jhJ4\n+DjnWykuWwafLNvDqn+Vs+rDbaz6vA2rNnSgtLIze6l9M1HXjA0Uta+gZ+FOinpCzyPaUTQwn57F\nBRT1yqBnT+jSRTeUijRUc5XAuznnysysN/BX4HjnXEW89ZXAW56qKli9cjer3lnLqrkb+GzJdlZ/\ntpc15Zms3tKRNXu7s57uuKh+07Iyqjik0w6KulbRs3cGRUe0o2ffthQV+SaS1eOOKsyLNFsCfxPo\nDOwBbnLOzaxrfSXwViYovu/58GPWvfcZaxZtZPW/d7JmtWN1eRard3VhDUWspidrKGILnQ7YRYd2\nVRzSfR/dijLp3iODbt2gWzfo3p2a1926+RJ9p04RfcqItCC6kUfSzxdfwCef+GHlSrYtW83qZdtY\ns3IXq9e1Yc2+7qymJ+voQRndKcsqYr3rxsaq/Li7zMnx/bTn5fl28dFDQUHs6eqxbl6VdBQvgasn\nJEmdggI49lg/AB2AfsHA3r2+v/QgufPJTD9evZqq0nVsKN1J2c6OrKc7ZXRjA13YSh6bMw5hy74e\nbK7sypbdBVSs78Tqqg5U7GpHxfYsKnfV3XKmbdsDk3p+vq/KycurPY6e16HD/iEnRy0vpfkpgUt6\natMGevf2w0kn1VqUCfRwjh4VFVBa6ofVq4PX8/e/XrvW90cQYRfZVJBPRX5fKjofwRd5h1GR25Mv\n2hVRkdWVijad+WJvHhV7cqnYlUN5WTbLl2ewdSts2eJb6zRERoZP5Lm5tRN79NDY5ep8UiLp4yDh\nZOaLyAUFMHhw/PV27/Z9EKxdC+vW0XbtWrqvW0f3YJq1M+Gzdf717t2x99Gxo79D9bAuVBV2Y2un\nXmzN68mW9j3Y2q4bW7K7sCWzkO2ZndiWkce2ve3YtiOD7dth27baQ3m5f6Je9fTWrfvvmm2Itm0P\nTPDt2vmnQrVrd+Drpi7LytIviDBQApeWLTsbDj3UD3VxztfJr10LGzb4obx8/+tgOrNsDQUfLaRg\nwwbYsSP2vsx8nUph4f4vmYICOKTgwHkFBezu2Jlt2YVsz8pnm3Vk246MWkk/1hdB5FBZ6cOrrPQh\nVVbufx3vO6k+GRk+oefk7E/q1a+jx02d17atH7Kza4/15dFwSuAi4DNGYaEfGmrHjtoJvjrpb9rk\nvwyqh02bfLVO9XRUVs0GCoOBjAzfnKaw0Fe+d+q0/6ps9eveUdPRy/Pyaupa9u71XStEJ/aGvt65\nc/+86tc7d/qaqeh51eslol1EdvaBiT3euCHrNHYcb1l2dnrdv6AELtJU7dvvr6dvKOd8lotO8pHJ\nPnJ661bf9eSWLfuHhjzwun17yMujTV4euXl55Mb6Ioic7hpcjY1VCd+IjOWc72AtXnKPfL17tx92\n7do/jnzdkHH192Fd6+3b1/A/T0NkZjbtS+F734OhQxMcS2J3JyJ1MvPJtX176NWr8ds754vHkQl9\n8+bYr6Ony8pqTzc0s7VvX/eV1ojBOnQgOxg6RS5r3x7y2+9/7+3bJ60vnaqqxn0pNGTckHW2bvU/\nyqqnt2xJ/HtTAhcJEzNfKs7NhUMOafp+qr8IqhP65s11V7RHD1u2+J7RIuft3Nm4GLKzayf0WEP1\n1dXGDJHbtGtHZmYGmZl+sqVRAhdpjSK/CIqKErPPqqrYV1y3bvVfFvUN1ZXvO3b4B8bGWqcpcnIa\nnvirr7BWD9HTseZFT7dtm7SrsErgIpIYmZm+Pr3TgV0iJIRz+6/Ixkr6jRmqt6tuTVQ9VFfSH+yV\n2FgJ/re/hVGjEnMuAkrgIhIOZvtLyZ07N99xqq/E7ty5f6i+AtvQ6Vjz8vISHqoSuIhIJLP9bQab\nIekmUhq1aBQRkcZQAhcRCSklcBGRkFICFxEJKSVwEZGQUgIXEQkpJXARkZBSAhcRCamkPtTYzMqB\nT5u4eRdgQwLDSRbFnXxhjV1xJ1eY4j7MOdc1emZSE/jBMLO5sZ7KnO4Ud/KFNXbFnVxhjTuSqlBE\nREJKCVxEJKTClMB/l+oAmkhxJ19YY1fcyRXWuGuEpg5cRERqC1MJXEREIiiBi4iEVCgSuJmNN7Nl\nZrbCzKamOp54zOxQM5tlZh+Z2YdmdkMwv9DMXjez5cG4INWxxmJmbczsfTN7MZjua2bvBuf9T2aW\nneoYo5lZvpk9bWZLzWyJmZ0QhvNtZjcGn5HFZjbdzHLS8Xyb2SNmVmZmiyPmxTy/5t0XxL/QzIan\nWdy/CD4nC83sWTPLj1j2/SDuZWZ2emqibry0T+Bm1gb4FfBlYCBwoZkNTG1UcVUB/+2cGwgcD3wj\niHUqMNM5dxQwM5hORzcASyKm7wDucc4dCXwBXJGSqOp2L/Cqc64/MBQff1qfbzPrCVwPlDjnioE2\nwAWk5/l+DBgfNS/e+f0ycFQwTAEeSFKMsTzGgXG/DhQ754YAHwPfBwj+Ry8ABgXb/DrIO2kv7RM4\nMAJY4Zz7xDm3G5gBTExxTDE559Y65+YHr7fik0lPfLyPB6s9Dnw1NRHGZ2a9gDOBh4JpA04Gng5W\nSbu4zawTMBp4GMA5t9s5V0EIzjf+cYbtzCwTaA+sJQ3Pt3NuNrApana88zsReMJ5c4B8MzskOZHW\nFitu59xfnXNVweQcoFfweiIwwzm3yzm3EliBzztpLwwJvCfwecR0aTAvrZlZH+AY4F2gu3NubbBo\nHdA9RWHVZRrwXWBfMN0ZqIj4wKfjee8LlAOPBlU/D5lZLml+vp1zq4G7gM/wiXszMI/0P9/V4p3f\nMP2vXg68ErwOU9y1hCGBh46ZdQCeAb7lnNsSucz5dptp1XbTzM4Cypxz81IdSyNlAsOBB5xzxwDb\niaouSdPzXYAv9fUFioBcDvy5HwrpeH7rY2Y/wFd3PpnqWA5WGBL4auDQiOlewby0ZGZZ+OT9pHPu\nz8Hs9dU/JYNxWarii2MkMMHMVuGrqE7G1y3nBz/xIT3PeylQ6px7N5h+Gp/Q0/18nwKsdM6VO+f2\nAH/G/w3S/XxXi3d+0/5/1cwuBc4CLnL7b4JJ+7jjCUMC/xdwVHCFPht/seGFFMcUU1Bv/DCwxDl3\nd8SiF4BLgteXAM8nO7a6OOe+75zr5Zzrgz+/f3fOXQTMAs4NVkvHuNcBn5tZv2DWOOAj0vx846tO\njjez9sFnpjrutD7fEeKd3xeAyUFrlOOBzRFVLSlnZuPx1YQTnHM7Iha9AFxgZm3NrC/+Iux7qYix\n0ZxzaT8AZ+CvGv8b+EGq46kjzhPxPycXAguC4Qx8ffJMYDnwN6Aw1bHW8R5OAl4MXh+O/yCvAP4P\naJvq+GLEOwyYG5zz54CCMJxv4MfAUmAx8HugbTqeb2A6vp5+D/4XzxXxzi9g+BZj/wYW4VvZpFPc\nK/B13dX/m7+JWP8HQdzLgC+n+rw3dNCt9CIiIRWGKhQREYlBCVxEJKSUwEVEQkoJXEQkpJTARURC\nSglcRCSklMBFRELq/wMooyBv+p8kVAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVIh7VtcZW2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}