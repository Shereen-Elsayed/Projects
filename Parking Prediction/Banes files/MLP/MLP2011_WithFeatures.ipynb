{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "MLP2011_WithFeatures.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_aVTChKIx3",
        "colab_type": "code",
        "outputId": "79dd4236-f42a-48d3-aca4-76d15f613888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkZGl_g0KFc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.version\n",
        "#Import Libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "import shutil\n",
        "import tensorflow.contrib.learn as tflearn\n",
        "import tensorflow.contrib.layers as tflayers\n",
        "from tensorflow.contrib.learn.python.learn import learn_runner\n",
        "import tensorflow.contrib.metrics as metrics\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "from random import shuffle\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import xgboost as xgb\n",
        "#TF Version\n",
        "tf.__version__\n",
        "\n",
        "#with warnings.catch_warnings():\n",
        "#    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "#    import h5py\n",
        "\n",
        "num_periods_output = 12 #to predict\n",
        "num_periods_input=24 #input\n",
        "l_rate = 0.0003   \n",
        "MiniBatches_size=128\n",
        "keep_probab=0.9            #(1-dropout_rate)\n",
        "keep_prob_testval=1.0  \n",
        "epochs = 500   \n",
        "Number_of_TimeFeatures=23+37+18\n",
        "inputs = Number_of_TimeFeatures-1 \n",
        "lamda=0.4 \n",
        "\n",
        "\n",
        "No_Of_weeks=1\n",
        "\n",
        "ALL_Test_Data=[]\n",
        "ALL_Test_Prediction=[]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4odAQaTMKFdB",
        "colab_type": "text"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpCUqQK9KFdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "def preprocessing(df_,num_features):\n",
        "    \n",
        "    if df_.ID[0]!=7 and df_.ID[0]!=8:\n",
        "        print(df_.Year.unique())\n",
        "        df=df_[(df_.Year==2017) | (df_.Year==2018)]\n",
        "        #.isin(years)\n",
        "        #print(df.loc[df['Year'].isin([2017,2018])])\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    else:\n",
        "        df=df_[(df_.Year==2015) | (df_.Year==2016)]\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    \n",
        "    '''df_=df[['ID','Occupancy','Year', 'Month', 'Day', 'Hour','Minute', 'Capacity', \n",
        "    'DayOfWeek','IsWeekend', 'temperature', 'dew_point', 'humidity', 'wind_speed', \n",
        "    'feels_like', 'Status', 'light_snow','snow_shower', 'fog', \n",
        "    'thunder', 'mostly_cloudy','rain', 'heavy_rain', 'mist', 'shallow_fog','light_freezing_rain',\n",
        "    'partly_cloudy', 'haze', 'light_rain', 'rain_shower', 'snow', 'light_snow_shower']]'''\n",
        "    \n",
        "    \n",
        "    \n",
        "    # select features\n",
        "    df=df[['ID','Occupancy','Year', 'Month', 'Day', 'Hour','Minute', \n",
        "    'DayOfWeek', 'temperature', 'dew_point', 'humidity', 'wind_speed', 'feels_like', 'Status','IsWeekend', 'light_snow','snow_shower', 'fog', \n",
        "    'thunder', 'mostly_cloudy','rain', 'heavy_rain', 'mist', 'shallow_fog','light_freezing_rain', 'partly_cloudy',\n",
        "     'haze', 'light_rain', 'rain_shower', 'snow', 'light_snow_shower','Events_Rugby','Events_Football','Events_Other_Sport','Events_Exhibitions']]\n",
        "    \n",
        "    ################################################encoding########################\n",
        "    df['Occupancy'] = pd.to_numeric(df['Occupancy'],errors='coerce')\n",
        "    df['Occupancy'] = df['Occupancy'].abs()\n",
        "    \n",
        "    Status=df.pop('Status')\n",
        "    df.loc[:,'Status_filling']=(Status=='Filling')*1.0\n",
        "    df.loc[:,'Status_static']=(Status=='Static')*1.0\n",
        "    df.loc[:,'Status_emptying']=(Status=='Emptying')*1.0\n",
        "    \n",
        "    Number_Of_Features=num_features\n",
        "    df=df.values\n",
        "    df = df.astype('float32')\n",
        "    split=num_periods_output+num_periods_input\n",
        "    \n",
        "    ##################################SPLIT##############################################\n",
        "    \n",
        "    ########################## SPLITTING FOR TESTING ##########################\n",
        "    test_val=np.floor(len(df)*0.2)\n",
        "    mod=test_val%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    test_val=int(test_val-mod)\n",
        "    Test_cut=int(np.floor(test_val/2))\n",
        "    Test=df[(len(df)-Test_cut):,:]\n",
        "    Valid=df[(len(df)-test_val):(len(df)-Test_cut),:]\n",
        "    ########################### SPLITTING FOR TRAIN ###########################\n",
        "    \n",
        "    new_cutted_df=df[:(len(df)-test_val),:]\n",
        "    Start_train_index=12*24*7*No_Of_weeks\n",
        "    Start_train_index=np.floor(Start_train_index)\n",
        "    Start_train_index=int(Start_train_index)\n",
        "    print('instances',Start_train_index)\n",
        "    Train=new_cutted_df[len(new_cutted_df)-Start_train_index:,:]\n",
        "    train_len=len(Train)\n",
        "    mod=train_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    train_len=int(train_len-mod)\n",
        "    Train=Train[0:train_len,:]\n",
        "    print('len Train',len(Train))\n",
        "   \n",
        "        \n",
        "     ############################################ Valid minibatches ##################################\n",
        "    \n",
        "    end_valid=len(Valid)\n",
        "    start=0\n",
        "    next=0\n",
        "    x_validbatches=[]\n",
        "    y_validbatches=[]\n",
        "    \n",
        "    count=0\n",
        "    #print('lennnn',len(Train))\n",
        "    while next+(num_periods_input+num_periods_output)<end_valid:\n",
        "        next=start+num_periods_input\n",
        "        x_validbatches.append(Valid[start:next,:])\n",
        "        y_validbatches.append(Valid[next:next+num_periods_output,1])\n",
        "        start=start+1\n",
        "    y_validbatches=np.asarray(y_validbatches)\n",
        "    y_validbatches = y_validbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_validbatches=np.asarray(x_validbatches)\n",
        "    x_validbatches = x_validbatches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "    print('len x_validbatches ',len(x_validbatches))\n",
        "    ############################################ TRAIN minibatches ##################################\n",
        "    \n",
        "    end=len(Train)\n",
        "    start=0\n",
        "    next=0\n",
        "    x_batches=[]\n",
        "    y_batches=[]\n",
        "    \n",
        "    count=0\n",
        "    #print('lennnn',len(Train))\n",
        "    while next+(num_periods_input+num_periods_output)<end:\n",
        "        next=start+num_periods_input\n",
        "        x_batches.append(Train[start:next,:])\n",
        "        y_batches.append(Train[next:next+num_periods_output,1])\n",
        "        start=start+1\n",
        "    y_batches=np.asarray(y_batches)\n",
        "    y_batches = y_batches.reshape(-1, num_periods_output, 1)   \n",
        "    x_batches=np.asarray(x_batches)\n",
        "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "    print('len x_batches ',len(x_batches))\n",
        "    \n",
        "    ###########################################TEST#####################################\n",
        "    ############################################ TEST minibatches ##################################\n",
        "    end_test=len(Test)\n",
        "    start_test=0\n",
        "    next_test=0\n",
        "    x_testbatches=[]\n",
        "    y_testbatches=[]\n",
        "    \n",
        "    \n",
        "    #print('lennnn',len(Train))\n",
        "    while next_test+(num_periods_input+num_periods_output)<end_test:\n",
        "        next_test=start_test+num_periods_input\n",
        "        x_testbatches.append(Test[start_test:next_test,:])\n",
        "        y_testbatches.append(Test[next_test:next_test+num_periods_output,1])\n",
        "        start_test=start_test+1\n",
        "    y_testbatches=np.asarray(y_testbatches)\n",
        "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_testbatches=np.asarray(x_testbatches)\n",
        "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features) \n",
        "    print('len Test',len(Test))\n",
        "    print('len xTestbatches',len(x_testbatches))\n",
        "    ######################## Sampling##########################################\n",
        "    \n",
        "    #x_batches, y_batches, x_validbatches, y_validbatches, x_testbatches, y_testbatches\n",
        "    \n",
        "    return x_batches, y_batches, x_validbatches, y_validbatches, x_testbatches, y_testbatches\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GeTnVETKFdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_locationfiles(path,loc_id):\n",
        "    filename=path + '/BN00'+str(loc_id)+'.csv'\n",
        "    print(filename)\n",
        "    data_loc=pd.read_csv(filename)\n",
        "    #print(data_loc.head())\n",
        "    #mod=len(data_loc)%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    #data_loc=data_loc[:len(data_loc)-mod]\n",
        "    return data_loc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBMD4AcXKFdI",
        "colab_type": "code",
        "outputId": "ab13aad8-6a1e-4a02-efd3-3b794f58c21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "    data_path=r'/content/drive/My Drive/FINAL_DATA_EVENTS'\n",
        "    #r'/home/shero/Desktop/OurProject/BanesData/Model/occupation_loc/'\n",
        "    data_All=pd.DataFrame()\n",
        "    x_batches_Full=[]\n",
        "    y_batches_Full=[]\n",
        "    X_Valid_Full=[]\n",
        "    Y_Valid_Full=[]\n",
        "    X_Test_Full=[]\n",
        "    Y_Test_Full=[]\n",
        "    for loc_id in range(1,9):\n",
        "        #========\n",
        "        data=load_locationfiles(data_path,loc_id)\n",
        "        header=list(data.columns.values)\n",
        "        data=pd.DataFrame(data,columns=header)\n",
        "        x_batches, y_batches,x_valid,y_valid,X_Test,Y_Test=preprocessing(data,37)\n",
        "        #===============================\n",
        "        for element1 in (x_batches):\n",
        "            x_batches_Full.append(element1)\n",
        "            \n",
        "        for element2 in (y_batches):\n",
        "            y_batches_Full.append(element2)\n",
        "\n",
        "        for element3 in (x_valid):\n",
        "            X_Valid_Full.append(element3)\n",
        "            \n",
        "        for element4 in (y_valid):\n",
        "            Y_Valid_Full.append(element4)\n",
        "                        \n",
        "        for element5 in (X_Test):\n",
        "            X_Test_Full.append(element5)\n",
        "            \n",
        "        for element6 in (Y_Test):\n",
        "            Y_Test_Full.append(element6)\n",
        "    #---------------------shuffle minibatches X and Y together-------------------------------------\n",
        "    print(len(x_batches_Full),'     length of all file : ',len(y_batches_Full))\n",
        "    combined = list(zip(x_batches_Full, y_batches_Full))\n",
        "    random.shuffle(combined)\n",
        "    shuffled_batch_features, shuffled_batch_y = zip(*combined)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN001.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[628]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  18229\n",
            "len x_batches  1957\n",
            "len Test 18288\n",
            "len xTestbatches 18229\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN002.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1056]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  19075\n",
            "len x_batches  1957\n",
            "len Test 19134\n",
            "len xTestbatches 19075\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN003.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[ 860 1500 1160 1360]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  14773\n",
            "len x_batches  1957\n",
            "len Test 14832\n",
            "len xTestbatches 14773\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN004.csv\n",
            "[2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[698]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  19615\n",
            "len x_batches  1957\n",
            "len Test 19674\n",
            "len xTestbatches 19615\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN005.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1320 1230]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  14647\n",
            "len x_batches  1957\n",
            "len Test 14706\n",
            "len xTestbatches 14647\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN006.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[521 519 525 513]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  19579\n",
            "len x_batches  1957\n",
            "len Test 19638\n",
            "len xTestbatches 19579\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN007.csv\n",
            "[2015 2016]\n",
            "[720 626 800 860 700 623 519 528 563 638 603 630 596 602 840]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  9625\n",
            "len x_batches  1957\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN008.csv\n",
            "[2015 2016]\n",
            "[140 180 132]\n",
            "instances 2016\n",
            "len Train 2016\n",
            "len x_validbatches  9625\n",
            "len x_batches  1957\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "15656      length of all file :  15656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQ2LajOKFdL",
        "colab_type": "code",
        "outputId": "0207c60c-cf37-4b99-afad-d0eee6c4d869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "#xgboost part\n",
        "print(len(x_batches_Full))\n",
        "All_Training_Instances=[]\n",
        "\n",
        "Static_Features=[[1,0,0,0,0,0,0,0,1,3,4,6,7,19,9,17,51.3787,-2.3622],[0,1,0,0,0,0,0,0,0,3,9,11,3,29,10,13,51.3843,-2.3686],\n",
        "                     [0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,51.4113,-2.3869],[0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,51.3902,-2.4059]\n",
        "                    ,[0,0,0,0,1,0,0,0,0,0,0,4,2,3,0,0,51.3529,-2.3838],[0,0,0,0,0,1,0,0,2,4,8,6,3,39,13,14,51.3842,-2.3590],\n",
        "                     [0,0,0,0,0,0,1,0,1,1,3,6,6,17,9,7,51.3782,-2.3589],[0,0,0,0,0,0,0,1,1,2,3,6,6,18,9,7,51.3783,-2.3593]]\n",
        " \n",
        "#=============== change each window into Instance =================================\n",
        "for i in range(0,len(shuffled_batch_features)):\n",
        "    hold=[]\n",
        "    temp=[]\n",
        "    for j in range(0,len(shuffled_batch_features[i])):\n",
        "      #print(len(hold))\n",
        "      \n",
        "      if j==(len(shuffled_batch_features[i])-1):\n",
        "          index=int(shuffled_batch_features[i][j][0])\n",
        "          hold=np.concatenate((hold, Static_Features[index-1]), axis=None)\n",
        "          temp=np.delete(shuffled_batch_features[i][j], [0], axis=0) \n",
        "          hold=np.concatenate((hold, temp), axis=None)\n",
        "          \n",
        "      else:\n",
        "          hold=np.concatenate((hold, shuffled_batch_features[i][j][1]), axis=None)\n",
        "          \n",
        "    #print(len(hold))\n",
        "    All_Training_Instances.append(hold)\n",
        "    \n",
        "\n",
        "print(len(All_Training_Instances[0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15656\n",
            "77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5zKLm39w34R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    #batch_size=\n",
        "    X_returned=[]\n",
        "    Y_returned=[]\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        if (start + batch_size)>len(features):\n",
        "            break\n",
        "        else:\n",
        "            end = start + batch_size\n",
        "        X_returned.append(features[start:end])\n",
        "        Y_returned.append(labels[start:end])\n",
        "    return X_returned, Y_returned\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H23SYSNC3y6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####### MLP session##################\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, inputs])   #create variable objects\n",
        "y = tf.placeholder(tf.float32, [None, num_periods_output])\n",
        "learning_rate=tf.placeholder(tf.float32, None)\n",
        "keep_prob = tf.placeholder(tf.float32, None)\n",
        "\n",
        "\n",
        "MLP1= tf.layers.dense(X, units=100,activation=tf.nn.relu) \n",
        "MLP2= tf.layers.dense(MLP1, units=50,activation=tf.nn.relu)  \n",
        "Dropout = tf.nn.dropout(MLP2,keep_prob=keep_probab)\n",
        "output= tf.layers.dense(Dropout, units=num_periods_output)  \n",
        "\n",
        "\n",
        "outputs = tf.reshape(output, [-1, num_periods_output])          #shape of results\n",
        "#Regularization part\n",
        "tv = tf.trainable_variables()\n",
        "regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
        "Error=tf.square(outputs - y)\n",
        "#Total_err=Error+regularization_cost\n",
        "loss = tf.reduce_mean(tf.square(outputs - y))+ lamda*regularization_cost   #define the cost function which evaluates the quality of our model\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  \n",
        "\n",
        "training_op = optimizer.minimize(loss)    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3QbiznjKFdN",
        "colab_type": "code",
        "outputId": "5c76a0fa-5a40-4a3a-dc13-69377365c26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#=================Testing=====================\n",
        "All_Testing_Instances=[]\n",
        "All_Validation_Instances=[]\n",
        "#=============== change each window into Instance =================================\n",
        "print(len(X_Test_Full))\n",
        "for i in range(0,len(X_Test_Full)):\n",
        "  hold=[]\n",
        "  temp=[]\n",
        "  for j in range(0,len(X_Test_Full[i])):\n",
        "       #print(len(hold))\n",
        "      if j==(len(X_Test_Full[i])-1):\n",
        "          index=int(X_Test_Full[i][j][0])\n",
        "          hold=np.concatenate((hold, Static_Features[index-1]), axis=None)\n",
        "          temp=np.delete(X_Test_Full[i][j], [0], axis=0) \n",
        "          hold=np.concatenate((hold, temp), axis=None)\n",
        "      else:\n",
        "          hold=np.concatenate((hold, X_Test_Full[i][j][1]), axis=None)\n",
        "   \n",
        "  All_Testing_Instances.append(hold)\n",
        "\n",
        "#=============== change each window into Instance validation =================================\n",
        "print(len(X_Valid_Full))\n",
        "for i in range(0,len(X_Valid_Full)):\n",
        "  hold=[]\n",
        "  temp=[]\n",
        "  for j in range(0,len(X_Valid_Full[i])):\n",
        "       #print(len(hold))\n",
        "      if j==(len(X_Valid_Full[i])-1):\n",
        "          index=int(X_Valid_Full[i][j][0])\n",
        "          hold=np.concatenate((hold, Static_Features[index-1]), axis=None)\n",
        "          temp=np.delete(X_Valid_Full[i][j], [0], axis=0) \n",
        "          hold=np.concatenate((hold, temp), axis=None)\n",
        "      else:\n",
        "          hold=np.concatenate((hold, X_Valid_Full[i][j][1]), axis=None)\n",
        "   \n",
        "  All_Validation_Instances.append(hold)\n",
        "\n",
        "\n",
        "#prediction=multioutput.predict(All_Testing_Instances)\n",
        "print(len(All_Testing_Instances[0]))\n",
        "#===========================calling MultiOutput XGoost=========================\n",
        "All_Testing_Instances=np.reshape(All_Testing_Instances, (len(All_Testing_Instances),len(All_Testing_Instances[0])))\n",
        "Y_Test_Full=np.reshape(Y_Test_Full, (len(Y_Test_Full),num_periods_output))\n",
        "#===========================  Validation set =========================\n",
        "All_Validation_Instances=np.reshape(All_Validation_Instances, (len(All_Validation_Instances),len(All_Validation_Instances[0])))\n",
        "Y_Valid_Full=np.reshape(Y_Valid_Full, (len(Y_Valid_Full),num_periods_output))\n",
        "#========== reshape train ==============================\n",
        "All_Training_Instances=np.reshape(All_Training_Instances, (len(All_Training_Instances),len(All_Training_Instances[0])))\n",
        "shuffled_batch_y=np.reshape(shuffled_batch_y, (len(shuffled_batch_y),num_periods_output))\n",
        "\n",
        "# ======= mini batches for MLP=======\n",
        "All_Training_Instances_batches,shuffled_batch_y_batches=batch_features_labels(All_Training_Instances,shuffled_batch_y,MiniBatches_size)\n",
        "\n",
        "Training_Error=[]\n",
        "Validation_Error=[]\n",
        "train_mse=[]\n",
        "validation_mse=[]\n",
        "\n",
        "\n",
        "print(All_Training_Instances.shape)\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()           #initialize all the variables\n",
        "    init.run()\n",
        "    for ep in range(epochs):\n",
        "        for i in range(0,len(All_Training_Instances_batches)): \n",
        "            _,Train_Batch_error=sess.run([training_op,Error],\n",
        "                                                        feed_dict={\n",
        "                                                            keep_prob:keep_probab,\n",
        "                                                            X: All_Training_Instances_batches[i],\n",
        "                                                            y: shuffled_batch_y_batches[i],\n",
        "                                                            learning_rate: l_rate                                                     \n",
        "                                                            })  \n",
        "            Training_Error.append(Train_Batch_error)\n",
        "        if ep % 3 == 0:\n",
        "            Sum_train=np.sum(Training_Error,axis=2)\n",
        "            Sum_train_1=np.sum(Sum_train,axis=1)\n",
        "            Sum_train_2=np.sum(Sum_train_1,axis=0)\n",
        "            Mean_train=Sum_train_2/(len(Training_Error)*MiniBatches_size*num_periods_output)\n",
        "            print(\"epoch:\",int(ep+1))\n",
        "            print(\"\\tRMSE Training:\", (Mean_train)**0.5)\n",
        "            train_mse.append((Mean_train)**0.5)\n",
        "            All_Validation_Instances_batches,Y_Validation_Full_batches=batch_features_labels(All_Validation_Instances,Y_Valid_Full,MiniBatches_size)\n",
        "            for i in range(0,len(All_Validation_Instances_batches)): \n",
        "                y_predict,Valid_error_batch=sess.run([outputs,Error], \n",
        "                                                            feed_dict={\n",
        "                                                            keep_prob:keep_prob_testval,\n",
        "                                                            X: All_Validation_Instances_batches[i], \n",
        "                                                            y: Y_Validation_Full_batches[i]\n",
        "                                                            })\n",
        "                Validation_Error.append(Valid_error_batch)\n",
        "            Sum_valid=np.sum(Validation_Error,axis=2)\n",
        "            Sum_valid_1=np.sum(Sum_valid,axis=1)\n",
        "            Sum_valid_2=np.sum(Sum_valid_1,axis=0)  \n",
        "            Mean_valid=Sum_valid_2/(len(Validation_Error)*MiniBatches_size*num_periods_output)\n",
        "            print(\"\\tRMSE Validation:\", (Mean_valid)**0.5)\n",
        "            validation_mse.append((Mean_valid)**0.5)\n",
        "\n",
        "    print('Fitting Done!')\n",
        "    ################################################# Testing  ###########################################\n",
        "    Testing_Error=[]\n",
        "    All_Testing_Instances_batches,Y_Test_Full_batches=batch_features_labels(All_Testing_Instances,Y_Test_Full,MiniBatches_size)\n",
        "    for i in range(0,len(All_Testing_Instances_batches)): \n",
        "          y_predict,Test_error_batch=sess.run([outputs,Error], \n",
        "                                                            feed_dict={\n",
        "                                                            keep_prob:keep_prob_testval,\n",
        "                                                            X: All_Testing_Instances_batches[i], \n",
        "                                                            y: Y_Test_Full_batches[i]\n",
        "                                                            })\n",
        "          Testing_Error.append(Test_error_batch)\n",
        "    Sum_test=np.sum(Testing_Error,axis=2)\n",
        "    Sum_test_1=np.sum(Sum_test,axis=1)\n",
        "    Sum_test_2=np.sum(Sum_test_1,axis=0)  \n",
        "    Mean_test=Sum_test_2/(len(Testing_Error)*MiniBatches_size*num_periods_output)\n",
        "    print(\"\\tRMSE Testing:\", (Mean_test)**0.5)\n",
        "    #testing_mse.append((Mean_test)**0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125168\n",
            "125168\n",
            "77\n",
            "(15656, 77)\n",
            "epoch: 1\n",
            "\tRMSE Training: 55.36690522314816\n",
            "\tRMSE Validation: 27.826807236104276\n",
            "epoch: 4\n",
            "\tRMSE Training: 32.81385861095721\n",
            "\tRMSE Validation: 22.523960857094252\n",
            "epoch: 7\n",
            "\tRMSE Training: 26.628528910034973\n",
            "\tRMSE Validation: 20.014844371702548\n",
            "epoch: 10\n",
            "\tRMSE Training: 23.44350260109718\n",
            "\tRMSE Validation: 18.454053014756493\n",
            "epoch: 13\n",
            "\tRMSE Training: 21.414918362145\n",
            "\tRMSE Validation: 17.352584249710354\n",
            "epoch: 16\n",
            "\tRMSE Training: 19.983281519824178\n",
            "\tRMSE Validation: 16.523135805033213\n",
            "epoch: 19\n",
            "\tRMSE Training: 18.888721330469775\n",
            "\tRMSE Validation: 15.878237886118928\n",
            "epoch: 22\n",
            "\tRMSE Training: 18.024178922549883\n",
            "\tRMSE Validation: 15.343619070996178\n",
            "epoch: 25\n",
            "\tRMSE Training: 17.299979468701295\n",
            "\tRMSE Validation: 14.881971162994429\n",
            "epoch: 28\n",
            "\tRMSE Training: 16.697436224274078\n",
            "\tRMSE Validation: 14.503096764571985\n",
            "epoch: 31\n",
            "\tRMSE Training: 16.18180820003124\n",
            "\tRMSE Validation: 14.166995527995748\n",
            "epoch: 34\n",
            "\tRMSE Training: 15.73947697628424\n",
            "\tRMSE Validation: 13.871079904886209\n",
            "epoch: 37\n",
            "\tRMSE Training: 15.353617477168335\n",
            "\tRMSE Validation: 13.611179409523919\n",
            "epoch: 40\n",
            "\tRMSE Training: 15.010451732116051\n",
            "\tRMSE Validation: 13.375299355194208\n",
            "epoch: 43\n",
            "\tRMSE Training: 14.691944102197532\n",
            "\tRMSE Validation: 13.15702003353559\n",
            "epoch: 46\n",
            "\tRMSE Training: 14.404718034010394\n",
            "\tRMSE Validation: 12.955919133099691\n",
            "epoch: 49\n",
            "\tRMSE Training: 14.143494848379994\n",
            "\tRMSE Validation: 12.77602539187583\n",
            "epoch: 52\n",
            "\tRMSE Training: 13.907820255910053\n",
            "\tRMSE Validation: 12.612297045807097\n",
            "epoch: 55\n",
            "\tRMSE Training: 13.69114736408374\n",
            "\tRMSE Validation: 12.467045032290818\n",
            "epoch: 58\n",
            "\tRMSE Training: 13.493360558422818\n",
            "\tRMSE Validation: 12.328259996492298\n",
            "epoch: 61\n",
            "\tRMSE Training: 13.308680206000313\n",
            "\tRMSE Validation: 12.199771052696065\n",
            "epoch: 64\n",
            "\tRMSE Training: 13.138577741456874\n",
            "\tRMSE Validation: 12.082608303052005\n",
            "epoch: 67\n",
            "\tRMSE Training: 12.981794012264\n",
            "\tRMSE Validation: 11.973364542700663\n",
            "epoch: 70\n",
            "\tRMSE Training: 12.834944949851943\n",
            "\tRMSE Validation: 11.871458901476618\n",
            "epoch: 73\n",
            "\tRMSE Training: 12.697374290411332\n",
            "\tRMSE Validation: 11.776524248302774\n",
            "epoch: 76\n",
            "\tRMSE Training: 12.567642690477534\n",
            "\tRMSE Validation: 11.687376499485522\n",
            "epoch: 79\n",
            "\tRMSE Training: 12.444948983627492\n",
            "\tRMSE Validation: 11.608832950132467\n",
            "epoch: 82\n",
            "\tRMSE Training: 12.330348129959576\n",
            "\tRMSE Validation: 11.52863886421593\n",
            "epoch: 85\n",
            "\tRMSE Training: 12.22017291879375\n",
            "\tRMSE Validation: 11.458764589649743\n",
            "epoch: 88\n",
            "\tRMSE Training: 12.116727157095815\n",
            "\tRMSE Validation: 11.38709527557254\n",
            "epoch: 91\n",
            "\tRMSE Training: 12.019008669058245\n",
            "\tRMSE Validation: 11.31805358797544\n",
            "epoch: 94\n",
            "\tRMSE Training: 11.926748946193674\n",
            "\tRMSE Validation: 11.256831960174114\n",
            "epoch: 97\n",
            "\tRMSE Training: 11.83943179511815\n",
            "\tRMSE Validation: 11.196278280442385\n",
            "epoch: 100\n",
            "\tRMSE Training: 11.755972787262795\n",
            "\tRMSE Validation: 11.14289539786465\n",
            "epoch: 103\n",
            "\tRMSE Training: 11.67790909930431\n",
            "\tRMSE Validation: 11.088348587012117\n",
            "epoch: 106\n",
            "\tRMSE Training: 11.602033825388206\n",
            "\tRMSE Validation: 11.035274575719557\n",
            "epoch: 109\n",
            "\tRMSE Training: 11.528622933328073\n",
            "\tRMSE Validation: 10.984753671645873\n",
            "epoch: 112\n",
            "\tRMSE Training: 11.460078285982304\n",
            "\tRMSE Validation: 10.9373406156058\n",
            "epoch: 115\n",
            "\tRMSE Training: 11.393920759267738\n",
            "\tRMSE Validation: 10.891530485535554\n",
            "epoch: 118\n",
            "\tRMSE Training: 11.329590185951389\n",
            "\tRMSE Validation: 10.847285978590708\n",
            "epoch: 121\n",
            "\tRMSE Training: 11.267741433817784\n",
            "\tRMSE Validation: 10.80478015360074\n",
            "epoch: 124\n",
            "\tRMSE Training: 11.20855371409324\n",
            "\tRMSE Validation: 10.765028309819641\n",
            "epoch: 127\n",
            "\tRMSE Training: 11.151051570182249\n",
            "\tRMSE Validation: 10.724977955347637\n",
            "epoch: 130\n",
            "\tRMSE Training: 11.096538433018663\n",
            "\tRMSE Validation: 10.689211155898578\n",
            "epoch: 133\n",
            "\tRMSE Training: 11.043547940190072\n",
            "\tRMSE Validation: 10.652421042846273\n",
            "epoch: 136\n",
            "\tRMSE Training: 10.991743105893017\n",
            "\tRMSE Validation: 10.617438492797561\n",
            "epoch: 139\n",
            "\tRMSE Training: 10.942701104999879\n",
            "\tRMSE Validation: 10.583853357865973\n",
            "epoch: 142\n",
            "\tRMSE Training: 10.893871283435473\n",
            "\tRMSE Validation: 10.551191059594736\n",
            "epoch: 145\n",
            "\tRMSE Training: 10.848094189348819\n",
            "\tRMSE Validation: 10.519763320221053\n",
            "epoch: 148\n",
            "\tRMSE Training: 10.803046913991654\n",
            "\tRMSE Validation: 10.489670960575435\n",
            "epoch: 151\n",
            "\tRMSE Training: 10.759136583030449\n",
            "\tRMSE Validation: 10.460131627454615\n",
            "epoch: 154\n",
            "\tRMSE Training: 10.717102745132841\n",
            "\tRMSE Validation: 10.433409555743383\n",
            "epoch: 157\n",
            "\tRMSE Training: 10.676895699200474\n",
            "\tRMSE Validation: 10.405422415271204\n",
            "epoch: 160\n",
            "\tRMSE Training: 10.637878325096732\n",
            "\tRMSE Validation: 10.38101731782416\n",
            "epoch: 163\n",
            "\tRMSE Training: 10.599591705784894\n",
            "\tRMSE Validation: 10.355118206097405\n",
            "epoch: 166\n",
            "\tRMSE Training: 10.56294598593456\n",
            "\tRMSE Validation: 10.329725741934881\n",
            "epoch: 169\n",
            "\tRMSE Training: 10.527133757805075\n",
            "\tRMSE Validation: 10.304768789252892\n",
            "epoch: 172\n",
            "\tRMSE Training: 10.491867073379964\n",
            "\tRMSE Validation: 10.280955379068338\n",
            "epoch: 175\n",
            "\tRMSE Training: 10.457447323688163\n",
            "\tRMSE Validation: 10.258556811010134\n",
            "epoch: 178\n",
            "\tRMSE Training: 10.424227071523392\n",
            "\tRMSE Validation: 10.236376110865056\n",
            "epoch: 181\n",
            "\tRMSE Training: 10.392565210163259\n",
            "\tRMSE Validation: 10.214359929054995\n",
            "epoch: 184\n",
            "\tRMSE Training: 10.36103904147682\n",
            "\tRMSE Validation: 10.193971754169521\n",
            "epoch: 187\n",
            "\tRMSE Training: 10.330641747165211\n",
            "\tRMSE Validation: 10.17392893998538\n",
            "epoch: 190\n",
            "\tRMSE Training: 10.30061979644765\n",
            "\tRMSE Validation: 10.15417864280161\n",
            "epoch: 193\n",
            "\tRMSE Training: 10.271797541824158\n",
            "\tRMSE Validation: 10.134936133268221\n",
            "epoch: 196\n",
            "\tRMSE Training: 10.243201484287432\n",
            "\tRMSE Validation: 10.117783856097175\n",
            "epoch: 199\n",
            "\tRMSE Training: 10.21588373164485\n",
            "\tRMSE Validation: 10.099853339728408\n",
            "epoch: 202\n",
            "\tRMSE Training: 10.189258604558354\n",
            "\tRMSE Validation: 10.082023074506978\n",
            "epoch: 205\n",
            "\tRMSE Training: 10.163452908501323\n",
            "\tRMSE Validation: 10.064552157084618\n",
            "epoch: 208\n",
            "\tRMSE Training: 10.137714254961939\n",
            "\tRMSE Validation: 10.048146997266036\n",
            "epoch: 211\n",
            "\tRMSE Training: 10.113059630305306\n",
            "\tRMSE Validation: 10.031687748496248\n",
            "epoch: 214\n",
            "\tRMSE Training: 10.088386458750277\n",
            "\tRMSE Validation: 10.015995206455436\n",
            "epoch: 217\n",
            "\tRMSE Training: 10.064931097855718\n",
            "\tRMSE Validation: 10.000607095885886\n",
            "epoch: 220\n",
            "\tRMSE Training: 10.04178552512144\n",
            "\tRMSE Validation: 9.985644439686302\n",
            "epoch: 223\n",
            "\tRMSE Training: 10.019263039062121\n",
            "\tRMSE Validation: 9.970678723588977\n",
            "epoch: 226\n",
            "\tRMSE Training: 9.997239572578113\n",
            "\tRMSE Validation: 9.956205165465654\n",
            "epoch: 229\n",
            "\tRMSE Training: 9.975539499807274\n",
            "\tRMSE Validation: 9.94273304589396\n",
            "epoch: 232\n",
            "\tRMSE Training: 9.954097233006012\n",
            "\tRMSE Validation: 9.928892308994945\n",
            "epoch: 235\n",
            "\tRMSE Training: 9.933051092748968\n",
            "\tRMSE Validation: 9.917206898238042\n",
            "epoch: 238\n",
            "\tRMSE Training: 9.912299686782559\n",
            "\tRMSE Validation: 9.903968701601716\n",
            "epoch: 241\n",
            "\tRMSE Training: 9.892418957852293\n",
            "\tRMSE Validation: 9.891425694036053\n",
            "epoch: 244\n",
            "\tRMSE Training: 9.872570578391212\n",
            "\tRMSE Validation: 9.879408272160848\n",
            "epoch: 247\n",
            "\tRMSE Training: 9.853362831959961\n",
            "\tRMSE Validation: 9.868424034048077\n",
            "epoch: 250\n",
            "\tRMSE Training: 9.834481548226341\n",
            "\tRMSE Validation: 9.856916931382695\n",
            "epoch: 253\n",
            "\tRMSE Training: 9.816511373624541\n",
            "\tRMSE Validation: 9.846299490837913\n",
            "epoch: 256\n",
            "\tRMSE Training: 9.798249262494432\n",
            "\tRMSE Validation: 9.835161209019143\n",
            "epoch: 259\n",
            "\tRMSE Training: 9.780198777095547\n",
            "\tRMSE Validation: 9.823751832765604\n",
            "epoch: 262\n",
            "\tRMSE Training: 9.762558266333583\n",
            "\tRMSE Validation: 9.813150378439387\n",
            "epoch: 265\n",
            "\tRMSE Training: 9.745842790444094\n",
            "\tRMSE Validation: 9.802524230557093\n",
            "epoch: 268\n",
            "\tRMSE Training: 9.729163615828824\n",
            "\tRMSE Validation: 9.792112229772851\n",
            "epoch: 271\n",
            "\tRMSE Training: 9.712312711615853\n",
            "\tRMSE Validation: 9.781963571399924\n",
            "epoch: 274\n",
            "\tRMSE Training: 9.695557699119975\n",
            "\tRMSE Validation: 9.773260531837245\n",
            "epoch: 277\n",
            "\tRMSE Training: 9.679225182327386\n",
            "\tRMSE Validation: 9.763058000399788\n",
            "epoch: 280\n",
            "\tRMSE Training: 9.663230566455713\n",
            "\tRMSE Validation: 9.753247537384551\n",
            "epoch: 283\n",
            "\tRMSE Training: 9.647582402973852\n",
            "\tRMSE Validation: 9.743568802370019\n",
            "epoch: 286\n",
            "\tRMSE Training: 9.632065320275782\n",
            "\tRMSE Validation: 9.73406522266656\n",
            "epoch: 289\n",
            "\tRMSE Training: 9.616995555429495\n",
            "\tRMSE Validation: 9.724378577056514\n",
            "epoch: 292\n",
            "\tRMSE Training: 9.601695550886056\n",
            "\tRMSE Validation: 9.714703579715268\n",
            "epoch: 295\n",
            "\tRMSE Training: 9.586470023976895\n",
            "\tRMSE Validation: 9.70530558604121\n",
            "epoch: 298\n",
            "\tRMSE Training: 9.571460026098938\n",
            "\tRMSE Validation: 9.696272150673419\n",
            "epoch: 301\n",
            "\tRMSE Training: 9.557051665338943\n",
            "\tRMSE Validation: 9.687709997118068\n",
            "epoch: 304\n",
            "\tRMSE Training: 9.542938587071477\n",
            "\tRMSE Validation: 9.678959686957645\n",
            "epoch: 307\n",
            "\tRMSE Training: 9.528756016460457\n",
            "\tRMSE Validation: 9.671077191546127\n",
            "epoch: 310\n",
            "\tRMSE Training: 9.514815374794653\n",
            "\tRMSE Validation: 9.662790051895682\n",
            "epoch: 313\n",
            "\tRMSE Training: 9.500977850089587\n",
            "\tRMSE Validation: 9.654033893897722\n",
            "epoch: 316\n",
            "\tRMSE Training: 9.487415349312773\n",
            "\tRMSE Validation: 9.645891712208176\n",
            "epoch: 319\n",
            "\tRMSE Training: 9.473911876963097\n",
            "\tRMSE Validation: 9.637781224215265\n",
            "epoch: 322\n",
            "\tRMSE Training: 9.460603165372484\n",
            "\tRMSE Validation: 9.630447735473181\n",
            "epoch: 325\n",
            "\tRMSE Training: 9.447057407086891\n",
            "\tRMSE Validation: 9.621997202783378\n",
            "epoch: 328\n",
            "\tRMSE Training: 9.43421511842434\n",
            "\tRMSE Validation: 9.614281221152119\n",
            "epoch: 331\n",
            "\tRMSE Training: 9.421436141420607\n",
            "\tRMSE Validation: 9.606818537143244\n",
            "epoch: 334\n",
            "\tRMSE Training: 9.408855055237916\n",
            "\tRMSE Validation: 9.59945078271286\n",
            "epoch: 337\n",
            "\tRMSE Training: 9.39616576686224\n",
            "\tRMSE Validation: 9.592064387066172\n",
            "epoch: 340\n",
            "\tRMSE Training: 9.383950457955551\n",
            "\tRMSE Validation: 9.585037151979849\n",
            "epoch: 343\n",
            "\tRMSE Training: 9.371856400731284\n",
            "\tRMSE Validation: 9.577881475937993\n",
            "epoch: 346\n",
            "\tRMSE Training: 9.359896944118058\n",
            "\tRMSE Validation: 9.570592886083661\n",
            "epoch: 349\n",
            "\tRMSE Training: 9.34805951355763\n",
            "\tRMSE Validation: 9.563665882046408\n",
            "epoch: 352\n",
            "\tRMSE Training: 9.33629242176994\n",
            "\tRMSE Validation: 9.556759114839673\n",
            "epoch: 355\n",
            "\tRMSE Training: 9.324977981002766\n",
            "\tRMSE Validation: 9.549911620125696\n",
            "epoch: 358\n",
            "\tRMSE Training: 9.313496832553945\n",
            "\tRMSE Validation: 9.54317643847586\n",
            "epoch: 361\n",
            "\tRMSE Training: 9.302505929209493\n",
            "\tRMSE Validation: 9.536798076454026\n",
            "epoch: 364\n",
            "\tRMSE Training: 9.291408037317005\n",
            "\tRMSE Validation: 9.530016863895595\n",
            "epoch: 367\n",
            "\tRMSE Training: 9.280393601270358\n",
            "\tRMSE Validation: 9.52329343570812\n",
            "epoch: 370\n",
            "\tRMSE Training: 9.26940479793233\n",
            "\tRMSE Validation: 9.516598353707876\n",
            "epoch: 373\n",
            "\tRMSE Training: 9.258909146589678\n",
            "\tRMSE Validation: 9.510234821475452\n",
            "epoch: 376\n",
            "\tRMSE Training: 9.248374496201883\n",
            "\tRMSE Validation: 9.504470706876331\n",
            "epoch: 379\n",
            "\tRMSE Training: 9.238241876366255\n",
            "\tRMSE Validation: 9.498206229433263\n",
            "epoch: 382\n",
            "\tRMSE Training: 9.227799470395489\n",
            "\tRMSE Validation: 9.491897436214272\n",
            "epoch: 385\n",
            "\tRMSE Training: 9.217496314517296\n",
            "\tRMSE Validation: 9.485721389825944\n",
            "epoch: 388\n",
            "\tRMSE Training: 9.207490741397208\n",
            "\tRMSE Validation: 9.479799216314618\n",
            "epoch: 391\n",
            "\tRMSE Training: 9.197681253064045\n",
            "\tRMSE Validation: 9.473819239420786\n",
            "epoch: 394\n",
            "\tRMSE Training: 9.187950910267684\n",
            "\tRMSE Validation: 9.467889049217494\n",
            "epoch: 397\n",
            "\tRMSE Training: 9.1782585644692\n",
            "\tRMSE Validation: 9.462902700894446\n",
            "epoch: 400\n",
            "\tRMSE Training: 9.168833077333232\n",
            "\tRMSE Validation: 9.457586545318772\n",
            "epoch: 403\n",
            "\tRMSE Training: 9.159170927184624\n",
            "\tRMSE Validation: 9.452155855292949\n",
            "epoch: 406\n",
            "\tRMSE Training: 9.149761844294991\n",
            "\tRMSE Validation: 9.446813128397253\n",
            "epoch: 409\n",
            "\tRMSE Training: 9.14052736172914\n",
            "\tRMSE Validation: 9.441378737844213\n",
            "epoch: 412\n",
            "\tRMSE Training: 9.131408421904418\n",
            "\tRMSE Validation: 9.436031572901687\n",
            "epoch: 415\n",
            "\tRMSE Training: 9.122488543247044\n",
            "\tRMSE Validation: 9.431180526085976\n",
            "epoch: 418\n",
            "\tRMSE Training: 9.113644407880184\n",
            "\tRMSE Validation: 9.425796528718891\n",
            "epoch: 421\n",
            "\tRMSE Training: 9.104749689480228\n",
            "\tRMSE Validation: 9.420443249812427\n",
            "epoch: 424\n",
            "\tRMSE Training: 9.096022624778\n",
            "\tRMSE Validation: 9.415683478237153\n",
            "epoch: 427\n",
            "\tRMSE Training: 9.087240639346403\n",
            "\tRMSE Validation: 9.410288684557788\n",
            "epoch: 430\n",
            "\tRMSE Training: 9.07872755061185\n",
            "\tRMSE Validation: 9.405153215624056\n",
            "epoch: 433\n",
            "\tRMSE Training: 9.070204943997666\n",
            "\tRMSE Validation: 9.400587447879404\n",
            "epoch: 436\n",
            "\tRMSE Training: 9.06169772000574\n",
            "\tRMSE Validation: 9.39531401922431\n",
            "epoch: 439\n",
            "\tRMSE Training: 9.053478362248697\n",
            "\tRMSE Validation: 9.390306676324133\n",
            "epoch: 442\n",
            "\tRMSE Training: 9.045155849892064\n",
            "\tRMSE Validation: 9.386338969193897\n",
            "epoch: 445\n",
            "\tRMSE Training: 9.036796041281297\n",
            "\tRMSE Validation: 9.381393406524968\n",
            "epoch: 448\n",
            "\tRMSE Training: 9.02860511831776\n",
            "\tRMSE Validation: 9.37643020668863\n",
            "epoch: 451\n",
            "\tRMSE Training: 9.020314393252429\n",
            "\tRMSE Validation: 9.371419743732666\n",
            "epoch: 454\n",
            "\tRMSE Training: 9.012105059385728\n",
            "\tRMSE Validation: 9.366377201961365\n",
            "epoch: 457\n",
            "\tRMSE Training: 9.004282679664882\n",
            "\tRMSE Validation: 9.361413634176236\n",
            "epoch: 460\n",
            "\tRMSE Training: 8.99655896309953\n",
            "\tRMSE Validation: 9.35664312279373\n",
            "epoch: 463\n",
            "\tRMSE Training: 8.98872268864806\n",
            "\tRMSE Validation: 9.352167617601234\n",
            "epoch: 466\n",
            "\tRMSE Training: 8.981016510786299\n",
            "\tRMSE Validation: 9.347634616676624\n",
            "epoch: 469\n",
            "\tRMSE Training: 8.973380679219844\n",
            "\tRMSE Validation: 9.342841341127073\n",
            "epoch: 472\n",
            "\tRMSE Training: 8.96577678081043\n",
            "\tRMSE Validation: 9.338196510338884\n",
            "epoch: 475\n",
            "\tRMSE Training: 8.958260993882268\n",
            "\tRMSE Validation: 9.333893827299608\n",
            "epoch: 478\n",
            "\tRMSE Training: 8.95077492298198\n",
            "\tRMSE Validation: 9.329238421530405\n",
            "epoch: 481\n",
            "\tRMSE Training: 8.943372590661125\n",
            "\tRMSE Validation: 9.324949000116156\n",
            "epoch: 484\n",
            "\tRMSE Training: 8.93618140091601\n",
            "\tRMSE Validation: 9.320441255794037\n",
            "epoch: 487\n",
            "\tRMSE Training: 8.928869544768467\n",
            "\tRMSE Validation: 9.316295836929179\n",
            "epoch: 490\n",
            "\tRMSE Training: 8.921658213547392\n",
            "\tRMSE Validation: 9.312492706489385\n",
            "epoch: 493\n",
            "\tRMSE Training: 8.91450622782787\n",
            "\tRMSE Validation: 9.308392060577912\n",
            "epoch: 496\n",
            "\tRMSE Training: 8.90748644818547\n",
            "\tRMSE Validation: 9.30420827314751\n",
            "epoch: 499\n",
            "\tRMSE Training: 8.900464126096564\n",
            "\tRMSE Validation: 9.300031443426754\n",
            "Fitting Done!\n",
            "\tRMSE Testing: 9.489869731255855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffvx1P9i34te",
        "colab_type": "code",
        "outputId": "6353b05a-6f1b-4fd9-cbf4-fa33bef033ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "Train_draw=[]\n",
        "Valid_draw=[]\n",
        "for i in range(0,len(train_mse)):\n",
        "  if i%1==0:\n",
        "    Train_draw.append(train_mse[i])\n",
        "    Valid_draw.append(validation_mse[i])\n",
        "\n",
        "plt.title(\"Training vs. Validation Error\", fontsize=14)\n",
        "#plt.legend(loc=\"lower right\")\n",
        "plt.plot(Train_draw[1:], markersize=10, label=\"Training Error\",color='red')\n",
        "plt.plot(Valid_draw[1:], markersize=10, label=\"Validation Error\",color='blue')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig('Training vs Testing Error.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEJCAYAAACNNHw2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVIh7VtcZW2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}