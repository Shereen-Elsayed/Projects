{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2211_Conn_Beg_EventLSTM_validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "TensorFlow-GPU-1.13",
      "language": "python",
      "name": "tf-gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PuuRX58dsI_3",
        "outputId": "403d7cce-f978-4bd1-bf32-47cd0bc3b813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lVbzKDghsFuf",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.version\n",
        "#Import Libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "import tensorflow.contrib.learn as tflearn\n",
        "import tensorflow.contrib.layers as tflayers\n",
        "from tensorflow.contrib.learn.python.learn import learn_runner\n",
        "import tensorflow.contrib.metrics as metrics\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "from random import shuffle\n",
        "#TF Version\n",
        "tf.__version__\n",
        "\n",
        "#with warnings.catch_warnings():\n",
        "#    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "#    import h5py\n",
        "\n",
        "#num_periods_output = 4 #to predict\n",
        "#num_periods_input=8 #input\n",
        "\n",
        "ALL_Test_Data=[]\n",
        "ALL_Test_Prediction=[]\n",
        "Event_Based_StartIndex=16\n",
        "Number_of_EventBased=21\n",
        "Number_of_TimeFeatures=16\n",
        "\n",
        "\n",
        "No_Of_weeks=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7VWsYwOysFuq"
      },
      "source": [
        "<h5>Preprocessing data</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TnP7bj4HsFut",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "def preprocessing(df_,num_features):\n",
        "    \n",
        "    if df_.ID[0]!=7 and df_.ID[0]!=8:\n",
        "        print(df_.Year.unique())\n",
        "        df=df_[(df_.Year==2017) | (df_.Year==2018)]\n",
        "        #.isin(years)\n",
        "        #print(df.loc[df['Year'].isin([2017,2018])])\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    else:\n",
        "        df=df_[(df_.Year==2015) | (df_.Year==2016)]\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    \n",
        "    '''df_=df[['ID','Occupancy','Year', 'Month', 'Day', 'Hour','Minute', 'Capacity', \n",
        "    'DayOfWeek','IsWeekend', 'temperature', 'dew_point', 'humidity', 'wind_speed', \n",
        "    'feels_like', 'Status', 'light_snow','snow_shower', 'fog', \n",
        "    'thunder', 'mostly_cloudy','rain', 'heavy_rain', 'mist', 'shallow_fog','light_freezing_rain',\n",
        "    'partly_cloudy', 'haze', 'light_rain', 'rain_shower', 'snow', 'light_snow_shower']]'''\n",
        "    \n",
        "    \n",
        "    # select features\n",
        "    df=df[['ID','Occupancy','Year', 'Month', 'Day', 'Hour','Minute', \n",
        "    'DayOfWeek', 'temperature', 'dew_point', 'humidity', 'wind_speed', 'feels_like', 'Status','IsWeekend', 'light_snow','snow_shower', 'fog', \n",
        "    'thunder', 'mostly_cloudy','rain', 'heavy_rain', 'mist', 'shallow_fog','light_freezing_rain', 'partly_cloudy',\n",
        "     'haze', 'light_rain', 'rain_shower', 'snow', 'light_snow_shower','Events_Rugby','Events_Football','Events_Other_Sport','Events_Exhibitions']]\n",
        "    \n",
        "    \n",
        "    ################################################encoding########################\n",
        "    df['Occupancy'] = pd.to_numeric(df['Occupancy'],errors='coerce')\n",
        "    df['Occupancy'] = df['Occupancy'].abs()\n",
        "    \n",
        "    Status=df.pop('Status')\n",
        "    df.loc[:,'Status_filling']=(Status=='Filling')*1.0\n",
        "    df.loc[:,'Status_static']=(Status=='Static')*1.0\n",
        "    df.loc[:,'Status_emptying']=(Status=='Emptying')*1.0\n",
        "    #DayOfWeek=df.pop('DayOfWeek')\n",
        "    #df.loc[:,'DayOfWeek_0']=(DayOfWeek==0)*1.0\n",
        "    #df.loc[:,'DayOfWeek_1']=(DayOfWeek==1)*1.0\n",
        "    #df.loc[:,'DayOfWeek_2']=(DayOfWeek==2)*1.0\n",
        "    #df.loc[:,'DayOfWeek_3']=(DayOfWeek==3)*1.0\n",
        "    #df.loc[:,'DayOfWeek_4']=(DayOfWeek==4)*1.0\n",
        "    #df.loc[:,'DayOfWeek_5']=(DayOfWeek==5)*1.0\n",
        "    #df.loc[:,'DayOfWeek_6']=(DayOfWeek==6)*1.0\n",
        "    \n",
        "    Number_Of_Features=num_features\n",
        "    df=df.values\n",
        "    df = df.astype('float32')\n",
        "    split=num_periods_output+num_periods_input\n",
        "    \n",
        "    \n",
        "    ##################################SPLIT##############################################\n",
        "    print('LEN DF BEFORE CUTTING ANYTHING',len(df))\n",
        "     ########################## SPLITTING FOR TESTING & VALIDATION ##########################\n",
        "    #test_len=np.floor(len(df)*0.2)\n",
        "    test_val_len=np.floor(len(df)*0.2)\n",
        "    #mod=test_len%(num_periods_input+num_periods_output)\n",
        "    mod=test_val_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    test_val_len=int(test_val_len-mod)\n",
        "    Test_Val=df[(len(df)-test_val_len):,:]\n",
        "    \n",
        "    ############################ VALIDATION & TESTING ##################################\n",
        "    valid_len=np.floor(len(Test_Val)*0.5)\n",
        "    Valid=Test_Val[0:(len(Test_Val)-int(valid_len)),:]\n",
        "    Test=Test_Val[(len(Test_Val)-int(valid_len)):,:]\n",
        "    \n",
        "    ########################### SPLITTING FOR TRAIN ###########################\n",
        "    \n",
        "    new_cutted_df=df[:(len(df)-test_val_len),:]\n",
        "    Start_train_index=int(12*24*7*No_Of_weeks)\n",
        "    #print('instances',Start_train_index)\n",
        "    remain=len(new_cutted_df)-Start_train_index\n",
        "    #print('len(new_cutted_df)-Start_train_index',remain)\n",
        "    #print('len(new_cutted_df)',len(new_cutted_df))\n",
        "    Train=new_cutted_df[len(new_cutted_df)-Start_train_index:,:]\n",
        "    #print('Train',Train[0])\n",
        "    #print('len(Train)',len(Train))\n",
        "    #print(Train.shape)\n",
        "    train_len=len(Train)\n",
        "    mod=train_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    train_len=int(train_len-mod)\n",
        "    Train=Train[0:train_len,:]\n",
        "    #print('len Train',len(Train))\n",
        "   \n",
        "        \n",
        "    #############################  Normalization on train and validation separatly  #############\n",
        "     \n",
        "    ID_Train=Train[:,0]\n",
        "    Train=np.delete(Train,[0],1)\n",
        "    #x_batches=x_batches.drop(columns=['ID'], axis=1)\n",
        "    occ_Train=Train[:,0]\n",
        "    Train=np.delete(Train,[0],1)\n",
        "    #x_batches=x_batches.drop(columns=['Occupancy'], axis=1)\n",
        "    #normalizing data\n",
        "    Train = Train.astype('float32')\n",
        "    normalizer = Normalizer().fit(Train)\n",
        "    Train=normalizer.transform(Train)\n",
        "    \n",
        "    ID_Valid=Valid[:,0]\n",
        "    Valid=np.delete(Valid,[0],1)\n",
        "    #X_test=X_test.drop(columns=['ID'], axis=1)\n",
        "    occ_Valid=Valid[:,0]\n",
        "    Valid=np.delete(Valid,[0],1)\n",
        "    #X_test=X_test.drop(columns=['Occupancy'], axis=1)\n",
        "    Valid = Valid.astype('float32')\n",
        "    Valid=normalizer.transform(Valid)\n",
        "    #------------------\n",
        "    ID_Train=np.reshape(ID_Train,(len(ID_Train),1))\n",
        "    occ_Train=np.reshape(occ_Train,(len(occ_Train),1))\n",
        "    \n",
        "    Train=np.append(occ_Train, Train, axis=1)\n",
        "    Train=np.append(ID_Train, Train, axis=1)\n",
        "    #------------------\n",
        "    ID_Valid=np.reshape(ID_Valid,(len(ID_Valid),1))\n",
        "    occ_Valid=np.reshape(occ_Valid,(len(occ_Valid),1))\n",
        "    \n",
        "    Valid=np.append(occ_Valid,Valid, axis=1)\n",
        "    Valid=np.append(ID_Valid, Valid, axis=1)\n",
        "\n",
        "    ############################################ TRAIN minibatches ##################################\n",
        "    \n",
        "    end=len(Train)\n",
        "    start=0\n",
        "    next=0\n",
        "    x_batches=[]\n",
        "    y_batches=[]\n",
        "    \n",
        "    count=0\n",
        "    #print('lennnn',len(Train))\n",
        "    while next+(num_periods_input+num_periods_output)<end:\n",
        "        next=start+num_periods_input\n",
        "        x_batches.append(Train[start:next,:])\n",
        "        y_batches.append(Train[next:next+num_periods_output,1])\n",
        "        start=start+1\n",
        "    y_batches=np.asarray(y_batches)\n",
        "    y_batches = y_batches.reshape(-1, num_periods_output, 1)   \n",
        "    x_batches=np.asarray(x_batches)\n",
        "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "    print('len x_batches ',len(x_batches))\n",
        "    \n",
        "    ############################################ VALID minibatches ##################################\n",
        "    \n",
        "    end_val=len(Valid)\n",
        "    start_val=0\n",
        "    next_val=0\n",
        "    x_validbatches=[]\n",
        "    y_validbatches=[]\n",
        "    \n",
        "    while next_val+(num_periods_input+num_periods_output)<end_val:\n",
        "        next_val=start_val+num_periods_input\n",
        "        x_validbatches.append(Valid[start_val:next_val,:])\n",
        "        y_validbatches.append(Valid[next_val:next_val+num_periods_output,1])\n",
        "        start_val=start_val+1\n",
        "    y_validbatches=np.asarray(y_validbatches)\n",
        "    y_validbatches = y_validbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_validbatches=np.asarray(x_validbatches)\n",
        "    x_validbatches = x_validbatches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "\n",
        "    ###########################################TEST#####################################\n",
        "    \n",
        "    ID_Test=Test[:,0]\n",
        "    Test=np.delete(Test,[0],1)\n",
        "    #X_test=X_test.drop(columns=['ID'], axis=1)\n",
        "    occ_Test=Test[:,0]\n",
        "    Test=np.delete(Test,[0],1)\n",
        "    #X_test=X_test.drop(columns=['Occupancy'], axis=1)\n",
        "    Test = Test.astype('float32')\n",
        "    Test=normalizer.transform(Test)\n",
        "    \n",
        "    #------------------\n",
        "    ID_Test=np.reshape(ID_Test,(len(ID_Test),1))\n",
        "    occ_Test=np.reshape(occ_Test,(len(occ_Test),1))\n",
        "    \n",
        "    Test=np.append(occ_Test,Test, axis=1)\n",
        "    Test=np.append(ID_Test, Test, axis=1)\n",
        "    \n",
        "    ############################################ TEST minibatches ##################################\n",
        "    end_test=len(Test)\n",
        "    start_test=0\n",
        "    next_test=0\n",
        "    x_testbatches=[]\n",
        "    y_testbatches=[]\n",
        "    \n",
        "    \n",
        "    #print('lennnn',len(Train))\n",
        "    while next_test+(num_periods_input+num_periods_output)<end_test:\n",
        "        next_test=start_test+num_periods_input\n",
        "        x_testbatches.append(Test[start_test:next_test,:])\n",
        "        y_testbatches.append(Test[next_test:next_test+num_periods_output,1])\n",
        "        start_test=start_test+1\n",
        "    y_testbatches=np.asarray(y_testbatches)\n",
        "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_testbatches=np.asarray(x_testbatches)\n",
        "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features) \n",
        "    print('len Test',len(Test))\n",
        "    print('len xTestbatches',len(x_testbatches))\n",
        "    ######################## Sampling##########################################\n",
        "    \n",
        "    #x_batches, y_batches, x_validbatches, y_validbatches, x_testbatches, y_testbatches\n",
        "    \n",
        "    return x_batches, y_batches,x_validbatches, y_validbatches, x_testbatches, y_testbatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TmhRJEHVsFu1",
        "colab": {}
      },
      "source": [
        "def load_locationfiles(path,loc_id):\n",
        "    filename=path + '/BN00'+str(loc_id)+'.csv'\n",
        "    print(filename)\n",
        "    data_loc=pd.read_csv(filename)\n",
        "    #mod=len(data_loc)%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    #data_loc=data_loc[:len(data_loc)-mod]\n",
        "    return data_loc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7MYuE1zOsFu8"
      },
      "source": [
        "# Creating Tensorflow Graph + Run Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NKje0imlsFu_"
      },
      "source": [
        "##### Training Params:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nso4-lghsFvB",
        "colab": {}
      },
      "source": [
        "##### Prediction Params #####\n",
        "num_periods_output = 12    #to predict\n",
        "num_periods_input=24       #input\n",
        "#num_periods = 4           #number of periods per vector we are using to predict one period ahead\n",
        "\n",
        "##### Graph Params #####\n",
        "inputs = Number_of_TimeFeatures-1                #number of vectors submitted\n",
        "hidden = 128               #number of neurons we will recursively work through, can be changed to improve accuracy\n",
        "hidden_event=128\n",
        "output = 1                 #number of output vectors\n",
        "num_layers=1\n",
        "num_layers_EventBased=1\n",
        "\n",
        "##### Static Features #####\n",
        "Number_Of_Static_Features=18\n",
        "no_sequences=256            #re-iterating factor for static features\n",
        "\n",
        "##### Optimization Params #####\n",
        "batchsize=256\n",
        "l_rate = 0.00004          #small learning rate so we don't overshoot the minimum\n",
        "keep_prob_static=0.7       #number of epochs using the constant init_learning_rate\n",
        "keep_prob_event=0.7\n",
        "lamda=0.3                #regularization param \n",
        "keep_probab=0.95            #(1-dropout_rate)\n",
        "keep_prob_testval=1.0      #(1-dropout_rate) for testing and validation runs\n",
        "epochs = 130                #number of iterations or training cycles, includes both the FeedFoward and Backpropogation\n",
        "\n",
        "\n",
        "#where to save logs\n",
        "#logs_path = '/tmp/tf_logs_3_LSTM_Fully_0410_ConnEnd_Shift_FSTop3/example/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BkLBQDYosFvI",
        "colab": {}
      },
      "source": [
        "# model decaying learning rate, instead of fixed one\n",
        "#def compute_LR(init_epoch,max_epoch,learning_rate_decay,init_learning_rate):\n",
        "#    LR_to_use = [\n",
        "#        init_learning_rate * (\n",
        "#            learning_rate_decay ** max(float(i+1-init_epoch),0.0)\n",
        "#        ) for i in range(max_epoch)\n",
        "#    ]\n",
        "#    print(\"Middle LR:\", LR_to_use[len(LR_to_use) // 2])\n",
        "#    return LR_to_use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IWGs1v9usFvO",
        "outputId": "7aebcbe5-e29d-4881-c980-5c1fef0fd19c",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_mse=[]\n",
        "validation_mse=[]\n",
        "test_mse=[]\n",
        "tf.reset_default_graph()   #We didn't have any previous graph objects running, but this would reset the graphs\n",
        "\n",
        "\n",
        "#=============================LSTM part===========================================\n",
        "\n",
        "lstm_graph=tf.Graph()\n",
        "with lstm_graph.as_default():\n",
        "    X = tf.placeholder(tf.float32, [None, num_periods_input, inputs])   #create variable objects\n",
        "    y = tf.placeholder(tf.float32, [None, num_periods_output, output])\n",
        "    learning_rate=tf.placeholder(tf.float32, None)\n",
        "    #batchsize = tf.placeholder(tf.int16)\n",
        "    #dropout = tf.placeholder(tf.float32)\n",
        "    is_train = tf.placeholder(tf.bool)\n",
        "    EventBased_X=tf.placeholder(tf.float32, [None,num_periods_input,Number_of_EventBased])\n",
        "    static_X = tf.placeholder(tf.float32, [None,Number_Of_Static_Features])\n",
        "    keep_prob = tf.placeholder(tf.float32, None)\n",
        "\n",
        "    \n",
        "    #====================================================================================\n",
        "    #=========================Static Features part=======================================\n",
        "\n",
        "    Layer1 = tf.layers.dense(static_X, units=100,activation=tf.nn.relu)  \n",
        "    Static_Output_1 = tf.layers.dense(Layer1,units= 50,activation=tf.nn.relu) \n",
        "    Static_Output = tf.nn.dropout(Static_Output_1,keep_prob=keep_prob_static) # dropout \n",
        "    #===================================== Combine before LSTM ===========================\n",
        "    Static_Output=tf.reshape(Static_Output, [-1, num_periods_input,50])\n",
        "\n",
        "    Combined_output=tf.concat([X,Static_Output],2)\n",
        "    #========================================= LSTM ===========================================\n",
        "    with tf.variable_scope('Time'):\n",
        "        #create our RNN object\n",
        "        cells = []\n",
        "        for i in range(num_layers):\n",
        "            LSTM_cell=tf.contrib.rnn.LSTMCell(num_units=hidden, activation=tf.nn.relu)\n",
        "            #LSTM_cell=tf.contrib.rnn.DropoutWrapper(LSTM_cell, output_keep_prob=keep_prob)\n",
        "            cells.append(LSTM_cell )\n",
        "\n",
        "        cells = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
        "        #x_norm = tf.layers.batch_normalization(X, training=is_train)\n",
        "        rnn_output, states = tf.nn.dynamic_rnn(cells, Combined_output, dtype=tf.float32)               #choose dynamic over static\n",
        "\n",
        "        # ======= Fully connected after LSTM part==========\n",
        "        stacked_rnn_output = tf.reshape(rnn_output, [-1,(hidden*num_periods_input)])# to be able to concat. the static features\n",
        "\n",
        "        LSTM_Output_1 = tf.layers.dense(stacked_rnn_output, units=100,activation=tf.nn.relu)  \n",
        "        LSTM_Output = tf.nn.dropout(LSTM_Output_1,keep_prob=keep_prob) # dropout\n",
        "    #========================LSTM Event Based============================================\n",
        "    with tf.variable_scope('Event'):\n",
        "        #====================================================================\n",
        "        cells_event = []\n",
        "        for i in range(num_layers_EventBased):\n",
        "            LSTM_cell_event=tf.contrib.rnn.LSTMCell(num_units=hidden_event, activation=tf.nn.relu)\n",
        "            #LSTM_cell_event=tf.contrib.rnn.DropoutWrapper(LSTM_cell_event, output_keep_prob=keep_prob)\n",
        "            cells_event.append(LSTM_cell_event )\n",
        "\n",
        "        cells_event = tf.contrib.rnn.MultiRNNCell(cells_event,state_is_tuple=True)\n",
        "\n",
        "        rnn_output_event, states_event = tf.nn.dynamic_rnn(cells_event,EventBased_X, dtype=tf.float32)               #choose dynamic over static\n",
        "\n",
        "        # ======= Fully connected after Event Based LSTM part===========================================\n",
        "        #===================================================================================\n",
        "        stacked_rnn_output_event = tf.reshape(rnn_output_event, [-1,(hidden_event*num_periods_input)])# to be able to concat. the static features\n",
        "\n",
        "        LSTM_Output_event_1 = tf.layers.dense(stacked_rnn_output_event, units=100,activation=tf.nn.relu)  \n",
        "        LSTM_Output_event = tf.nn.dropout(LSTM_Output_event_1,keep_prob=keep_prob_event)\n",
        "        #====================================================================================\n",
        "    #=========================Combination part===========================================\n",
        "\n",
        "    Combined_output=tf.concat([LSTM_Output,LSTM_Output_event],1)\n",
        "    stacked_outputs = tf.layers.dense(Combined_output, units=12) #specify the type of layer (dense)\n",
        "    outputs = tf.reshape(stacked_outputs, [-1, num_periods_output, output])          #shape of results\n",
        "    #Regularization part\n",
        "    tv = tf.trainable_variables()\n",
        "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
        "    Error=tf.square(outputs - y)\n",
        "    #Total_err=Error+regularization_cost\n",
        "    loss = tf.reduce_mean(tf.square(outputs - y))+ lamda*regularization_cost   #define the cost function which evaluates the quality of our model\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)          #gradient descent method\n",
        "    gvs = optimizer.compute_gradients(loss)\n",
        "    capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
        "    \n",
        "    #opt to get gradient info:\n",
        "    grads=tf.gradients(loss,tv)\n",
        "    grads=list(zip(grads,tv))\n",
        "    \n",
        "    training_op = optimizer.apply_gradients(capped_gvs)\n",
        "    #training_op = optimizer.minimize(loss)          #train the result of the application of the cost_function                                 \n",
        "    \n",
        "    #Before running session, set variables to track in Tensorboard\n",
        "    #with lstm_graph.as_default():\n",
        "    saver = tf.train.Saver()\n",
        "    #Creat Summary to monitor scalar values (loss):\n",
        "    #tf.summary.scalar(\"loss\", loss)\n",
        "    \n",
        "    # Create a summary to monitor prediction tensor\n",
        "    #tf.summary.histogram(\"Prediction\", predictions)\n",
        "    \n",
        "    # Gradient Info\n",
        "    for var in tv:\n",
        "        tf.summary.histogram(var.name,var)\n",
        "    \n",
        "    # Merge all summaries into a single op\n",
        "    merged_summary_op = tf.summary.merge_all()\n",
        "\n",
        "#====================================================================================\n",
        "\n",
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    #batch_size=\n",
        "    X_returned=[]\n",
        "    Y_returned=[]\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        if (start + batch_size)>len(features):\n",
        "            break\n",
        "        else:\n",
        "            end = start + batch_size\n",
        "        X_returned.append(features[start:end])\n",
        "        Y_returned.append(labels[start:end])\n",
        "    return X_returned, Y_returned\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Static_Features=[[1,0,0,0,0,0,0,0,1,3,4,6,7,19,9,17,51.3787,-2.3622],[0,1,0,0,0,0,0,0,0,3,9,11,3,29,10,13,51.3843,-2.3686],\n",
        "                 [0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,51.4113,-2.3869],[0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,51.3902,-2.4059],\n",
        "                 [0,0,0,0,1,0,0,0,0,0,0,4,2,3,0,0,51.3529,-2.3838],[0,0,0,0,0,1,0,0,2,4,8,6,3,39,13,14,51.3842,-2.3590],\n",
        "                 [0,0,0,0,0,0,1,0,1,1,3,6,6,17,9,7,51.3782,-2.3589],[0,0,0,0,0,0,0,1,1,2,3,6,6,18,9,7,51.3783,-2.3593]]\n",
        "\n",
        "#Static_Features=[    [1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],\n",
        "#                     [0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0]\n",
        "#                    ,[0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],\n",
        "#                     [0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]]\n",
        "\n",
        "\n",
        "\n",
        "# COMPUTE LEARNING RATE schedule beforehand\n",
        "#LR_schedule = compute_LR(init_epoch,epochs,learning_rate_decay,initial_LR) \n",
        "with tf.Session(graph=lstm_graph) as sess:    \n",
        "    \n",
        "    init = tf.global_variables_initializer()           #initialize all the variables\n",
        "    init.run()\n",
        "    \n",
        "    # creating the writer inside the session\n",
        "    #writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
        "    \n",
        "    #### LOOP OVER LOCATIONS ####\n",
        "    data_path=r'/content/drive/My Drive/FINAL_DATA_EVENTS'\n",
        "    #r'/content/drive/My Drive/FINAL_DATA_EVENTS'\n",
        "    #r'D:\\Users\\danie\\Desktop\\jupyter_notebooks\\Project\\FINAL_DATA_EVENTS'\n",
        "    data_All=pd.DataFrame()\n",
        "    x_batches_Full=[]\n",
        "    y_batches_Full=[]\n",
        "    X_Valid_Full=[]\n",
        "    Y_Valid_Full=[]\n",
        "    X_Test_Full=[]\n",
        "    Y_Test_Full=[]\n",
        "    for loc_id in range(1,9):\n",
        "        #========\n",
        "        data=load_locationfiles(data_path,loc_id)\n",
        "        header=list(data.columns.values)\n",
        "        data=pd.DataFrame(data,columns=header)\n",
        "        x_batches, y_batches, X_Valid, Y_Valid,X_Test,Y_Test=preprocessing(data,(Number_of_TimeFeatures+Number_of_EventBased))\n",
        "        #===============================\n",
        "        for element1 in (x_batches):\n",
        "            x_batches_Full.append(element1)\n",
        "            \n",
        "        for element2 in (y_batches):\n",
        "            y_batches_Full.append(element2)\n",
        "            \n",
        "        for element3 in (X_Valid):\n",
        "            X_Valid_Full.append(element3)\n",
        "            \n",
        "        for element4 in (Y_Valid):\n",
        "            Y_Valid_Full.append(element4)\n",
        "            \n",
        "        for element5 in (X_Test):\n",
        "            X_Test_Full.append(element5)\n",
        "            \n",
        "        for element6 in (Y_Test):\n",
        "            Y_Test_Full.append(element6)            \n",
        "    \n",
        "    for ep in range(epochs):\n",
        "        #CALCULATE LR TO USE!!\n",
        "        #curr_lr=LR_schedule[ep]\n",
        "        Training_Error=[]\n",
        "        #even though NOT SHUFFLED YET\n",
        "        shuffled_batch_features,shuffled_batch_y=batch_features_labels(x_batches_Full,y_batches_Full,batchsize)\n",
        "        #---------------------shuffle minibatches X and Y together-------------------------------------\n",
        "        combined = list(zip(shuffled_batch_features, shuffled_batch_y))\n",
        "        random.shuffle(combined)\n",
        "        shuffled_batch_features[:], shuffled_batch_y[:] = zip(*combined)\n",
        "        #===========================================================================================\n",
        "        print(len(shuffled_batch_features))\n",
        "        \n",
        "        for i in range(0,len(shuffled_batch_features)): \n",
        "            #print('====================')\n",
        "            batch_features=shuffled_batch_features[i]\n",
        "            #print(batch_features)\n",
        "            batch_y=shuffled_batch_y[i]\n",
        "            static_train=[]\n",
        "            Event_Based_Train=[]\n",
        "            \n",
        "            #print('Before drop',batch_features[0][0][0])\n",
        "            for h in range(0,no_sequences):\n",
        "                Event_window=[]\n",
        "                for k in range(0,num_periods_input):\n",
        "                    loc_id=int(batch_features[h][k][0])\n",
        "                    static_train.append(Static_Features[loc_id-1][:]) # \n",
        "                    Event_window.append(batch_features[h][k][Event_Based_StartIndex:])                    \n",
        "                Event_Based_Train.append(Event_window)\n",
        "                \n",
        "            EventBased_EndIndex=Event_Based_StartIndex+Number_of_EventBased\n",
        "            Event_Based_index=list(range(Event_Based_StartIndex, EventBased_EndIndex))\n",
        "            #print('THE EVENT BASED:  ',Event_Based_index)\n",
        "            batch_features=np.delete(batch_features,Event_Based_index, axis=2) \n",
        "            batch_features=np.delete(batch_features,[0], axis=2) \n",
        "            \n",
        "            #print('X',len(batch_features))\n",
        "            #print('Y',len(batch_y[0]))\n",
        "            _,Train_Batch_error,summary=sess.run([training_op,Error,merged_summary_op],\n",
        "                                                 feed_dict={\n",
        "                                                     keep_prob:keep_probab,\n",
        "                                                     X: batch_features,\n",
        "                                                     y: batch_y,\n",
        "                                                     learning_rate: l_rate,\n",
        "                                                     static_X:static_train,\n",
        "                                                     EventBased_X:Event_Based_Train})  \n",
        "            Training_Error.append(Train_Batch_error)\n",
        "            \n",
        "        \n",
        "        #write logs (per epoch)\n",
        "        #writer.add_summary(summary, ep)\n",
        "        \n",
        "        if ep % 3 == 0:\n",
        "            ################  evaluate training error\n",
        "            #print('Length of whole:',len(Training_Error),'Length of each: ',len(Training_Error[0][0]))\n",
        "            Sum_train=np.sum(Training_Error,axis=2)\n",
        "            Sum_train_1=np.sum(Sum_train,axis=1)\n",
        "            Sum_train_2=np.sum(Sum_train_1,axis=0)\n",
        "            '''print('Sum_train shape:',Sum_train.shape)\n",
        "            print('Sum_train_1 shape:',Sum_train_1.shape)\n",
        "            print('Sum_train_2 shape:',Sum_train_2.shape)'''\n",
        "            Mean_train=Sum_train_2/(len(Training_Error)*batchsize*num_periods_output)\n",
        "            print(\"epoch:\",int(ep+1))\n",
        "            print(\"\\tRMSE Training:\", (Mean_train)**0.5)\n",
        "            #mse = loss.eval(feed_dict={\n",
        "            #                              X: batch_features,\n",
        "            #                             y: batch_y,\n",
        "            #                            static_X:static_train,\n",
        "            #                           is_train:True})\n",
        "            train_mse.append((Mean_train)**0.5)\n",
        "                \n",
        "            \n",
        "            Validation_Error=[]\n",
        "            unshuffled_valid_batch_features,unshuffled_valid_batch_y=batch_features_labels(X_Valid_Full,Y_Valid_Full,batchsize)\n",
        "            \n",
        "            #---------------------shuffle minibatches X and Y together-------------------------------------\n",
        "            #combined = list(zip(unshuffled_valid_batch_features, unshuffled_valid_batch_y))\n",
        "            #random.shuffle(combined)\n",
        "            #shuffled_batch_features[:], shuffled_batch_y[:] = zip(*combined)\n",
        "            #===========================================================================================\n",
        "            #print(len(unshuffled_valid_batch_features))\n",
        "            \n",
        "            for i in range(0,len(unshuffled_valid_batch_features)):\n",
        "                batch_features_valid=unshuffled_valid_batch_features[i]\n",
        "                #print(batch_features)\n",
        "                batch_y_valid=unshuffled_valid_batch_y[i]\n",
        "                static_Valid=[]\n",
        "                Event_Based_Valid=[]\n",
        "                #print('Before drop',batch_features[0][0][0])\n",
        "                for h in range(0,no_sequences):\n",
        "                    Event_window_valid=[]\n",
        "                    for k in range(0,num_periods_input):\n",
        "                        loc_id=int(batch_features_valid[h][k][0])\n",
        "                        static_Valid.append(Static_Features[loc_id-1][:]) # \n",
        "                        Event_window_valid.append(batch_features_valid[h][k][Event_Based_StartIndex:])\n",
        "                    Event_Based_Valid.append(Event_window_valid)\n",
        "                    \n",
        "                EventBased_EndIndex=Event_Based_StartIndex+Number_of_EventBased\n",
        "                Event_Based_index=list(range(Event_Based_StartIndex, EventBased_EndIndex))\n",
        "                batch_features_valid=np.delete(batch_features_valid,Event_Based_index, axis=2)\n",
        "                batch_features_valid=np.delete(batch_features_valid,[0], axis=2)\n",
        "                \n",
        "                \n",
        "                y_predict,Valid_error_batch=sess.run([outputs,Error], \n",
        "                                                     feed_dict={\n",
        "                                                     keep_prob:keep_prob_testval,\n",
        "                                                     X: batch_features_valid, \n",
        "                                                     y: batch_y_valid,\n",
        "                                                     static_X:static_Valid,\n",
        "                                                     EventBased_X:Event_Based_Valid})\n",
        "                #print('Valid_error_batch',Valid_error_batch)\n",
        "                Validation_Error.append(Valid_error_batch)\n",
        "            \n",
        "            Sum_valid=np.sum(Validation_Error,axis=2)\n",
        "            Sum_valid_1=np.sum(Sum_valid,axis=1)\n",
        "            Sum_valid_2=np.sum(Sum_valid_1,axis=0)\n",
        "            '''print('Sum_train shape:',Sum_valid.shape)\n",
        "            print('Sum_train_1 shape:',Sum_valid_1.shape)\n",
        "            print('Sum_train_2 shape:',Sum_valid_2.shape)'''\n",
        "            Mean_valid=Sum_valid_2/(len(Validation_Error)*batchsize*num_periods_output)\n",
        "            print(\"\\tRMSE Validation:\", (Mean_valid)**0.5)\n",
        "            #mse = loss.eval(feed_dict={\n",
        "            #                              X: batch_features,\n",
        "            #                             y: batch_y,\n",
        "            #                            static_X:static_train,\n",
        "            #                           is_train:True})\n",
        "            #train_mse.append((Mean_train)**0.5)\n",
        "            \n",
        "            validation_mse.append((Mean_valid)**0.5)\n",
        "              \n",
        "    # save training session --> TO RESTORE\n",
        "    saver.save(sess,\"checkpoints-conn_beg/har.ckpt\")\n",
        "    print(\"Run the command line:\\n\" \\\n",
        "          \"--> tensorboard --logdir=/tmp/tf_logs_1_LSTM_Fully_1110_ConnBeg_Shift_FS4/example \" \\\n",
        "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
            "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
            "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
            "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
            "INFO:tensorflow:Summary name Time/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0 is illegal; using Time/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel_0 instead.\n",
            "INFO:tensorflow:Summary name Time/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0 is illegal; using Time/rnn/multi_rnn_cell/cell_0/lstm_cell/bias_0 instead.\n",
            "INFO:tensorflow:Summary name Time/dense/kernel:0 is illegal; using Time/dense/kernel_0 instead.\n",
            "INFO:tensorflow:Summary name Time/dense/bias:0 is illegal; using Time/dense/bias_0 instead.\n",
            "INFO:tensorflow:Summary name Event/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0 is illegal; using Event/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel_0 instead.\n",
            "INFO:tensorflow:Summary name Event/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0 is illegal; using Event/rnn/multi_rnn_cell/cell_0/lstm_cell/bias_0 instead.\n",
            "INFO:tensorflow:Summary name Event/dense/kernel:0 is illegal; using Event/dense/kernel_0 instead.\n",
            "INFO:tensorflow:Summary name Event/dense/bias:0 is illegal; using Event/dense/bias_0 instead.\n",
            "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
            "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN001.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[628]\n",
            "LEN DF BEFORE CUTTING ANYTHING 182935\n",
            "len x_batches  1957\n",
            "len Test 18288\n",
            "len xTestbatches 18229\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN002.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1056]\n",
            "LEN DF BEFORE CUTTING ANYTHING 191475\n",
            "len x_batches  1957\n",
            "len Test 19134\n",
            "len xTestbatches 19075\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN003.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[ 860 1500 1160 1360]\n",
            "LEN DF BEFORE CUTTING ANYTHING 148340\n",
            "len x_batches  1957\n",
            "len Test 14832\n",
            "len xTestbatches 14773\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN004.csv\n",
            "[2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[698]\n",
            "LEN DF BEFORE CUTTING ANYTHING 196905\n",
            "len x_batches  1957\n",
            "len Test 19674\n",
            "len xTestbatches 19615\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN005.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1320 1230]\n",
            "LEN DF BEFORE CUTTING ANYTHING 147111\n",
            "len x_batches  1957\n",
            "len Test 14706\n",
            "len xTestbatches 14647\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN006.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[521 519 525 513]\n",
            "LEN DF BEFORE CUTTING ANYTHING 196381\n",
            "len x_batches  1957\n",
            "len Test 19638\n",
            "len xTestbatches 19579\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN007.csv\n",
            "[2015 2016]\n",
            "[720 626 800 860 700 623 519 528 563 638 603 630 596 602 840]\n",
            "LEN DF BEFORE CUTTING ANYTHING 96914\n",
            "len x_batches  1957\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN008.csv\n",
            "[2015 2016]\n",
            "[140 180 132]\n",
            "LEN DF BEFORE CUTTING ANYTHING 96915\n",
            "len x_batches  1957\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "61\n",
            "epoch: 1\n",
            "\tRMSE Training: [42.23694507]\n",
            "\tRMSE Validation: [30.15371027]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 4\n",
            "\tRMSE Training: [21.92000153]\n",
            "\tRMSE Validation: [19.27949093]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 7\n",
            "\tRMSE Training: [18.93590218]\n",
            "\tRMSE Validation: [16.72740937]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 10\n",
            "\tRMSE Training: [16.79520575]\n",
            "\tRMSE Validation: [14.97692542]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 13\n",
            "\tRMSE Training: [15.22719001]\n",
            "\tRMSE Validation: [13.82273052]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 16\n",
            "\tRMSE Training: [14.17510698]\n",
            "\tRMSE Validation: [12.68604126]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 19\n",
            "\tRMSE Training: [13.14473698]\n",
            "\tRMSE Validation: [11.84302569]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 22\n",
            "\tRMSE Training: [12.59089265]\n",
            "\tRMSE Validation: [11.56381159]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 25\n",
            "\tRMSE Training: [12.44786515]\n",
            "\tRMSE Validation: [11.2698652]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 28\n",
            "\tRMSE Training: [12.10362541]\n",
            "\tRMSE Validation: [11.01227029]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 31\n",
            "\tRMSE Training: [11.75405769]\n",
            "\tRMSE Validation: [10.72695429]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 34\n",
            "\tRMSE Training: [11.52332244]\n",
            "\tRMSE Validation: [10.57955969]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 37\n",
            "\tRMSE Training: [11.36752188]\n",
            "\tRMSE Validation: [10.25499678]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 40\n",
            "\tRMSE Training: [11.08866798]\n",
            "\tRMSE Validation: [10.09631829]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 43\n",
            "\tRMSE Training: [11.02340968]\n",
            "\tRMSE Validation: [10.12182348]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 46\n",
            "\tRMSE Training: [10.85769873]\n",
            "\tRMSE Validation: [10.16013771]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 49\n",
            "\tRMSE Training: [10.87696411]\n",
            "\tRMSE Validation: [10.27207572]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 52\n",
            "\tRMSE Training: [10.5503414]\n",
            "\tRMSE Validation: [9.94825345]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 55\n",
            "\tRMSE Training: [10.60512546]\n",
            "\tRMSE Validation: [10.01869251]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 58\n",
            "\tRMSE Training: [10.28602538]\n",
            "\tRMSE Validation: [9.73595322]\n",
            "61\n",
            "61\n",
            "61\n",
            "epoch: 61\n",
            "\tRMSE Training: [10.19541445]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_irySJICsFvV",
        "colab": {}
      },
      "source": [
        "print('min training RMSE:',min(train_mse))\n",
        "print('min validation RMSE:',min(validation_mse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zeoeBeZxsFvZ"
      },
      "source": [
        "### Plot Training + Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DgI4Lq9asFva",
        "colab": {}
      },
      "source": [
        "Train_draw=[]\n",
        "Valid_draw=[]\n",
        "for i in range(0,len(train_mse)):\n",
        "    if i%1==0:\n",
        "        Train_draw.append(train_mse[i])\n",
        "        Valid_draw.append(validation_mse[i])\n",
        "\n",
        "plt.title(\"Training vs. Validation Error\", fontsize=14)\n",
        "#plt.legend(loc=\"lower right\")\n",
        "plt.plot(Train_draw[1:], markersize=10, label=\"Training Error\",color='red')\n",
        "plt.plot(Valid_draw[1:], markersize=10, label=\"Validation Error\",color='blue')\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig('Training vs Testing Error.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FpSktO3hsFvd"
      },
      "source": [
        "### Testing Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-MESF4QjsFve",
        "colab": {}
      },
      "source": [
        "Test_Error=[]\n",
        "with tf.Session(graph=lstm_graph) as sess: \n",
        "    # Restore\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-conn_beg'))\n",
        "    test_batch_features,test_batch_y=batch_features_labels(X_Test_Full,Y_Test_Full,batchsize)\n",
        "    #---------------------shuffle minibatches X and Y together-------------------------------------\n",
        "    #combined = list(zip(shuffled_batch_features, shuffled_batch_y))\n",
        "    #random.shuffle(combined)\n",
        "    #shuffled_batch_features[:], shuffled_batch_y[:] = zip(*combined)\n",
        "    #===========================================================================================\n",
        "    print(len(test_batch_features))\n",
        "    \n",
        "    for i in range(0,len(test_batch_features)):\n",
        "        batch_features_test=test_batch_features[i]\n",
        "        #print(batch_features)\n",
        "        batch_y_test=test_batch_y[i]\n",
        "        static_test=[]\n",
        "        Event_Based_Test=[]\n",
        "        #print('Before drop',batch_features[0][0][0])\n",
        "        for h in range(0,no_sequences):\n",
        "            Event_window_Test=[]\n",
        "            for k in range(0,num_periods_input):\n",
        "                loc_id=int(batch_features_test[h][k][0])\n",
        "                static_test.append(Static_Features[loc_id-1][:]) # \n",
        "                Event_window_Test.append(batch_features_test[h][k][Event_Based_StartIndex:])\n",
        "            Event_Based_Test.append(Event_window_Test)\n",
        "            \n",
        "        EventBased_EndIndex=Event_Based_StartIndex+Number_of_EventBased\n",
        "        Event_Based_index=list(range(Event_Based_StartIndex, EventBased_EndIndex))\n",
        "        batch_features_test=np.delete(batch_features_test,Event_Based_index, axis=2)  \n",
        "        batch_features_test=np.delete(batch_features_test, [0], axis=2) \n",
        "        #print('X',len(batch_features[0]))\n",
        "        #print('Y',len(batch_y[0]))\n",
        "        Test_Batch_error,y_predict_test,summary=sess.run([Error,outputs,merged_summary_op],\n",
        "                                             feed_dict={\n",
        "                                                 keep_prob:keep_prob_testval,\n",
        "                                                 X: batch_features_test,\n",
        "                                                 y: batch_y_test,\n",
        "                                                 learning_rate: l_rate,\n",
        "                                                 static_X:static_test,\n",
        "                                                 is_train:False,\n",
        "                                                 EventBased_X:Event_Based_Test})\n",
        "\n",
        "        #print('error:',Train_Batch_error[0])\n",
        "        Test_Error.append(Test_Batch_error)\n",
        "\n",
        "    Sum_test=np.sum(Test_Error,axis=2)\n",
        "    Sum_test_1=np.sum(Sum_test,axis=1)\n",
        "    Sum_test_2=np.sum(Sum_test_1,axis=0)\n",
        "    print('Sum_test shape:',Sum_test.shape)\n",
        "    print('Sum_test_1 shape:',Sum_test_1.shape)\n",
        "    print('Sum_test_2 shape:',Sum_test_2.shape) \n",
        "    Mean_test=Sum_test_2/(len(Test_Error)*batchsize*num_periods_output) \n",
        "    print(\"\\tRMSE Testing:\", (Mean_test)**0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "15SGHOrMsFvg",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}