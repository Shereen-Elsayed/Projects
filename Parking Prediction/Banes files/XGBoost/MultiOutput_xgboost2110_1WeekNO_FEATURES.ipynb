{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "MultiOutput_xgboost2110_1WeekNO_FEATURES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_aVTChKIx3",
        "colab_type": "code",
        "outputId": "27a8859c-38a0-407e-925c-070d6e24f85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkZGl_g0KFc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.version\n",
        "#Import Libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "import shutil\n",
        "import tensorflow.contrib.learn as tflearn\n",
        "import tensorflow.contrib.layers as tflayers\n",
        "from tensorflow.contrib.learn.python.learn import learn_runner\n",
        "import tensorflow.contrib.metrics as metrics\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "from random import shuffle\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import xgboost as xgb\n",
        "#TF Version\n",
        "tf.__version__\n",
        "\n",
        "#with warnings.catch_warnings():\n",
        "#    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "#    import h5py\n",
        "\n",
        "num_periods_output = 12 #to predict\n",
        "num_periods_input=24 #input\n",
        "\n",
        "No_Of_weeks=24\n",
        "\n",
        "ALL_Test_Data=[]\n",
        "ALL_Test_Prediction=[]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4odAQaTMKFdB",
        "colab_type": "text"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpCUqQK9KFdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "def preprocessing(df_,num_features):\n",
        "    \n",
        "    if df_.ID[0]!=7 and df_.ID[0]!=8:\n",
        "        print(df_.Year.unique())\n",
        "        df=df_[(df_.Year==2017) | (df_.Year==2018)]\n",
        "        #.isin(years)\n",
        "        #print(df.loc[df['Year'].isin([2017,2018])])\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    else:\n",
        "        df=df_[(df_.Year==2015) | (df_.Year==2016)]\n",
        "        print(df.Year.unique())\n",
        "        print(df.Capacity.unique())\n",
        "    \n",
        "    '''df_=df[['ID','Occupancy','Year', 'Month', 'Day', 'Hour','Minute', 'Capacity', \n",
        "    'DayOfWeek','IsWeekend', 'temperature', 'dew_point', 'humidity', 'wind_speed', \n",
        "    'feels_like', 'Status', 'light_snow','snow_shower', 'fog', \n",
        "    'thunder', 'mostly_cloudy','rain', 'heavy_rain', 'mist', 'shallow_fog','light_freezing_rain',\n",
        "    'partly_cloudy', 'haze', 'light_rain', 'rain_shower', 'snow', 'light_snow_shower']]'''\n",
        "    # select features\n",
        "    df=df[['ID','Occupancy']]\n",
        "    \n",
        "    ################################################encoding########################\n",
        "    df['Occupancy'] = pd.to_numeric(df['Occupancy'],errors='coerce')\n",
        "    df['Occupancy'] = df['Occupancy'].abs()\n",
        "    Number_Of_Features=num_features\n",
        "    df=df.values\n",
        "    df = df.astype('float32')\n",
        "    split=num_periods_output+num_periods_input\n",
        "    \n",
        "    ####################### CUT THE PORTION of the data that we are working on \n",
        "    \n",
        "     ########################## SPLITTING FOR TESTING ##########################\n",
        "    test_len=np.floor(len(df)*0.1)\n",
        "    mod=test_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    test_len=int(test_len-mod)\n",
        "    Test=df[(len(df)-test_len):,:]\n",
        "\n",
        "    ########################### SPLITTING FOR TRAIN ###########################\n",
        "    train_cut=int(np.floor(len(df)*0.2))\n",
        "    new_cutted_df=df[:(len(df)-train_cut),:]\n",
        "    Start_train_index=int(np.floor(12*24*7*No_Of_weeks))\n",
        "    print('instances',Start_train_index)\n",
        "    Train=new_cutted_df[len(new_cutted_df)-Start_train_index:,:]\n",
        "    train_len=len(Train)\n",
        "    mod=train_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    train_len=int(train_len-mod)\n",
        "    Train=Train[0:train_len,:]\n",
        "    print('len Train',len(Train))\n",
        "    \n",
        "    ############################################ TRAIN minibatches ##################################\n",
        "    \n",
        "    end=len(Train)\n",
        "    start=0\n",
        "    next=0\n",
        "    x_batches=[]\n",
        "    y_batches=[]\n",
        "    \n",
        "    count=0\n",
        "    #print('lennnn',len(Train))\n",
        "    while next+(num_periods_input+num_periods_output)<end:\n",
        "        next=start+num_periods_input\n",
        "        x_batches.append(Train[start:next,:])\n",
        "        y_batches.append(Train[next:next+num_periods_output,1])\n",
        "        start=start+1\n",
        "    y_batches=np.asarray(y_batches)\n",
        "    y_batches = y_batches.reshape(-1, num_periods_output, 1)   \n",
        "    x_batches=np.asarray(x_batches)\n",
        "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "    print('len x_batches ',len(x_batches))\n",
        "    \n",
        "    ############################################ TEST minibatches ##################################\n",
        "    end_test=len(Test)\n",
        "    start_test=0\n",
        "    next_test=0\n",
        "    x_testbatches=[]\n",
        "    y_testbatches=[]\n",
        "    \n",
        "    \n",
        "    #print('lennnn',len(Train))\n",
        "    while next_test+(num_periods_input+num_periods_output)<end_test:\n",
        "        next_test=start_test+num_periods_input\n",
        "        x_testbatches.append(Test[start_test:next_test,:])\n",
        "        y_testbatches.append(Test[next_test:next_test+num_periods_output,1])\n",
        "        start_test=start_test+1\n",
        "    y_testbatches=np.asarray(y_testbatches)\n",
        "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_testbatches=np.asarray(x_testbatches)\n",
        "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features) \n",
        "    print('len Test',len(Test))\n",
        "    print('len xTestbatches',len(x_testbatches))\n",
        "    ######################## Sampling##########################################\n",
        "    \n",
        "    #x_batches, y_batches, x_validbatches, y_validbatches, x_testbatches, y_testbatches\n",
        "    \n",
        "    return x_batches, y_batches, x_testbatches, y_testbatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GeTnVETKFdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_locationfiles(path,loc_id):\n",
        "    filename=path + '/BN00'+str(loc_id)+'.csv'\n",
        "    print(filename)\n",
        "    data_loc=pd.read_csv(filename)\n",
        "    #print(data_loc.head())\n",
        "    #mod=len(data_loc)%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    #data_loc=data_loc[:len(data_loc)-mod]\n",
        "    return data_loc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBMD4AcXKFdI",
        "colab_type": "code",
        "outputId": "bda8f73b-23bf-428c-cf4c-036002d749c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "    data_path=r'/content/drive/My Drive/FINAL_DATA_EVENTS'\n",
        "    #r'/home/shero/Desktop/OurProject/BanesData/Model/occupation_loc/'\n",
        "    data_All=pd.DataFrame()\n",
        "    x_batches_Full=[]\n",
        "    y_batches_Full=[]\n",
        "    X_Test_Full=[]\n",
        "    Y_Test_Full=[]\n",
        "    for loc_id in range(1,9):\n",
        "        #========\n",
        "        data=load_locationfiles(data_path,loc_id)\n",
        "        header=list(data.columns.values)\n",
        "        data=pd.DataFrame(data,columns=header)\n",
        "        x_batches, y_batches,X_Test,Y_Test=preprocessing(data,2)\n",
        "        #===============================\n",
        "        for element1 in (x_batches):\n",
        "            x_batches_Full.append(element1)\n",
        "            \n",
        "        for element2 in (y_batches):\n",
        "            y_batches_Full.append(element2)\n",
        "                        \n",
        "        for element5 in (X_Test):\n",
        "            X_Test_Full.append(element5)\n",
        "            \n",
        "        for element6 in (Y_Test):\n",
        "            Y_Test_Full.append(element6)\n",
        "    #---------------------shuffle minibatches X and Y together-------------------------------------\n",
        "    print(len(x_batches_Full),'     length of all file : ',len(y_batches_Full))\n",
        "    combined = list(zip(x_batches_Full, y_batches_Full))\n",
        "    random.shuffle(combined)\n",
        "    shuffled_batch_features, shuffled_batch_y = zip(*combined)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN001.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[628]\n",
            "instances 96768\n",
            "len Train 96768\n",
            "len x_batches  96709\n",
            "len Test 18288\n",
            "len xTestbatches 18229\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN002.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1056]\n",
            "instances 96768\n",
            "len Train 96768\n",
            "len x_batches  96709\n",
            "len Test 19116\n",
            "len xTestbatches 19057\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN003.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[ 860 1500 1160 1360]\n",
            "instances 96768\n",
            "len Train 96768\n",
            "len x_batches  96709\n",
            "len Test 14832\n",
            "len xTestbatches 14773\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN004.csv\n",
            "[2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[698]\n",
            "instances 96768\n",
            "len Train 96768\n",
            "len x_batches  96709\n",
            "len Test 19656\n",
            "len xTestbatches 19597\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN005.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[1320 1230]\n",
            "instances 96768\n",
            "len Train 96768\n",
            "len x_batches  96709\n",
            "len Test 14688\n",
            "len xTestbatches 14629\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN006.csv\n",
            "[2014 2015 2016 2017 2018 2019]\n",
            "[2017 2018]\n",
            "[521 519 525 513]\n",
            "instances 96768\n",
            "len Train 96768\n",
            "len x_batches  96709\n",
            "len Test 19620\n",
            "len xTestbatches 19561\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN007.csv\n",
            "[2015 2016]\n",
            "[720 626 800 860 700 623 519 528 563 638 603 630 596 602 840]\n",
            "instances 96768\n",
            "len Train 19224\n",
            "len x_batches  19165\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "/content/drive/My Drive/FINAL_DATA_EVENTS/BN008.csv\n",
            "[2015 2016]\n",
            "[140 180 132]\n",
            "instances 96768\n",
            "len Train 19224\n",
            "len x_batches  19165\n",
            "len Test 9684\n",
            "len xTestbatches 9625\n",
            "618584      length of all file :  618584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQ2LajOKFdL",
        "colab_type": "code",
        "outputId": "903b8fb5-5f8f-4f46-ed2d-15db0daaf7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "#xgboost part\n",
        "print(len(x_batches_Full))\n",
        "All_Training_Instances=[]\n",
        "\n",
        "Static_Features=[[1,0,0,0,0,0,0,0,1,3,4,6,7,19,9,17,51.3787,-2.3622],[0,1,0,0,0,0,0,0,0,3,9,11,3,29,10,13,51.3843,-2.3686],\n",
        "                     [0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,51.4113,-2.3869],[0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,51.3902,-2.4059]\n",
        "                    ,[0,0,0,0,1,0,0,0,0,0,0,4,2,3,0,0,51.3529,-2.3838],[0,0,0,0,0,1,0,0,2,4,8,6,3,39,13,14,51.3842,-2.3590],\n",
        "                     [0,0,0,0,0,0,1,0,1,1,3,6,6,17,9,7,51.3782,-2.3589],[0,0,0,0,0,0,0,1,1,2,3,6,6,18,9,7,51.3783,-2.3593]]\n",
        " \n",
        "#=============== change each window into Instance =================================\n",
        "for i in range(0,len(shuffled_batch_features)):\n",
        "    hold=[]\n",
        "    temp=[]\n",
        "    for j in range(0,len(shuffled_batch_features[i])):\n",
        "      #print(len(hold))\n",
        "      \n",
        "      if j==(len(shuffled_batch_features[i])-1):\n",
        "          hold=np.concatenate((hold, shuffled_batch_features[i][j][1]), axis=None)       \n",
        "      else:\n",
        "          hold=np.concatenate((hold, shuffled_batch_features[i][j][1]), axis=None)\n",
        "          \n",
        "    #print(hold)\n",
        "    All_Training_Instances.append(hold)\n",
        "    \n",
        "\n",
        "print(len(All_Training_Instances[0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "618584\n",
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3QbiznjKFdN",
        "colab_type": "code",
        "outputId": "39693b27-2ec9-4c3b-e67e-8bc349ff821b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "#=================Testing=====================\n",
        "All_Testing_Instances=[]\n",
        "\n",
        "#=============== change each window into Instance =================================\n",
        "print(len(X_Test_Full))\n",
        "for i in range(0,len(X_Test_Full)):\n",
        "  hold=[]\n",
        "  temp=[]\n",
        "  for j in range(0,len(X_Test_Full[i])):\n",
        "       #print(len(hold))\n",
        "      if j==(len(X_Test_Full[i])-1):\n",
        "          hold=np.concatenate((hold, X_Test_Full[i][j][1]), axis=None)\n",
        "      else:\n",
        "          hold=np.concatenate((hold, X_Test_Full[i][j][1]), axis=None)\n",
        "   \n",
        "  All_Testing_Instances.append(hold)\n",
        "\n",
        "#prediction=multioutput.predict(All_Testing_Instances)\n",
        "print(len(All_Testing_Instances[0]))\n",
        "#===========================calling MultiOutput XGoost=========================\n",
        "All_Testing_Instances=np.reshape(All_Testing_Instances, (len(All_Testing_Instances),len(All_Testing_Instances[0])))\n",
        "Y_Test_Full=np.reshape(Y_Test_Full, (len(Y_Test_Full),num_periods_output))\n",
        "\n",
        "#========== reshape train ==============================\n",
        "All_Training_Instances=np.reshape(All_Training_Instances, (len(All_Training_Instances),len(All_Training_Instances[0])))\n",
        "shuffled_batch_y=np.reshape(shuffled_batch_y, (len(shuffled_batch_y),num_periods_output))\n",
        "\n",
        "\n",
        "\n",
        "print(All_Training_Instances.shape)\n",
        "model=xgb.XGBRegressor(learning_rate =0.1,\n",
        " n_estimators=1000,\n",
        " max_depth=5,\n",
        " min_child_weight=1,\n",
        " gamma=0,\n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " nthread=4,\n",
        " scale_pos_weight=1,\n",
        " seed=27,silent=False)\n",
        "\n",
        "\n",
        "multioutput=MultiOutputRegressor(model).fit(All_Training_Instances,shuffled_batch_y)\n",
        "\n",
        "\n",
        "print('Fitting Done!')\n",
        "\n",
        "prediction=multioutput.predict(All_Testing_Instances)\n",
        "#print('prediction ',prediction.shape)\n",
        "#print('test ',Y_Test_Full.shape)\n",
        "\n",
        "MSE=np.mean(( prediction- Y_Test_Full)**2)\n",
        "\n",
        "print('RMSE: ',MSE**0.5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125096\n",
            "24\n",
            "(618584, 24)\n",
            "[12:15:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
            "  \"because it will generate extra copies and increase memory consumption\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[12:25:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:35:36] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:45:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[12:55:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:06:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:16:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:26:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:36:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:47:27] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[13:57:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[14:07:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Fitting Done!\n",
            "RMSE:  8.296422109078812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKA6_M8rbwv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}