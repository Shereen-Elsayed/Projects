{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NoFeatures_MultiOutput_xgboost_Birm_2011.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4irvyK-SMkW",
        "colab_type": "code",
        "outputId": "1350cc31-1658-47bb-ff62-a03d51cc121b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liUNnx6XSedu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.version\n",
        "#Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "%matplotlib inline\n",
        "from random import shuffle\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "num_periods_output = 2 #to predict\n",
        "num_periods_input=4 #input\n",
        "\n",
        "No_Of_weeks = 4\n",
        "#Total_No_Features = 15\n",
        "Total_No_Features = 2\n",
        "\n",
        "ALL_Test_Data=[]\n",
        "ALL_Test_Prediction=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAoAla77SilI",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKoJlv9ESfXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "def preprocessing(df,num_features):\n",
        "    \n",
        "    df = df.drop(['Occupancy'], axis = 1)\n",
        "    df['Occupancy'] = df['Occ_percent']\n",
        "    df = df.drop(['Occ_percent'], axis = 1)\n",
        "\n",
        "    df['LastUpdated'] = pd.to_datetime(df['LastUpdated'])\n",
        "    df['Month'] = df['LastUpdated'].dt.month\n",
        "    df['Day'] = df['LastUpdated'].dt.day\n",
        "    df['Hour'] = df['LastUpdated'].dt.hour\n",
        "    df['Minute'] = df['LastUpdated'].dt.minute\n",
        "     \n",
        "    # select features\n",
        "    #with features\n",
        "   # df=df[['ID','Occupancy','temperature','Month','Day','Hour','Minute','dew_point', 'humidity', 'wind_speed', 'feels_like','Events_Football_City', 'Events_Football_Derby',\n",
        "    #   'Events_Football_Aston', 'Events_Rugby']]    \n",
        "    \n",
        "    #Without features\n",
        "    df=df[['ID','Occupancy']]\n",
        "\n",
        "    ################################################encoding########################\n",
        "    df['Occupancy'] = pd.to_numeric(df['Occupancy'],errors='coerce')\n",
        "    df['Occupancy'] = df['Occupancy'].abs()\n",
        "    \n",
        "        \n",
        "    Number_Of_Features=num_features\n",
        "    df=df.values\n",
        "    df = df.astype('float32')\n",
        "    split=num_periods_output+num_periods_input  \n",
        "    \n",
        "    \n",
        "    ##################################SPLIT##############################################\n",
        "    \n",
        "     ########################## SPLITTING FOR TESTING ##########################\n",
        "    test_len=np.floor(len(df)*0.2)\n",
        "    mod=test_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    test_len=int(test_len-mod)\n",
        "    Test=df[(len(df)-test_len):,:]\n",
        "\n",
        "    ########################### SPLITTING FOR TRAIN ###########################\n",
        "    \n",
        "    new_cutted_df=df[:(len(df)-test_len),:]\n",
        "    Start_train_index=int(np.floor(2*24*7*No_Of_weeks))\n",
        "    print('instances',Start_train_index)\n",
        "    Train=new_cutted_df[len(new_cutted_df)-Start_train_index:,:]\n",
        "    train_len=len(Train)\n",
        "    mod=train_len%(num_periods_input+num_periods_output)\n",
        "    #let thelength be divisable by 12\n",
        "    train_len=int(train_len-mod)\n",
        "    Train=Train[0:train_len,:]\n",
        "    print('len Train',len(Train))\n",
        "   \n",
        "    '''\n",
        "    #############################  Normalization on train and validation separatly  #############\n",
        "     #statistical Features: 'DayOfWeek_Bin_avg','Hour_Bin_avg','DayOfWeek_Hour_Bin_avg'\n",
        "    ID_Train=Train[:,0]\n",
        "    Train=np.delete(Train,[0],1)\n",
        "    #x_batches=x_batches.drop(columns=['ID'], axis=1)\n",
        "    occ_Train=Train[:,0]\n",
        "    Train=np.delete(Train,[0],1)\n",
        "       \n",
        "    #normalizing data\n",
        "    Train = Train.astype('float32')\n",
        "    normalizer = MinMaxScaler().fit(Train)\n",
        "    #normalizer = Normalizer().fit(Train)\n",
        "    Train=normalizer.transform(Train)\n",
        "    \n",
        "    ID_Train=np.reshape(ID_Train,(len(ID_Train),1))\n",
        "    occ_Train=np.reshape(occ_Train,(len(occ_Train),1))\n",
        "\n",
        "    Train=np.append(occ_Train, Train, axis=1)\n",
        "    Train=np.append(ID_Train, Train, axis=1)'''\n",
        "\n",
        "    ############################################ TRAIN minibatches ##################################\n",
        "    \n",
        "    end=len(Train)\n",
        "    start=0\n",
        "    next=0\n",
        "    x_batches=[]\n",
        "    y_batches=[]\n",
        "    \n",
        "    count=0\n",
        "    #print('lennnn',len(Train))\n",
        "    while next+(num_periods_input+num_periods_output)<end:\n",
        "        next=start+num_periods_input\n",
        "        x_batches.append(Train[start:next,:])\n",
        "        y_batches.append(Train[next:next+num_periods_output,1])\n",
        "        start=start+1\n",
        "    y_batches=np.asarray(y_batches)\n",
        "    y_batches = y_batches.reshape(-1, num_periods_output, 1)   \n",
        "    x_batches=np.asarray(x_batches)\n",
        "    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)   \n",
        "    print('len x_batches ',len(x_batches))\n",
        "    \n",
        "    \n",
        "    ###########################################TEST#####################################\n",
        "    '''\n",
        "    ID_Test=Test[:,0]\n",
        "    Test=np.delete(Test,[0],1)\n",
        "    #X_test=X_test.drop(columns=['ID'], axis=1)\n",
        "    occ_Test=Test[:,0]\n",
        "    Test=np.delete(Test,[0],1)\n",
        "        \n",
        "    Test = Test.astype('float32')\n",
        "    Test=normalizer.transform(Test)\n",
        "    \n",
        "    #------------------\n",
        "    ID_Test=np.reshape(ID_Test,(len(ID_Test),1))\n",
        "    occ_Test=np.reshape(occ_Test,(len(occ_Test),1))\n",
        "\n",
        "    Test=np.append(occ_Test,Test, axis=1)\n",
        "    Test=np.append(ID_Test, Test, axis=1)\n",
        "    '''\n",
        "    ############################################ TEST minibatches ##################################\n",
        "    end_test=len(Test)\n",
        "    start_test=0\n",
        "    next_test=0\n",
        "    x_testbatches=[]\n",
        "    y_testbatches=[]\n",
        "    \n",
        "    \n",
        "    #print('lennnn',len(Train))\n",
        "    while next_test+(num_periods_input+num_periods_output)<end_test:\n",
        "        next_test=start_test+num_periods_input\n",
        "        x_testbatches.append(Test[start_test:next_test,:])\n",
        "        y_testbatches.append(Test[next_test:next_test+num_periods_output,1])\n",
        "        start_test=start_test+1\n",
        "    y_testbatches=np.asarray(y_testbatches)\n",
        "    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)   \n",
        "    x_testbatches=np.asarray(x_testbatches)\n",
        "    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features) \n",
        "    print('len Test',len(Test))\n",
        "    print('len xTestbatches',len(x_testbatches))\n",
        "    ######################## Sampling##########################################\n",
        "    \n",
        "    #x_batches, y_batches, x_validbatches, y_validbatches, x_testbatches, y_testbatches\n",
        "    \n",
        "    return x_batches, y_batches, x_testbatches, y_testbatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhE-xQyhjiBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_locationfiles(path,loc_id):\n",
        "    filename=path + '/Birm'+str(loc_id)+'.csv'\n",
        "    print(filename)\n",
        "    data_loc=pd.read_csv(filename)\n",
        "    return data_loc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti1cae2tjoOS",
        "colab_type": "code",
        "outputId": "063d7368-bc11-4b6a-8b99-c048de242c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "    data_path=r'/content/drive/My Drive/Birm_Data_Events'\n",
        "\n",
        "    data_All=pd.DataFrame()\n",
        "    x_batches_Full=[]\n",
        "    y_batches_Full=[]\n",
        "    X_Test_Full=[]\n",
        "    Y_Test_Full=[]\n",
        "    range_list = [x for x in range(1,31) if x != 8 and x != 21]\n",
        "\n",
        "    for loc_id in range_list:\n",
        "        #========\n",
        "        data=load_locationfiles(data_path,loc_id)\n",
        "        header=list(data.columns.values)\n",
        "        data=pd.DataFrame(data,columns=header)\n",
        "        x_batches, y_batches,X_Test,Y_Test=preprocessing(data,Total_No_Features)\n",
        "        #===============================\n",
        "        for element1 in (x_batches):\n",
        "            x_batches_Full.append(element1)\n",
        "            \n",
        "        for element2 in (y_batches):\n",
        "            y_batches_Full.append(element2)\n",
        "                        \n",
        "        for element5 in (X_Test):\n",
        "            X_Test_Full.append(element5)\n",
        "            \n",
        "        for element6 in (Y_Test):\n",
        "            Y_Test_Full.append(element6)\n",
        "    #---------------------shuffle minibatches X and Y together-------------------------------------\n",
        "    print(len(x_batches_Full),'     length of all file : ',len(y_batches_Full))\n",
        "    combined = list(zip(x_batches_Full, y_batches_Full))\n",
        "    random.shuffle(combined)\n",
        "    shuffled_batch_features, shuffled_batch_y = zip(*combined)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Birm_Data_Events/Birm1.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm2.csv\n",
            "instances 1344\n",
            "len Train 318\n",
            "len x_batches  309\n",
            "len Test 252\n",
            "len xTestbatches 243\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm3.csv\n",
            "instances 1344\n",
            "len Train 306\n",
            "len x_batches  297\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm4.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm5.csv\n",
            "instances 1344\n",
            "len Train 390\n",
            "len x_batches  381\n",
            "len Test 234\n",
            "len xTestbatches 225\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm6.csv\n",
            "instances 1344\n",
            "len Train 390\n",
            "len x_batches  381\n",
            "len Test 234\n",
            "len xTestbatches 225\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm7.csv\n",
            "instances 1344\n",
            "len Train 390\n",
            "len x_batches  381\n",
            "len Test 234\n",
            "len xTestbatches 225\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm9.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm10.csv\n",
            "instances 1344\n",
            "len Train 318\n",
            "len x_batches  309\n",
            "len Test 252\n",
            "len xTestbatches 243\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm11.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm12.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm13.csv\n",
            "instances 1344\n",
            "len Train 306\n",
            "len x_batches  297\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm14.csv\n",
            "instances 1344\n",
            "len Train 510\n",
            "len x_batches  501\n",
            "len Test 204\n",
            "len xTestbatches 195\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm15.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm16.csv\n",
            "instances 1344\n",
            "len Train 306\n",
            "len x_batches  297\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm17.csv\n",
            "instances 1344\n",
            "len Train 390\n",
            "len x_batches  381\n",
            "len Test 234\n",
            "len xTestbatches 225\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm18.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm19.csv\n",
            "instances 1344\n",
            "len Train 390\n",
            "len x_batches  381\n",
            "len Test 234\n",
            "len xTestbatches 225\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm20.csv\n",
            "instances 1344\n",
            "len Train 378\n",
            "len x_batches  369\n",
            "len Test 240\n",
            "len xTestbatches 231\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm22.csv\n",
            "instances 1344\n",
            "len Train 378\n",
            "len x_batches  369\n",
            "len Test 240\n",
            "len xTestbatches 231\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm23.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm24.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm25.csv\n",
            "instances 1344\n",
            "len Train 306\n",
            "len x_batches  297\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm26.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm27.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm28.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm29.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "/content/drive/My Drive/Birm_Data_Events/Birm30.csv\n",
            "instances 1344\n",
            "len Train 288\n",
            "len x_batches  279\n",
            "len Test 258\n",
            "len xTestbatches 249\n",
            "8856      length of all file :  8856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4rGUPhikOCE",
        "colab_type": "code",
        "outputId": "c4488e0d-b441-4a9e-91a4-7a082cefc740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "#xgboost part\n",
        "print(len(x_batches_Full))\n",
        "All_Training_Instances=[]\n",
        "\n",
        "'''Static_Features=[[1,0,0,0,0,0,0,0,1,3,4,6,7,19,9,17,51.3787,-2.3622],[0,1,0,0,0,0,0,0,0,3,9,11,3,29,10,13,51.3843,-2.3686],\n",
        "                     [0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,51.4113,-2.3869],[0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,0,51.3902,-2.4059]\n",
        "                    ,[0,0,0,0,1,0,0,0,0,0,0,4,2,3,0,0,51.3529,-2.3838],[0,0,0,0,0,1,0,0,2,4,8,6,3,39,13,14,51.3842,-2.3590],\n",
        "                     [0,0,0,0,0,0,1,0,1,1,3,6,6,17,9,7,51.3782,-2.3589],[0,0,0,0,0,0,0,1,1,2,3,6,6,18,9,7,51.3783,-2.3593]]'''\n",
        " \n",
        "#=============== change each window into Instance =================================\n",
        "for i in range(0,len(shuffled_batch_features)):\n",
        "    hold=[]\n",
        "\n",
        "    for j in range(0,len(shuffled_batch_features[i])):\n",
        "      hold=np.concatenate((hold, shuffled_batch_features[i][j][1]), axis=None) \n",
        "    All_Training_Instances.append(hold) \n",
        "\n",
        "print(len(All_Training_Instances[0]))\n",
        "\n",
        "#=================Testing=====================\n",
        "All_Testing_Instances=[]\n",
        "\n",
        "#=============== change each window into Instance =================================\n",
        "print(len(X_Test_Full))\n",
        "for i in range(0,len(X_Test_Full)):\n",
        "  hold=[]\n",
        "  for j in range(0,len(X_Test_Full[i])):\n",
        "    \n",
        "    hold=np.concatenate((hold, X_Test_Full[i][j][1]), axis=None)\n",
        "   \n",
        "  All_Testing_Instances.append(hold)\n",
        "\n",
        "print(len(All_Testing_Instances[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8856\n",
            "4\n",
            "6750\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui3lIrVDma7x",
        "colab_type": "code",
        "outputId": "935fdd76-2742-4a27-ccb5-3f3b8625edfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "All_Testing_Instances=np.reshape(All_Testing_Instances, (len(All_Testing_Instances),len(All_Testing_Instances[0])))\n",
        "Y_Test_Full=np.reshape(Y_Test_Full, (len(Y_Test_Full),num_periods_output))\n",
        "\n",
        "#========== reshape train ==============================\n",
        "All_Training_Instances=np.reshape(All_Training_Instances, (len(All_Training_Instances),len(All_Training_Instances[0])))\n",
        "shuffled_batch_y=np.reshape(shuffled_batch_y, (len(shuffled_batch_y),num_periods_output))\n",
        "\n",
        "print(All_Training_Instances.shape)\n",
        "model=xgb.XGBRegressor(learning_rate =0.01,\n",
        " n_estimators=800,\n",
        " max_depth=5,\n",
        " min_child_weight=1,\n",
        " gamma=0,\n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " nthread=4,\n",
        " scale_pos_weight=1,\n",
        " seed=27,silent=False)\n",
        "\n",
        "multioutput=MultiOutputRegressor(model).fit(All_Training_Instances,shuffled_batch_y)\n",
        "\n",
        "print('Fitting Done!')\n",
        "\n",
        "prediction=multioutput.predict(All_Testing_Instances)\n",
        "MSE=np.mean(( prediction- Y_Test_Full)**2)\n",
        "\n",
        "print('RMSE: ',MSE**0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8856, 4)\n",
            "[21:29:11] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
            "  \"because it will generate extra copies and increase memory consumption\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[21:29:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Fitting Done!\n",
            "RMSE:  8.560023040027449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Z25i6Wq0Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}