{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Naive Forecast _ Predict Last Value.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"u3snmuco2Ca9","colab_type":"code","outputId":"45cc3ab7-de78-40ba-b903-7dbf0e6903f1","executionInfo":{"status":"ok","timestamp":1572455315426,"user_tz":-60,"elapsed":1571,"user":{"displayName":"Shero Hatem","photoUrl":"","userId":"13278018262454796058"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FTvce_Rb2BNd","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import itertools"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQ_KbBP62BNh","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import Normalizer\n","num_periods_output=12\n","num_periods_input=24\n","\n","def preprocessing(df,num_features):\n","    \n","    Number_Of_Features=num_features\n","    \n","\n","    '''df=df.drop(['LastUpdate'],axis=1)\n","    #print(df.iloc[:,5])\n","    ################################################encoding####################\n","    df['Occupancy'] = pd.to_numeric(df['Occupancy'],errors='coerce')\n","    df['Occupancy'] = df['Occupancy'].abs()\n","    \n","    Status=df.pop('Status')\n","    df.loc[:,'Status_filling']=(Status=='Filling')*1.0\n","    df.loc[:,'Status_static']=(Status=='Static')*1.0\n","    df.loc[:,'Status_emptying']=(Status=='Emptying')*1.0\n","    DayOfWeek=df.pop('DayOfWeek')\n","    df.loc[:,'DayOfWeek_0']=(DayOfWeek==0)*1.0\n","    df.loc[:,'DayOfWeek_1']=(DayOfWeek==1)*1.0\n","    df.loc[:,'DayOfWeek_2']=(DayOfWeek==2)*1.0\n","    df.loc[:,'DayOfWeek_3']=(DayOfWeek==3)*1.0\n","    df.loc[:,'DayOfWeek_4']=(DayOfWeek==4)*1.0\n","    df.loc[:,'DayOfWeek_5']=(DayOfWeek==5)*1.0\n","    df.loc[:,'DayOfWeek_6']=(DayOfWeek==6)*1.0\n","    \n","    \n","    df=df.values\n","    df = df.astype('float32')\n","    split=num_periods_output+num_periods_input\n","    '''\n","    ##################################SPLIT##############################################\n","    \n","    ########################### SPLITTING FOR TRAIN ###########################\n","    train_len=np.floor(len(df)*0.8)\n","    mod=train_len%(num_periods_input+num_periods_output)\n","    #let thelength be divisable by 12\n","    train_len=int(train_len-mod)\n","    Train=df[0:train_len]\n","    ######################### SPLITTING FOR VALIDATION ########################\n","    #valid_len=np.floor(len(df)*0.1)\n","    #mod=valid_len%(num_periods_input+num_periods_output)\n","    #let thelength be divisable by 12\n","    #valid_len=int(valid_len-mod)\n","    #Valid=df[train_len:(train_len+valid_len),:]\n","    ########################## SPLITTING FOR TESTING ##########################\n","    test_len=np.floor(len(df)*0.2)\n","    #test_len=np.floor(len(df)*0.01)\n","    mod=test_len%(num_periods_input+num_periods_output)\n","    #let thelength be divisable by 12\n","    test_len=int(test_len-mod)\n","    Test=df[len(df)-test_len:len(df)]\n","    #Test=df[len(df)-test_len:len(df),:]\n","    #############################  Normalization on train and validation separatly  #############\n","    '''\n","    ID_Train=Train[:,6]\n","    Train=np.delete(Train,[6],1)\n","    #x_batches=x_batches.drop(columns=['ID'], axis=1)\n","    occ_Train=Train[:,5]\n","    Train=np.delete(Train,[5],1)\n","    #x_batches=x_batches.drop(columns=['Occupancy'], axis=1)\n","    #normalizing data\n","    Train = Train.astype('float32')\n","    normalizer = Normalizer().fit(Train)\n","    Train=normalizer.transform(Train)\n","    \n","    ID_Valid=Valid[:,0]\n","    Valid=np.delete(Valid,[0],1)\n","    #X_test=X_test.drop(columns=['ID'], axis=1)\n","    occ_Valid=Valid[:,0]\n","    Valid=np.delete(Valid,[0],1)\n","    #X_test=X_test.drop(columns=['Occupancy'], axis=1)\n","    Valid = Valid.astype('float32')\n","    Valid=normalizer.transform(Valid)\n","    #------------------\n","    ID_Train=np.reshape(ID_Train,(len(ID_Train),1))\n","    occ_Train=np.reshape(occ_Train,(len(occ_Train),1))\n","    \n","    Train=np.append(occ_Train, Train, axis=1)\n","    Train=np.append(ID_Train, Train, axis=1)\n","    #------------------\n","    ID_Valid=np.reshape(ID_Valid,(len(ID_Valid),1))\n","    occ_Valid=np.reshape(occ_Valid,(len(occ_Valid),1))\n","    \n","    Valid=np.append(occ_Valid,Valid, axis=1)\n","    Valid=np.append(ID_Valid, Valid, axis=1)'''\n","    \n","    ############################################ TRAIN minibatches ##################################\n","    \n","    end=len(Train)\n","    start=0\n","    next=0\n","    x_batches=[]\n","    y_batches=[]\n","    \n","    #print('lennnn',len(Train))\n","    while next+num_periods_input<end:\n","        next=start+num_periods_input\n","        x_batches.append(Train[start:next])\n","        y_batches.append(Train[next:next+num_periods_output])\n","        start=start+1\n","    y_batches=np.asarray(y_batches)\n","    y_batches = y_batches.reshape(-1, num_periods_output, 1)   \n","    x_batches=np.asarray(x_batches)\n","    x_batches = x_batches.reshape(-1, num_periods_input, Number_Of_Features)   \n","    \n","    ############################################ VALID minibatches ##################################\n","    '''\n","    end_val=len(Valid)\n","    start_val=0\n","    next_val=0\n","    x_validbatches=[]\n","    y_validbatches=[]\n","    \n","    #print('lennnn',len(Train))\n","    while next_val+num_periods_input<end_val:\n","        next_val=start_val+num_periods_input\n","        x_validbatches.append(Valid[start_val:next_val,:])\n","        #print('input for X:  ',Valid[start_val:next_val,:])\n","        y_validbatches.append(Valid[next_val:next_val+num_periods_output,1])\n","        #print('input for Y:  ',Valid[next_val:next_val+num_periods_output,1])\n","        start_val=start_val+1\n","    y_validbatches=np.asarray(y_validbatches)\n","    y_validbatches = y_validbatches.reshape(-1, num_periods_output, 1)   \n","    x_validbatches=np.asarray(x_validbatches)\n","    x_validbatches = x_validbatches.reshape(-1, num_periods_input, Number_Of_Features)  ''' \n","    \n","    ###########################################TEST#####################################\n","    '''\n","    ID_Test=Test[:,6]\n","    #print(ID_Test)\n","    Test=np.delete(Test,[6],1)\n","    #X_test=X_test.drop(columns=['ID'], axis=1)\n","    occ_Test=Test[:,5]\n","    #print(occ_Test)\n","    Test=np.delete(Test,[5],1)\n","    #X_test=X_test.drop(columns=['Occupancy'], axis=1)\n","    Test = Test.astype('float32')\n","    Test=normalizer.transform(Test)\n","    \n","    #------------------\n","    ID_Test=np.reshape(ID_Test,(len(ID_Test),1))\n","    occ_Test=np.reshape(occ_Test,(len(occ_Test),1))\n","    \n","    Test=np.append(occ_Test,Test, axis=1)\n","    #print(Test)\n","    Test=np.append(ID_Test, Test, axis=1)\n","    #print(Test)'''\n","    ############################################ TEST minibatches ##################################\n","    end_test=len(Test)\n","    start_test=0\n","    next_test=0\n","    x_testbatches=[]\n","    y_testbatches=[]\n","    \n","    \n","    #print('lennnn',len(Train))\n","    while next_test+num_periods_input<end_test:\n","        next_test=start_test+num_periods_input\n","        x_testbatches.append(Test[start_test:next_test])\n","        y_testbatches.append(Test[next_test:next_test+num_periods_output])\n","        start_test=start_test+1\n","    y_testbatches=np.asarray(y_testbatches)\n","    y_testbatches = y_testbatches.reshape(-1, num_periods_output, 1)   \n","    x_testbatches=np.asarray(x_testbatches)\n","    x_testbatches = x_testbatches.reshape(-1, num_periods_input, Number_Of_Features)  \n","    \n","    return x_batches, y_batches, x_testbatches, y_testbatches"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7x7T5fq2BNk","colab_type":"code","colab":{}},"source":["#filepath = 'C:/Users/chamu/Documents/Python Scripts/Parking Project/Naive Forecast/half_data_15/BN00'\n","filepath = '/content/drive/My Drive/FINAL_DATA_EVENTS/BN00'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHDgi8w_2BNm","colab_type":"code","colab":{}},"source":["def compute_se(prediction,actual):\n","    \n","    return (prediction-actual)**2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tVgcFNe2BNp","colab_type":"code","colab":{}},"source":["def compute_rmse(se_list):\n","    se = 0\n","    \n","    for i in range(0,len(se_list)):\n","        se = se + se_list[i]\n","        #print(se)\n","        \n","    rmse = np.sqrt(se/len(se_list))\n","    \n","    return rmse"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ShqTuAoD2BNr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":260},"outputId":"f5475164-4840-4f84-9a05-3f65b9eb20e0","executionInfo":{"status":"ok","timestamp":1572455333507,"user_tz":-60,"elapsed":19578,"user":{"displayName":"Shero Hatem","photoUrl":"","userId":"13278018262454796058"}}},"source":["se_list = []\n","for file in range(1,9):\n","    path = filepath + str(file) + '.csv'\n","    BN = pd.read_csv(path, sep=',', index_col = None)\n","    if BN.ID[0]!=7 and BN.ID[0]!=8:\n","        print(BN.Year.unique())\n","        BN=BN[(BN.Year==2017) | (BN.Year==2018)]\n","        #.isin(years)\n","        #print(df.loc[df['Year'].isin([2017,2018])])\n","        print(BN.Year.unique())\n","        #print(df.Capacity.unique())\n","    else:\n","        BN=BN[(BN.Year==2015) | (BN.Year==2016)]\n","        print(BN.Year.unique())\n","        #print(df.Capacity.unique())\n","    BN = BN[ 'Occupancy'].values\n","    #print(BN)\n","    #BN = BN.sort_values(by=['LastUpdate'])\n","    window_size=24\n","    x_batches_Full=[]\n","    y_batches_Full=[]\n","    X_Test_Full=[]\n","    Y_Test_Full=[]\n","    x_batches, y_batches,X_Test,Y_Test=preprocessing(BN,1)\n","    #===============================\n","    for element1 in (x_batches):\n","        x_batches_Full.append(element1)\n","            \n","    for element2 in (y_batches):\n","        y_batches_Full.append(element2)\n","                        \n","    for element5 in (X_Test):\n","        X_Test_Full.append(element5)\n","            \n","    for element6 in (Y_Test):\n","        Y_Test_Full.append(element6)\n","    \n","    #ground_truth = BN['Occupancy'].tolist() \n","     \n","    \n","    #prediction = ground_truth[0]  #predicts the last_value seen \n","    \n","    for i in range(0,len(X_Test_Full)):\n","      for j in range(0,num_periods_output):\n","        \n","        actual_value = Y_Test_Full[i][j]\n","        \n","        prediction = X_Test_Full[i][j+12]  #predicts the last_value        \n","        #print(actual_value,'  prediction ',prediction)\n","        se_list.append(compute_se(prediction,actual_value))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[2014 2015 2016 2017 2018 2019]\n","[2017 2018]\n","[2014 2015 2016 2017 2018 2019]\n","[2017 2018]\n","[2014 2015 2016 2017 2018 2019]\n","[2017 2018]\n","[2015 2016 2017 2018 2019]\n","[2017 2018]\n","[2014 2015 2016 2017 2018 2019]\n","[2017 2018]\n","[2014 2015 2016 2017 2018 2019]\n","[2017 2018]\n","[2015 2016]\n","[2015 2016]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z1A9oAFS2BNu","colab_type":"code","colab":{}},"source":["RMSE = compute_rmse(se_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"re2fRf8_2BNw","colab_type":"code","outputId":"c50b795e-364c-4fad-b51f-73185d6ed53a","executionInfo":{"status":"ok","timestamp":1572455335429,"user_tz":-60,"elapsed":21476,"user":{"displayName":"Shero Hatem","photoUrl":"","userId":"13278018262454796058"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print('RMSE: {}'.format(RMSE))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["RMSE: [13.96773168]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FFuUmzoR2BNz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}